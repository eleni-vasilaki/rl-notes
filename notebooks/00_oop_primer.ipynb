{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-Oriented Programming: A Primer for Reinforcement Learning\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/00_oop_primer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This primer introduces the object-oriented programming (OOP) techniques used throughout the reinforcement learning notebooks. The goal is not to present OOP as a set of templates to copy, but to develop the intuition for *when* and *why* these techniques help — and to prepare you for machine learning libraries used in research and industry, which are built on exactly the same ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning Picture\n",
    "\n",
    "The code examples in this primer are drawn from reinforcement learning (RL), so it helps to have the basic picture in mind before we start writing classes. You do not need to understand the maths — just the cast of characters.\n",
    "\n",
    "Imagine training a dog. The dog is in a room. It can do things — sit, walk left, walk right. After it does something, you might give it a treat, or not. Over many repetitions, the dog figures out which actions lead to treats.\n",
    "\n",
    "Reinforcement learning is this idea, made precise:\n",
    "\n",
    "- The **environment** is the room — the world the learner lives in. It keeps track of the current situation (the **state**), and when the learner does something, the environment responds: it produces a new state and possibly a **reward** (the treat).\n",
    "- The **agent** is the dog — the learner. It observes the current state, picks an **action**, and then updates its understanding of the world based on the reward it receives. Over time, it builds up **value estimates** — its own guess of how good each action is in each situation.\n",
    "- The **policy** is the agent's strategy for choosing actions. \"Always pick the action I think is best\" is one policy. \"Usually pick the best, but sometimes try something random\" is another. The policy is a *rule*, not a separate entity — it describes *how* the agent decides.\n",
    "\n",
    "One full run from start to finish — the dog enters the room, does things, and eventually reaches a goal or a time limit — is called an **episode**. Training typically involves many episodes: the agent gets better each time because it remembers what it learned in previous episodes.\n",
    "\n",
    "The whole process is a loop:\n",
    "\n",
    "```\n",
    "state = environment.reset()                         # Start a new episode\n",
    "while not done:\n",
    "    action = agent.select_action(state)              # Agent picks an action\n",
    "    state, reward, done = environment.step(action)   # Environment responds\n",
    "    agent.update(state, action, reward)              # Agent learns from the reward\n",
    "```\n",
    "\n",
    "Notice that the environment tracks the state — including the agent's position in the world. This might seem odd (shouldn't the agent know where *it* is?), but it is a practical design choice: the environment is the single source of truth for everything about the world, and the agent receives that information through the values returned by `reset()` and `step()`. Think of a board game — the *board* tracks where all the pieces are; the player just decides which move to make. This is also the convention followed by popular RL libraries such as Gymnasium and Stable-Baselines3, so adopting it here means the patterns you learn will transfer directly if you use those tools later.\n",
    "\n",
    "This loop is what every RL notebook in this course implements in one form or another. The purpose of OOP is to make this loop *readable* — each noun in the description becomes a class, each verb becomes a method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding Classes in Your Problem\n",
    "\n",
    "Look at the RL loop above. A useful rule of thumb for turning a description into code:\n",
    "\n",
    "> **Nouns suggest classes. Verbs suggest methods.**\n",
    "\n",
    "The nouns *agent* and *environment* are things with identity. They persist over time, they have properties (the agent has value estimates; the environment has a layout), and they do things (the agent selects; the environment transitions). These are natural candidates for classes.\n",
    "\n",
    "Other nouns — *action*, *reward*, *state* — are simple values: an integer, a float, a tuple. They don't need to be classes; a plain number or array will do.\n",
    "\n",
    "The *policy* is an interesting case. It is a rule, not an entity — \"usually pick the best action, but sometimes explore.\" It *could* be a function. It *could* be a class. The right choice depends on how much structure you need, and we will revisit this question as the primer develops.\n",
    "\n",
    "This distinction matters because it tells you where to invest your effort. A class is worth writing when something has **state that changes over time** and **operations that act on that state**. If something is just a value passed around, keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Key Question: \"What Needs to Remember?\"\n",
    "\n",
    "The single most useful question when deciding whether to use a class is:\n",
    "\n",
    "> **Does this thing need to remember something between one call and the next?**\n",
    "\n",
    "Look at the RL loop again. The environment needs to remember the current state — it changes every step. The agent needs to remember its value estimates — they get updated after every reward. Both have *state* that persists and changes, so both benefit from being classes.\n",
    "\n",
    "The policy, on the other hand, is a decision rule: \"given the current value estimates, pick an action.\" It does not need to remember anything of its own — it just looks at what the agent currently knows and makes a choice. That could be a simple function. (In Section 6, we will see that it can also be useful to wrap decision rules in classes for code organisation, but the motivation there is structure, not memory.)\n",
    "\n",
    "A sigmoid function is another clear case — it just transforms an input and returns an output:\n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "Similarly, a helper that splits data into train/test sets, or one that smooths a curve, is a pure transformation — no memory, no state, just input in and output out. Keep these as standalone functions.\n",
    "\n",
    "**Rule of thumb:** if you can describe something entirely as \"given *this*, return *that*\" with no hidden memory, it is a function. If it also needs to *keep track of something* across calls, it is a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anatomy of a Class\n",
    "\n",
    "A class bundles together **data** (attributes) and **behaviour** (methods) into a single unit. Here is the general structure:\n",
    "\n",
    "```python\n",
    "class ClassName:\n",
    "    \"\"\"One-line description of what this class represents.\"\"\"\n",
    "\n",
    "    def __init__(self, ...):       # Constructor: runs once when you create an object\n",
    "        self.attribute = ...       # Instance attribute: data stored on *this* object\n",
    "\n",
    "    def some_method(self, ...):    # Method: a function that can read/modify the object's data\n",
    "        ...\n",
    "```\n",
    "\n",
    "Three things to notice:\n",
    "\n",
    "- **`__init__`** is the constructor. It runs automatically when you write `ClassName(...)`. Its job is to set up the object's initial state.\n",
    "- **`self`** is how each object refers to itself. When you write `self.Q = np.zeros(k)`, you are storing an array *on this particular object*, not in some global variable. Two objects created from the same class have independent copies of their attributes.\n",
    "- **Methods** are functions defined inside the class. They always receive `self` as their first argument, giving them access to the object's data.\n",
    "\n",
    "Let us make this concrete with a simple example from reinforcement learning: a single arm of a multi-armed bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull arm A: reward = -0.14  (pulled 1 time)\n",
      "Pull arm B: reward = 2.70  (pulled 1 time)\n",
      "Pull arm A again: reward = 2.13  (pulled 2 times)\n",
      "Arm B is unaffected: pulled 1 time\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BanditArm:\n",
    "    \"\"\"A single arm of a multi-armed bandit.\"\"\"\n",
    "\n",
    "    def __init__(self, true_mean):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            true_mean (float): The true expected reward of this arm.\n",
    "        \"\"\"\n",
    "        self.true_mean = true_mean   # Stored on the object — this is state\n",
    "        self.pull_count = 0          # Also state: tracks how often we pulled\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Sample a reward (Gaussian noise around the true mean).\"\"\"\n",
    "        self.pull_count += 1         # Modifying the object's own state\n",
    "        return self.true_mean + np.random.randn()\n",
    "\n",
    "\n",
    "# Create two independent arm objects from the same blueprint\n",
    "arm_a = BanditArm(true_mean=1.0)\n",
    "arm_b = BanditArm(true_mean=3.0)\n",
    "\n",
    "# Each object tracks its own state independently\n",
    "print(f\"Pull arm A: reward = {arm_a.pull():.2f}  (pulled {arm_a.pull_count} time)\")\n",
    "print(f\"Pull arm B: reward = {arm_b.pull():.2f}  (pulled {arm_b.pull_count} time)\")\n",
    "print(f\"Pull arm A again: reward = {arm_a.pull():.2f}  (pulled {arm_a.pull_count} times)\")\n",
    "print(f\"Arm B is unaffected: pulled {arm_b.pull_count} time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that pulling `arm_a` twice does not affect `arm_b` at all. Each object is its own independent bundle of data. This is **encapsulation**: the arm's internal bookkeeping (its pull count, its true mean) is packaged together with the operations that use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A Complete Example: Environment and Agent\n",
    "\n",
    "In reinforcement learning, the two central nouns are the **environment** and the **agent**. Let us build both for a simple problem: a one-dimensional track where the agent must walk from the left end to the right end to receive a reward.\n",
    "\n",
    "We will write two classes and connect them through **composition** — the agent stores a reference to the environment it operates in. This keeps responsibilities separate: the environment knows the rules of the world; the agent knows the learning algorithm. Neither needs to know how the other works internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LinearTrackEnv:\n",
    "    \"\"\"A one-dimensional corridor. The agent starts at state 0 and\n",
    "    receives a reward of 1 upon reaching the far end.\"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): Number of states on the track.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        # The environment tracks the agent's position (see RL Picture above).\n",
    "        self.state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Place the agent back at the start. Returns the initial state.\"\"\"\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take one step.\n",
    "\n",
    "        Args:\n",
    "            action (int): 0 = move left, 1 = move right.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        if action == 1:\n",
    "            self.state = min(self.state + 1, self.size - 1)  # Clamp to right edge\n",
    "        else:\n",
    "            self.state = max(0, self.state - 1)              # Clamp to left edge\n",
    "\n",
    "        done = (self.state == self.size - 1)\n",
    "        reward = 1.0 if done else 0.0\n",
    "        return self.state, reward, done\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"Tabular Q-learning agent for discrete environments.\"\"\"\n",
    "\n",
    "    def __init__(self, env, eta=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env (LinearTrackEnv): The environment to learn in.\n",
    "            eta (float): Learning rate.\n",
    "            gamma (float): Discount factor.\n",
    "            epsilon (float): Exploration probability.\n",
    "        \"\"\"\n",
    "        self.env = env                  # Composition: the agent *has* an environment\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((env.size, 2))  # Q-table: one row per state, two actions\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(2)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Apply the Q-learning update rule.\"\"\"\n",
    "        target = reward if done else reward + self.gamma * np.max(self.Q[next_state])\n",
    "        self.Q[state, action] += self.eta * (target - self.Q[state, action])\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"Execute one complete episode. Returns the number of steps taken.\n",
    "\n",
    "        Note: we place the episode loop on the agent here for convenience.\n",
    "        Another valid design is to put this loop in a standalone function\n",
    "        that receives both the agent and the environment as arguments.\n",
    "        \"\"\"\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        return steps\n",
    "\n",
    "\n",
    "# --- Use the classes together ---\n",
    "env = LinearTrackEnv(size=6)\n",
    "agent = QLearningAgent(env, eta=0.5, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "steps_per_episode = []\n",
    "for ep in range(50):\n",
    "    steps = agent.run_episode()\n",
    "    steps_per_episode.append(steps)\n",
    "\n",
    "plt.plot(steps_per_episode)\n",
    "plt.axhline(y=env.size - 1, color='r', linestyle='--', label='Optimal')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Q-Learning on a Linear Track')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:**\n",
    "\n",
    "- The environment tracks the agent's position in `self.state`. This is the design choice introduced in the RL Picture section: the environment is the single source of truth for the world, and the agent learns about its situation through the values returned by `reset()` and `step()`.\n",
    "- `QLearningAgent.__init__` stores `self.env = env`. This is **composition**: the agent *has* an environment. When the agent needs to take a step, it calls `self.env.step(action)`. The agent does not need to know how the environment implements `step` — only that the method exists and returns `(next_state, reward, done)`.\n",
    "- The Q-table `self.Q` lives on the agent, because the agent is the thing that *remembers* value estimates.\n",
    "- `run_episode` lives on the agent here for convenience, but this is a design choice. It could equally be a standalone function: `def run_episode(agent, env): ...`. Neither approach is wrong — what matters is that the responsibilities are clear. In the notebooks you will see both styles.\n",
    "\n",
    "This Environment/Agent split is a pattern you will see in every notebook. It is also the standard structure used by popular RL libraries such as Gymnasium and Stable-Baselines3, so the patterns you practise here will transfer directly if you use those tools later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When to Keep Things as Functions\n",
    "\n",
    "Not everything should be a class. Here are some examples from the notebooks of things that are better left as standalone functions:\n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    \"\"\"No state needed — just a mathematical transformation.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def smooth_data(data, alpha=0.1):\n",
    "    \"\"\"Takes data in, returns smoothed data out. No memory between calls.\"\"\"\n",
    "    smoothed = np.zeros(len(data))\n",
    "    smoothed[0] = data[0]\n",
    "    for i in range(1, len(data)):\n",
    "        smoothed[i] = (1 - alpha) * smoothed[i - 1] + alpha * data[i]\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def split_data(X, y, train_size=0.7):\n",
    "    \"\"\"Partitions arrays. A pure transformation, no state.\"\"\"\n",
    "    n = int(train_size * len(X))\n",
    "    return X[:n], y[:n], X[n:], y[n:]\n",
    "```\n",
    "\n",
    "All three follow the same pattern: they take input, do something to it, and return output. They do not need to *remember* anything between calls. Wrapping these in a class would add complexity without any benefit.\n",
    "\n",
    "**A practical test:** if you find yourself writing a class with only `__init__` and one other method, and `__init__` just stores the arguments, consider whether a plain function would be clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Composition Revisited: Passing the Policy to the Agent\n",
    "\n",
    "In Section 4, the `QLearningAgent` has its action-selection logic (epsilon-greedy) hard-coded inside `select_action`. If we wanted to try softmax selection instead, we would need to edit the agent class or write a new one.\n",
    "\n",
    "A more flexible design uses the same idea as `self.env = env` — **composition** — but this time for the policy. The agent manages Q-values and learning; the policy is a separate object, passed in, that only knows how to pick an action given some values. We can swap policies without touching the agent.\n",
    "\n",
    "Here we apply this to the bandit problem, using `BanditArm` from Section 3 as our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy Q-values: [0.78 1.71 2.55]\n",
      "Softmax Q-values:        [0.   0.52 2.76]\n",
      "\n",
      "Swapped policy — agent now uses: SoftMaxPolicy\n",
      "\n",
      "Softmax Q-values: [1.47 1.72 2.84]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "class BanditArm:\n",
    "    \"\"\"A single arm of a multi-armed bandit.\n",
    "       It is identical to the original BanditArm from section 3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, true_mean):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            true_mean (float): The true expected reward of this arm.\n",
    "        \"\"\"\n",
    "        self.true_mean = true_mean   # Stored on the object — this is state\n",
    "        self.pull_count = 0          # Also state: tracks how often we pulled\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Sample a reward (Gaussian noise around the true mean).\"\"\"\n",
    "        self.pull_count += 1         # Modifying the object's own state\n",
    "        return self.true_mean + np.random.randn()\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Decision rule: usually pick the best, sometimes explore.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, Q):\n",
    "        \"\"\"Given Q-values, return an action.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(len(Q))\n",
    "        return np.argmax(Q)\n",
    "\n",
    "class SoftMaxPolicy:\n",
    "    \"\"\"Decision rule: sample actions proportional to estimated values.\"\"\"\n",
    "\n",
    "    def __init__(self, tau):\n",
    "        self.tau = tau\n",
    "\n",
    "    def select_action(self, Q):\n",
    "        \"\"\"Given Q-values, return an action.\"\"\"\n",
    "        exp_Q = np.exp(Q / self.tau)\n",
    "        probs = exp_Q / np.sum(exp_Q)\n",
    "        return np.random.choice(len(Q), p=probs)\n",
    "\n",
    "\n",
    "class BanditAgent:\n",
    "    \"\"\"Agent for the multi-armed bandit problem.\n",
    "\n",
    "    The agent tracks Q-values and learns from rewards.\n",
    "    The action-selection policy is passed in as a component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arms, policy:Any, eta=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arms (list): List of BanditArm objects (the environment).\n",
    "            policy: An object with a select_action(Q) method.\n",
    "            eta (float): Learning rate.\n",
    "        \"\"\"\n",
    "        self.arms = arms\n",
    "        self.policy = policy       # Composition: the agent *has* a policy\n",
    "        self.eta = eta\n",
    "        self.Q = np.zeros(len(arms))\n",
    "\n",
    "    def run(self, steps):\n",
    "        \"\"\"Interact with the bandit for a number of steps.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.select_action(self.Q)  # Delegate to the policy\n",
    "            reward = self.arms[action].pull()            # Interact with the environment\n",
    "            self.Q[action] += self.eta * (reward - self.Q[action])\n",
    "\n",
    "\n",
    "# --- Set up the environment: three arms from Section 3 ---\n",
    "arms_eg = [BanditArm(true_mean=1.0), BanditArm(true_mean=2.0), BanditArm(true_mean=3.0)]\n",
    "arms_sm = [BanditArm(true_mean=1.0), BanditArm(true_mean=2.0), BanditArm(true_mean=3.0)]\n",
    "\n",
    "# --- Same agent, different policies ---\n",
    "agent_eg = BanditAgent(arms_eg, policy=EpsilonGreedyPolicy(epsilon=0.2), eta=0.1)\n",
    "agent_sm = BanditAgent(arms_sm, policy=SoftMaxPolicy(tau=0.5), eta=0.1)\n",
    "\n",
    "agent_eg.run(steps=200)\n",
    "agent_sm.run(steps=200)\n",
    "\n",
    "print(f\"Epsilon-greedy Q-values: {agent_eg.Q.round(2)}\")\n",
    "print(f\"Softmax Q-values:        {agent_sm.Q.round(2)}\")\n",
    "\n",
    "# We can even swap the policy on an existing agent\n",
    "agent_eg.policy = SoftMaxPolicy(tau=1.0)\n",
    "print(f\"\\nSwapped policy — agent now uses: {agent_eg.policy.__class__.__name__}\") \n",
    "agent_eg.run(steps=200)\n",
    "print(f\"\\nSoftmax Q-values: {agent_eg.Q.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:**\n",
    "\n",
    "- The policy classes are lightweight — they have no Q-values, no memory. They are decision rules: given Q-values, return an action. This matches what we said in Section 2: a policy does not need to remember anything.\n",
    "- The agent stores `self.policy = policy`, just like it stores `self.env = env` in Section 4. Both are **composition**: the agent *has* a policy, the agent *has* an environment.\n",
    "- We can swap policies without rewriting the agent. This is useful for experimenting — try epsilon-greedy, try softmax, compare results — all with the same agent and environment code.\n",
    "- If we want to compare performance of the two agents, we should run longer experiments and observe statistical differences (mean performance, variance). This is best done in two identical but independent environments to avoid interference.\n",
    "\n",
    "**For VS Code users:**\n",
    "\n",
    "We import `Any` from `typing` and annotate policy as `Any` (`self.policy: Any = policy`) to avoid a false-positive Pylance type-checking warning. Pylance is the VS Code Python language engine that provides autocomplete and static analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When Classes Share Behaviour: Inheritance\n",
    "\n",
    "Sometimes you will write two classes and notice that large parts of them are identical. Inheritance lets you write the shared parts once in a **base class**, and then create **subclasses** that only specify what is different.\n",
    "\n",
    "Here is a concrete example. In Section 3, we wrote a `BanditArm` that generates Gaussian rewards (the true mean plus random noise). But in later notebooks, we will encounter arms with different reward distributions — for instance, binary arms that pay out a fixed amount with some probability, or nothing otherwise.\n",
    "\n",
    "Both types of arm need the same bookkeeping:\n",
    "\n",
    "- Track how many times the arm has been pulled (`pull_count`)\n",
    "- Accumulate total reward (`total_reward`)\n",
    "- Compute a running average (`average_reward`)\n",
    "- Reset all of this when starting fresh (`reset`)\n",
    "\n",
    "The only thing that differs is *how they generate a reward*. Rather than writing two separate classes with most of the code duplicated, we can write the shared parts once in a base class and let each subclass define its own `_sample_reward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 0 (GaussianArm ): pulled 436 times, avg reward = 1.98\n",
      "Arm 1 (BinaryArm   ): pulled  47 times, avg reward = 2.13\n",
      "Arm 2 (GaussianArm ): pulled  17 times, avg reward = 1.38\n",
      "\n",
      "Agent Q-values: [2.17 1.32 1.05]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "class BaseBanditArm:  \n",
    "    \"\"\"Base class for bandit arms. \n",
    "\n",
    "    Handles pull counting, reward tracking, and averaging.\n",
    "    Subclasses define how rewards are generated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pull_count = 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all bookkeeping.\"\"\"\n",
    "        self.pull_count = 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Pull the arm: update bookkeeping and return a reward.\"\"\"\n",
    "        self.pull_count += 1\n",
    "        reward = self._sample_reward()   # Defined by each subclass\n",
    "        self.total_reward += reward\n",
    "        return reward\n",
    "\n",
    "    def average_reward(self):\n",
    "        \"\"\"Return the average reward observed so far.\"\"\"\n",
    "        if self.pull_count == 0:\n",
    "            return 0.0\n",
    "        return self.total_reward / self.pull_count\n",
    "\n",
    "    def _sample_reward(self):\n",
    "        \"\"\"Generate a single reward. Subclasses must override this.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses define their reward distribution.\")\n",
    "\n",
    "\n",
    "class GaussianArm(BaseBanditArm):\n",
    "    \"\"\"Arm whose rewards are Gaussian: true_mean + noise.\"\"\"\n",
    "\n",
    "    def __init__(self, true_mean):\n",
    "        super().__init__()           # Set up pull_count, total_reward\n",
    "        self.true_mean = true_mean   # Add what makes this subclass different\n",
    "\n",
    "    def _sample_reward(self):\n",
    "        return self.true_mean + np.random.randn()\n",
    "\n",
    "\n",
    "class BinaryArm(BaseBanditArm):\n",
    "    \"\"\"Arm that pays a fixed amount with some probability, or nothing.\"\"\"\n",
    "\n",
    "    def __init__(self, p_success, magnitude=1.0):\n",
    "        super().__init__()\n",
    "        self.p_success = p_success\n",
    "        self.magnitude = magnitude\n",
    "\n",
    "    def _sample_reward(self):\n",
    "        return self.magnitude if np.random.rand() < self.p_success else 0.0\n",
    "\n",
    "\n",
    "# --- Create a mixed bandit with different arm types ---\n",
    "arms = [\n",
    "    GaussianArm(true_mean=2.0),\n",
    "    BinaryArm(p_success=0.3, magnitude=10.0),\n",
    "    GaussianArm(true_mean=1.5),\n",
    "]\n",
    "\n",
    "# The agent from Section 6 works with any arm — it only calls pull()\n",
    "class BanditAgent:\n",
    "    \"\"\"Agent for the multi-armed bandit problem.\n",
    "\n",
    "    The agent tracks Q-values and learns from rewards.\n",
    "    The action-selection policy is passed in as a component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arms, policy:Any, eta=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arms (list): List of BanditArm objects (the environment).\n",
    "            policy: An object with a select_action(Q) method.\n",
    "            eta (float): Learning rate.\n",
    "        \"\"\"\n",
    "        self.arms = arms\n",
    "        self.policy = policy       # Composition: the agent *has* a policy\n",
    "        self.eta = eta\n",
    "        self.Q = np.zeros(len(arms))\n",
    "\n",
    "    def run(self, steps):\n",
    "        \"\"\"Interact with the bandit for a number of steps.\"\"\"\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.select_action(self.Q)  # Delegate to the policy\n",
    "            reward = self.arms[action].pull()            # Interact with the environment\n",
    "            self.Q[action] += self.eta * (reward - self.Q[action])\n",
    "\n",
    "# EpsilonGreedyPolicy from Section 6\n",
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Decision rule: usually pick the best, sometimes explore.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, Q):\n",
    "        \"\"\"Given Q-values, return an action.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(len(Q))\n",
    "        return np.argmax(Q)\n",
    "\n",
    "agent = BanditAgent(arms, policy=EpsilonGreedyPolicy(epsilon=0.1), eta=0.1)\n",
    "agent.run(steps=500)\n",
    "\n",
    "for i, arm in enumerate(arms):\n",
    "    print(f\"Arm {i} ({arm.__class__.__name__:12s}): \"\n",
    "          f\"pulled {arm.pull_count:3d} times, \"\n",
    "          f\"avg reward = {arm.average_reward():.2f}\")\n",
    "print(f\"\\nAgent Q-values: {agent.Q.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to think about inheritance:**\n",
    "\n",
    "- Start by writing your classes *without* inheritance. If you notice that two classes have identical methods, that is a signal.\n",
    "- Extract the shared code into a base class. The base class represents *what all variants have in common*.\n",
    "- Each subclass adds only what makes it *different* — in this case, a different `_sample_reward` method.\n",
    "- `super().__init__()` calls the base class constructor, which sets up the shared attributes (`pull_count`, `total_reward`). The subclass constructor then adds its own attributes (`true_mean`, or `p_success` and `magnitude`).\n",
    "- `raise NotImplementedError` in the base class is a safety net: if someone creates a `BaseBanditArm()` directly and tries to call `pull()`, they get a clear error message instead of silent misbehaviour.\n",
    "\n",
    "Notice that `pull()` in the base class calls `self._sample_reward()` — and Python automatically runs the version defined in the subclass. This means the base class can define the *structure* of an operation (update bookkeeping, then generate a reward) while leaving the *specifics* to each subclass. This pattern appears throughout machine learning libraries: for example, in PyTorch, the base class `nn.Module` defines the training infrastructure, and your subclass provides `forward()` — exactly the same idea as `BanditArm` providing `pull()` while subclasses provide `_sample_reward()`.\n",
    "\n",
    "Also notice that the `BanditAgent` from Section 6 works with *any* arm type — Gaussian, binary, or a mix — because it only ever calls `pull()`. It does not need to know or care how the arm generates its rewards. This is one of the practical payoffs of inheritance: new arm types can be added without changing the agent at all.\n",
    "\n",
    "**When to use inheritance vs. not:**\n",
    "\n",
    "Inheritance is helpful when you have several classes that genuinely share a common interface and significant shared logic. It is *not* worth it when the shared code is just a line or two — in that case, a small amount of duplication is clearer than an inheritance hierarchy. In the notebooks, the simpler exercises use self-contained classes (no inheritance), while the more advanced solutions may use it where the shared logic is substantial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Documenting Your Classes\n",
    "\n",
    "Good documentation makes code readable months later. The convention used throughout the notebooks is:\n",
    "\n",
    "```python\n",
    "class SingleNeuronModel:\n",
    "    \"\"\"Single-layer neural model with sigmoid activation.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            output_dim (int): Number of output units.\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.b = np.zeros(output_dim)\n",
    "\n",
    "    def train(self, X, y_t, learning_rate, epochs):\n",
    "        \"\"\"Train the model using gradient descent on squared error loss.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray): Input data, shape (batch_size, input_dim).\n",
    "            y_t (ndarray): Target values, shape (batch_size, output_dim).\n",
    "            learning_rate (float): Step size for parameter updates.\n",
    "            epochs (int): Number of passes over the data.\n",
    "        \"\"\"\n",
    "        ...\n",
    "```\n",
    "\n",
    "Each class has a one-line summary. Each method that takes non-obvious arguments has an `Args:` section listing them with types and brief descriptions. Methods that return something have a `Returns:` section. This is not busywork — it is how you communicate what a function expects and what it gives back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Naming Conventions\n",
    "\n",
    "Python uses a simple convention for method names:\n",
    "\n",
    "- **`method_name`**: a normal method, part of the class's public interface. Other code is expected to call it.\n",
    "- **`_method_name`** (single leading underscore): a *helper* method intended for internal use within the class. Other code *can* call it, but the underscore signals \"this is an implementation detail, not part of the main interface.\"\n",
    "\n",
    "Later in the course we will combine artificial neural networks with reinforcement learning to build more powerful agents. In those notebooks, you will see classes like this:\n",
    "\n",
    "```python\n",
    "class LinearTrackQAgent:\n",
    "\n",
    "    def _one_hot(self, state):\n",
    "        \"\"\"Encode a state index as a one-hot vector. Internal helper.\"\"\"\n",
    "        x = np.zeros((1, self.states))\n",
    "        x[0, state] = 1\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action (public interface).\"\"\"\n",
    "        X = self._one_hot(state)\n",
    "        ...\n",
    "```\n",
    "\n",
    "The underscore on `_one_hot` tells the reader: \"this is a utility used inside the class; you don't need to call it directly.\" It keeps the public interface clean — users of the class only need to think about `select_action`, `update_Q`, and `train`. If you understand the patterns in this notebook, you already understand how libraries like PyTorch — a key tool for modern machine learning — are structured (see Section 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. From Here to Professional ML Libraries\n",
    "\n",
    "Later in the course we will combine artificial neural networks with the reinforcement learning techniques you have already seen, producing agents that can handle much larger and more complex problems. The class `SingleNeuronModel` in the table below is a preview of those later notebooks — you do not need to understand its internals yet.\n",
    "\n",
    "The reason we mention it here is that the OOP patterns you have just learnt are the same patterns used by professional machine learning libraries such as PyTorch. If you understand `__init__`, `self`, inheritance, and composition as covered in this notebook, you already understand how PyTorch is structured — and the transition to it will feel natural.\n",
    "\n",
    "For example, here is how the patterns map onto PyTorch:\n",
    "\n",
    "| Notebooks | PyTorch equivalent |\n",
    "|---|---|\n",
    "| `class SingleNeuronModel:` | `class MyModel(nn.Module):` |\n",
    "| `def __init__(self, ...):` | `def __init__(self, ...):` |\n",
    "| `self.W = np.random.randn(...) * 0.01` | `self.W = nn.Parameter(torch.randn(...) * 0.01)` |\n",
    "| `def output(self, X):` | `def forward(self, X):` |\n",
    "| `return self.activation(X @ self.W + self.b)` | `return torch.sigmoid(X @ self.W + self.b)` |\n",
    "\n",
    "In PyTorch, `nn.Module` is a **base class** — exactly like the `BanditArm` base class in Section 7. Writing `class MyModel(nn.Module)` creates a subclass. Calling `super().__init__()` runs the base class constructor. Defining `forward()` overrides a method — just like overriding `select_action()`.\n",
    "\n",
    "You do not need to know PyTorch for this course. The point is simply that getting comfortable with `__init__`, `self`, methods, and composition now means you are already learning the structure that these widely-used tools are built on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary: A Decision Guide\n",
    "\n",
    "When writing code for a new problem, ask yourself these questions in order:\n",
    "\n",
    "**1. Does this thing need to remember something?**\n",
    "- *No* → write a **function**.\n",
    "- *Yes* → it should probably be a **class**. Continue to question 2.\n",
    "\n",
    "**2. What are this object's responsibilities?**\n",
    "- List the *data* it needs to store (→ attributes in `__init__`).\n",
    "- List the *operations* it performs on that data (→ methods).\n",
    "- If something doesn't need the object's data, it might belong as a standalone function instead.\n",
    "\n",
    "**3. Am I writing two classes with a lot of identical code?**\n",
    "- *No* → keep them as separate, self-contained classes. Simplicity wins.\n",
    "- *Yes* → consider extracting the shared logic into a **base class** and using inheritance.\n",
    "\n",
    "**4. Does one object need to *use* another?**\n",
    "- Store a reference: `self.env = env`. This is **composition**. The two objects communicate through method calls (`self.env.step(action)`) rather than sharing internal state.\n",
    "\n",
    "**5. Should a component be swappable?**\n",
    "- Pass it in as an argument: `self.policy = policy`. This keeps your classes flexible — you can change behaviour without rewriting the class.\n",
    "\n",
    "The techniques introduced here — classes, composition, inheritance, and the function/class distinction — cover everything you will encounter in the reinforcement learning notebooks and provide a foundation for working with modern machine learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Official Python Tutorial — Classes](https://docs.python.org/3/tutorial/classes.html): comprehensive reference for all class features in Python.\n",
    "- [NumPy Documentation](https://numpy.org/doc/): reference for the array operations used throughout.\n",
    "- [Gymnasium Documentation](https://gymnasium.farama.org/): the most widely used RL environment library — uses the same Environment class pattern as these notebooks.\n",
    "- [PyTorch — What is `nn.Module`?](https://pytorch.org/docs/stable/generated/torch.nn.Module.html): for those interested in seeing how a major ML library builds on the OOP patterns covered here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}