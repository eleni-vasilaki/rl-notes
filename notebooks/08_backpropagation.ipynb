{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d59942-21e4-43f8-a248-2b4e800e6261",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks as Universal Function Approximators II\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/08_backpropagation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632004a3-54ec-48a9-9d52-4569218cefdb",
   "metadata": {},
   "source": [
    "## Function Approximation with Deep Networks\n",
    "\n",
    "The world we live in is a complex one. There is so much detail in everything that it becomes impossible for us to capture it all precisely.\n",
    "\n",
    "When I was little, I was fascinated by Agatha Christie’s books. In them, there’s a recurring character — an old lady who solves mysteries simply by observing human nature. She notices behaviours, compares them to people she already knows, and uses that resemblance to make sense of what’s happening. In other words, she interpolates: she takes a new situation and maps it onto familiar patterns from her past data.\n",
    "\n",
    "What we’re going to do here is somewhat analogous — but we’ll formalise it.\n",
    "\n",
    "In reinforcement learning, we often use a Q-table: a two-dimensional table indexed by states and actions, storing values that tell us how good a given action is in a given state. This works when the number of states and actions is small. But as soon as the state space becomes large — or worse, continuous — the table becomes intractable, and to put it bluntly, useless. The most challenging reinforcement problems today cannot be solved with Q-tables.\n",
    "\n",
    "This is where neural networks become indispensable.\n",
    "\n",
    "Until now, we’ve only worked with perceptrons — shallow, linear models. But to replace the Q-table in challenging problems, we’ll need something more expressive: deep networks.\n",
    "\n",
    "Forget everything you’ve heard about deep learning — or at least, set it aside for now. Don’t think of neural networks as classifiers, or clusterers, or compressors. Think of them for what they fundamentally are: function approximators.\n",
    "\n",
    "That’s their true power.\n",
    "\n",
    "The function we’re going to approximate is the Q-function — which, in tabular settings, is represented as a Q-table. That is, the mapping from a given state-action pair to an expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789c34f",
   "metadata": {},
   "source": [
    "## From Perceptrons to Deeper Networks\n",
    "\n",
    "In previous lectures, we introduced the delta rule — the update rule for the weights of a single neuron, which we freely referred to as a perceptron.\n",
    "\n",
    "A set of perceptrons — each making independent decisions — can actually solve surprisingly non-trivial problems. A good example is the MNIST dataset. You’re likely familiar with it: it’s one of the most widely used (and overused) benchmarks in machine learning. While it was quite impressive when introduced, it now mostly belongs to education, prototyping, and early-stage testing.\n",
    "\n",
    "For those who haven’t seen it, MNIST consists of images of digits 0 through 9, handwritten by different people and scaled to a uniform size. The task is to build a system — typically a neural network — that can correctly classify each image as the digit it represents.\n",
    "\n",
    "Now, if you try to tackle this using just 10 perceptrons — one for each digit — you’ll find that you can already achieve around 70% accuracy, or a bit more depending on preprocessing. That may sound surprising: this seems like a hard problem, but it turns out to be easier than it looks.\n",
    "\n",
    "The reason lies in what perceptrons do. At their core, perceptrons draw hyperplanes — they divide the input space with straight lines (or planes, or hyperplanes, depending on dimensionality). If your data can be separated using such divisions, the problem is called linearly separable.\n",
    "\n",
    "Let’s look at a very simple example: the logical OR gate, which we discussed before. The input consists of pairs of binary values — (0, 0), (0, 1), (1, 0), and (1, 1). The output is 1 if at least one of the inputs is 1. If we plot these four points in the 2D input space, we can clearly draw a single line that separates the class 0 (which occurs only at (0, 0)) from the three other points that belong to class 1. This is a textbook example of a linearly separable problem.\n",
    "\n",
    "Now let’s change the problem just slightly — it remains simple, but it introduces a fundamentally different geometric property. Consider the XOR gate. The input space is the same: (0, 0), (0, 1), (1, 0), and (1, 1). But now the output is 1 only when the inputs differ. In this case, no single line can separate the classes. You would need at least two lines — meaning, a combination of decision boundaries.\n",
    "\n",
    "This makes XOR one of the simplest examples of a non-linearly separable problem — and it reveals the limitation of single-layer perceptrons. You need more structure to solve it.\n",
    "\n",
    "And that structure comes from depth. That is, neurons arranged not just side by side, but in layers.\n",
    "\n",
    "Let’s revisit the structure of a neuron. Its input is a linear summation: the weighted sum of the input values. But there’s one essential additional ingredient: a nonlinear activation function. That nonlinearity is critical.\n",
    "\n",
    "Without it, it’s straightforward to show — and easy to construct counterexamples — that a network with multiple layers of purely linear operations collapses into a single linear layer. In other words, without nonlinearity, depth adds no expressive power. Nonlinearity may not be the only ingredient in modern neural networks’ success, but it is unquestionably a key one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9f5a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Plot XOR data points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1],\n",
    "            c='blue', marker='o', label='Class 0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1],\n",
    "            c='red',  marker='s', label='Class 1')\n",
    "\n",
    "# Diagonal separators spanning the whole plotting area\n",
    "xs = np.linspace(-0.1, 1.1, 200)\n",
    "plt.plot(xs, -xs + 0.5, '--', color='green')\n",
    "plt.plot(xs, -xs + 1.5, '--', color='purple')\n",
    "\n",
    "# Set limits with a margin and enforce square aspect ratio\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(-0.1, 1.1)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Final touches\n",
    "plt.title('XOR Problem (Non-linearly separable)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b3aff",
   "metadata": {},
   "source": [
    "# Exercise  \n",
    "\n",
    "You are going to demonstrate the importance of the non-linear activation function through a simple example.  \n",
    "Assume a small feedforward neural network with two input units, two hidden units, and one output unit.  \n",
    "For simplicity, we will call the input units $x_1$ and $x_2$, the hidden units $x_3$ and $x_4$, and the output unit $x_5$.  \n",
    "Their corresponding weights will be as we have defined them earlier: $w_{31}$ is the weight from unit 1 to 3, $w_{41}$ from unit 1 to 4, etc.  \n",
    "\n",
    "- Calculate the output unit as a function of the input units.  \n",
    "- Then, define a neural network with two input units and one output that, with different weights, can perform the same task. What is the mathematical relationship between the weights in the shallow network and the original ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5237f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "**Network Structure:**\n",
    "- **Input Units**: $x_1$ and $x_2$\n",
    "- **Hidden Units**: $x_3$ and $x_4$\n",
    "- **Output Unit**: $x_5$\n",
    "\n",
    "**Weights:**\n",
    "- $w_{31}$ is the weight from $x_1$ to $x_3$\n",
    "- $w_{32}$ is the weight from $x_2$ to $x_3$\n",
    "- $w_{41}$ is the weight from $x_1$ to $x_4$\n",
    "- $w_{42}$ is the weight from $x_2$ to $x_4$\n",
    "- $w_{53}$ is the weight from $x_3$ to $x_5$\n",
    "- $w_{54}$ is the weight from $x_4$ to $x_5$\n",
    "\n",
    "**Biases** (omitted for simplicity in equations):\n",
    "- $b_3$, $b_4$, and $b_5$\n",
    "\n",
    "### Calculate the Output Using Linear Activations\n",
    "\n",
    "If all activation functions are linear (i.e., identity functions where the output equals the input), the outputs of the hidden units $x_3$ and $x_4$ are linear combinations of the inputs:\n",
    "\n",
    "$$x_3 = w_{31} x_1 + w_{32} x_2$$\n",
    "$$x_4 = w_{41} x_1 + w_{42} x_2$$\n",
    "\n",
    "The output of the network, $x_5$, being a linear combination of the outputs from the hidden units, is:\n",
    "\n",
    "$$x_5 = w_{53} x_3 + w_{54} x_4$$\n",
    "\n",
    "Substituting the values of $x_3$ and $x_4$ from the above, we get:\n",
    "\n",
    "$$x_5 = (w_{53} w_{31} + w_{54} w_{41}) x_1 + (w_{53} w_{32} + w_{54} w_{42}) x_2$$\n",
    "\n",
    "Let:\n",
    "- $W_1 = w_{53} w_{31} + w_{54} w_{41}$\n",
    "- $W_2 = w_{53} w_{32} + w_{54} w_{42}$\n",
    "\n",
    "Thus, $x_5$ simplifies to:\n",
    "$$x_5 = W_1 x_1 + W_2 x_2$$\n",
    "\n",
    "### Redefine a Simpler Network\n",
    "\n",
    "The multi-layer linear network essentially performs as a single-layer linear network with the inputs directly connected to the output through new weights $W_1$ and $W_2$.\n",
    "\n",
    "Equivalent Weights for the Single-Layer Network\n",
    "\n",
    "For a single-layer network performing the same task, you directly connect $x_1$ and $x_2$ to $x_5$ with weights $W_1$ and $W_2$. These weights are equivalent to the combinations derived from the two-layer network:\n",
    "\n",
    "$$W_1 = w_{53} w_{31} + w_{54} w_{41}$$\n",
    "$$W_2 = w_{53} w_{32} + w_{54} w_{42}$$\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Without non-linear activation functions, adding more layers does not increase the model's capacity to learn complex patterns since it only extends the linear transformation, not contributing to additional complexity or abstraction capability. Non-linear activations enable neural networks to learn non-linear relationships, crucial for tasks like classifying non-linearly separable patterns such as XOR.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ac900",
   "metadata": {},
   "source": [
    "## Forward and Backward Passes\n",
    "\n",
    "In practical terms, the operation of a neural network consists of two main phases:\n",
    "\n",
    "1. Forward Pass:\n",
    "Starting from the input layer, we propagate activity forward through the network. At each layer, we compute the weighted sum of the inputs to each neuron, apply a nonlinear activation function, and pass the result to the next layer. This process continues until we reach the output layer, where the network produces a prediction. At this point, we compute the loss — the discrepancy between the predicted output and the target.\n",
    "\n",
    "2. Backward Pass:\n",
    "Once the error is known, we propagate information backwards through the network in order to update the model’s parameters. Using the chain rule, we compute the gradient of the loss with respect to each parameter — weights and biases — and adjust them accordingly. This gradient tells us how to change each parameter to reduce the overall error, allowing the network to learn from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e426802",
   "metadata": {},
   "source": [
    "## Backpropagation and the Chain Rule\n",
    "\n",
    "In order to derive the backpropagation update rules, we need to refer to the chain rule of differentiation. This is essential because in a multi-layer network, the effect of any one weight on the loss is indirect — it is propagated through several layers of neurons and connections, eventually influencing the output.\n",
    "\n",
    "In this course, we have aimed to use only the mathematical tools we have introduced and proved explicitly — primarily high-school or A-level mathematics — but with a strong emphasis on practical understanding over rote memorisation. The chain rule is one such tool. While it is often introduced as a simple identity, it carries important structural assumptions that are often overlooked in simpler problems.\n",
    "\n",
    "When it comes to neural networks, the naïve application of the chain rule — as seen in many online derivations — often leads to sloppy or incorrect results. These derivations sometimes gesture at the flavour of backpropagation without providing a rigorous path from first principles.\n",
    "\n",
    "To do this properly, we must recognise that the dependency structure of the loss is layered. Output neurons depend on weights from the previous layer, which in turn depend on earlier layers, and so on. The cleanest way to express this is via matrix calculus, taking derivatives with respect to entire vectors or matrices. However, this goes beyond the standard background assumed in this course, and therefore, we will not adopt it here. (A future version of these notes may include it.)\n",
    "\n",
    "Instead, we will keep our commitment to a principled derivation using only the mathematical tools we’ve already established. And to do so, we must now carefully generalise our earlier thinking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632b568",
   "metadata": {},
   "source": [
    "## General Setup: The Chain Rule Applied to a Loss Function\n",
    "\n",
    "Let us begin by considering a generic loss function, denoted by $L$. This function quantifies how well the network performs. We will first express $L$ in its online version, that is, the error computed for a single pattern (or data point). In practice, we often sum the errors over a batch of inputs, but we can safely defer that until the end. This is because:\n",
    "\n",
    "The derivative of a sum is the sum of the derivatives.\n",
    "\n",
    "So, for derivation purposes, we are justified in starting from a single pattern, then generalising.\n",
    "\n",
    "The original backpropagation paper by Rumelhart, Hinton, and Williams presented the algorithm in exactly this online setting. (As you likely know, Geoffrey Hinton has since been awarded the Nobel Prize in Physics for his contributions to this area.)\n",
    "\n",
    "Let’s define our total loss over a batch $X$ as:\n",
    "\n",
    "$$\n",
    "L = \\sum_{\\mathbf{x}} L_o(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "where $L_o$ is the online loss corresponding to a single data point $\\mathbf{x}$.\n",
    "\n",
    "Now suppose we are interested in computing the derivative of the total loss with respect to a particular weight $w_{ij}$ (e.g. from neuron $j$ to neuron $i$). By linearity of differentiation, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{ij}} = \\sum_{\\mathbf{x} } \\frac{\\partial L_o(\\mathbf{x})}{\\partial w_{ij}}.\n",
    "$$\n",
    "\n",
    "This is a structurally correct starting point for our derivation. We now focus on computing $\\frac{\\partial L_o}{\\partial w_{ij}}$ for a single datapoint $\\mathbf{x}$ using the chain rule. This requires us to track how changes in the weight affect the output neuron, and how that in turn affects the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf6269",
   "metadata": {},
   "source": [
    "## Gradient Flow Through the Network\n",
    "\n",
    "To compute this gradient, we begin by considering the derivative of the online loss function with respect to the parameter that we’re updating.\n",
    "\n",
    "We start by recalling that the loss function $L_o$ is a scalar. It measures the error — which, in our case, is effectively a sum across the errors of each output neuron.\n",
    "\n",
    "So we begin by considering the partial derivative of the online loss with respect to a single output neuron, say $y_i$, where $i$ indexes neurons in the output layer. That derivative,\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial y_i},\n",
    "$$\n",
    "tells us the direction in which $y_i$ would have to move to reduce the error. But — crucially — we never directly change the value of a neuron. What we change are the parameters of the network — weights and biases — which in turn affect the neuron’s output.\n",
    "\n",
    "Therefore, we need to apply the chain rule to find the effect of the weight we want to change to the specific neuron $y_i$:\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial w_{ij}^{(n)}},\n",
    "$$\n",
    "where $w_{ij}^{(n)}$ is the weight connecting neuron $j$ in the penultimate layer to output neuron $i$. Here, $j$ indexes the $(n-1)$-th layer — the one before the output — and we explicitly use the superscript $(n)$ to track layer depth.\n",
    "\n",
    "We might also consider a deeper weight, for example $w_{jk}^{(n-1)}$, which connects neuron $k$ in the $(n-2)$-th layer to neuron $j$ in the $(n-1)$-th. Then we compute:\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial w_{jk}^{(n-1)}},\n",
    "$$\n",
    "with layer indices keeping our notation unambiguous.\n",
    "\n",
    "These expressions tell us how a particular weight affects a single output neuron’s error. In other words, they give the gradient for that output only. Each output neuron would yield a different gradient — a different direction — for improving its own error.\n",
    "\n",
    "But we care about improving the total error, not just that of a single neuron.\n",
    "\n",
    "When a weight connects directly to the output layer, there is nothing between the weight and the output neuron it influences. In fact, the gradient of any other neuron apart from the specific output neuron of the weight is zero. So no special attention is needed there, as the effect of the weight on the error is via a single neuron.\n",
    "\n",
    "But when the weight belongs to a deeper layer — say, the penultimate one — there are intermediate neurons in the chain of influence. We can still apply the chain rule as long as there is a direct, uninterrupted dependency path.\n",
    "\n",
    "So to find the direction that improves performance across all outputs, we sum over all $i$:\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{ab}^{(n)}} = \\sum_i \\left( \\frac{\\partial L_o(x)}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial w_{ab}^{(n)}} \\right),\n",
    "$$\n",
    "where $w_{ab}$ is the weight we’re considering (and will vary depending on which layer we’re analysing).\n",
    "\n",
    "Because we ultimately care about the total loss across a batch:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{ab}^{(n-1)}} = \\sum_x \\sum_i \\left( \\frac{\\partial L_o(x)}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial w_{ab}^{(n-1)}} \\right).\n",
    "$$\n",
    "\n",
    "Let’s now consider a weight $w_{jk}^{(n-1)}$, which connects neuron $k$ in layer $(n-2)$ to neuron $j$ in layer $(n-1)$. That penultimate neuron, $x_j^{(n-1)}$, influences all output neurons $y_i$ in the final layer. Since the connections are feedforward and differentiable, and since we capture the effect of $x_j^{(n-1)}$ on all $y_i$ via the summation, we can safely expand the derivative along this full causal path.\n",
    "\n",
    "In that case, the gradient becomes:\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{jk}^{(n-1)}} = \\sum_i \\left( \\frac{\\partial L_o}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x_j^{(n-1)}} \\cdot \\frac{\\partial x_j^{(n-1)}}{\\partial w_{jk}^{(n-1)}} \\right)\n",
    "$$\n",
    "\n",
    "This chain shows the full influence of the deeper weight on the loss via the intermediate activation $x_j^{(n-1)}$ and its effect on each output $y_i$. Since the weight directly controls the hidden neuron, and that neuron directly affects the outputs, no dependencies are omitted — the chain is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9553f",
   "metadata": {},
   "source": [
    "### Example: Backpropagation calculation for a specific loss function\n",
    "\n",
    "We will now illustrate how these chain rule principles apply in practice by computing gradients for the summed squared error (SSE) loss:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\sum_{\\bf x } \\sum_{i} \\left(y^*_i({\\bf x}) - y_i({\\bf x}) \\right)^2.\n",
    "$$\n",
    "\n",
    "Using its online form:\n",
    "\n",
    "$$\n",
    "L_o = \\frac{1}{2}  \\sum_{i} \\left(y^*_i({\\bf x}) - y_i({\\bf x}) \\right)^2, \n",
    "$$\n",
    "\n",
    "we can write:\n",
    "\n",
    "$$\n",
    "L = \\sum_{\\bf{x}} L_o \n",
    "$$\n",
    "\n",
    "where ${\\bf x}$ is an index over the input (batch or datapoints), $i$ an index on output neurons, $y^*_i({\\bf x})$ is the desirable value of the output neuron $i$ for the datapoint ${\\bf x}$, and $y_i({\\bf x})$ the output neuron $i$ for the datapoint ${\\bf x}$. We seek to adapt the parameters $w_{ij}$ and $b_i$ in order to minimise the error. In general the weight update for one layer is given by:\n",
    "$$\n",
    "\\Delta w_{ij}^{k} = - \\eta \\frac{\\partial L}{\\partial w_{ij}^{k} }=  - \\eta \\sum_{\\bf x} \\frac{\\partial L_o}{\\partial w_{ij}^{k} }, \n",
    "$$\n",
    "where $\\eta$ is the ``learning rate'', a parameter that affects the size of the weight updates, and is typically assigned a small value,  $k$ an index over layers and $\\Delta w_{ij}^{k}$ the weight update from neuron $j$ to neuron $i$ for layer $k$. For the last layer $k=n$. You are reminded that $\\Delta w_{ij}^{k}$ denotes the difference between the previous weight and the new weight, hence:\n",
    "\n",
    "$$\n",
    "new~w_{ij}^{k}=  \\Delta w_{ij}^{k} +   old~w_{ij}^{k}.\n",
    "$$\n",
    "\n",
    "Similarly for the bias $b_{i}^{k} $:\n",
    "\n",
    "$$\n",
    "\\Delta b_{i}^{k} = - \\eta \\frac{\\partial L_o}{\\partial b_{i}^{k} }, \n",
    "$$\n",
    "\n",
    "Because the bias can be treated as a special weight with fixed input 1, we use the same equations for the weight updates, setting the input to 1. Note that since there is no ambiguity about the input of the bias, only one index, that of the neuron is required.\n",
    "\n",
    "To calculate this derivative, and accommodate for many layers, we recall that the neurons of the output layer are calculated by:\n",
    "\n",
    "$$\n",
    "y_i({\\bf x})=f\\left( \\sum_j w^{(n)}_{ij} x^{(n-1)}_j({\\bf x})\\right),\n",
    "$$\n",
    "where the neurons within the penultimate layer are denoted as $x_j^{(n-1)}({\\bf x})$, and $f$ is the activation function. We define the pre-activation function $h_i$ as:\n",
    "$$\n",
    "h_i^{(n)}= \\sum_j w^{(n)}_{ij} x^{(n-1)}_j ({\\bf x})\n",
    "$$\n",
    "For this derivation we will assume a sigmoidal activation function, i.e. $f(x)=\\frac{1}{1+e^{-x}}.$ \n",
    "\n",
    "We will calculate updates for the weights projecting to the output neurons:\n",
    "$$\n",
    "\\Delta w_{ij}^{(n)} = - \\eta \\frac{\\partial L_o}{\\partial w_{ij}^{(n)} }.\n",
    "$$\n",
    "\n",
    "Applying the chain rule blindly is, in general, problematic because one may miss out on the indirect effect of a hidden weight to the output neurons via other neurons. This risk though is not relevant for the weights to the output layer, as they contribute to only one neuron. Therefore we may apply the chain rule directly without further considerations. For the hidden layers, we will be more careful on how to apply the rule.\n",
    "\n",
    "In what follows we will drop the explicit notation of $({\\bf x})$ and remember that all neuron values and desirable outputs depend on the input.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{ij}^{(n)} } = \\frac{\\partial \\left(y^*_i - y_i \\right)^2} {\\partial y_i} \\frac{\\partial \\left( y_i \\right)} {\\partial w_{ij}^{(n)} }  \n",
    "$$\n",
    "$$\n",
    "= -  \\left(y^*_i - y_i \\right) \\frac{\\partial  y_i } {\\partial w_{ij}^{(n)} } \n",
    "$$\n",
    "$$ \n",
    "= -  \\left(y^*_i - y_i\\right) \\frac{\\partial  f\\left( \\sum_{j} w^{(n)}_{ij} x^{(n-1)}_j \\right) } {\\partial w_{ij}^{(n)} }\n",
    "$$\n",
    "\n",
    "We now focus on the derivative on the right-hand side of the equation. We will make use of the chain rule again [Reminder: $f'= f(1-f)$]:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial  f\\left( \\sum_j w^{(n)}_{ij} x^{(n-1)}_j \\right)}  {\\partial w_{ij}^{(n)} }  =   f(\\sum_j w^{(n)}_{ij} x^{(n-1)}_j )   \\left(1-f(\\sum_j w^{(n)}_{ij} x^{(n-1)}_j)  \\right) x^{(n-1)}_j  \n",
    "$$\n",
    "\n",
    "$$\n",
    "=  {f_i^\\prime}^{(n)} x^{(n-1)}_j.\n",
    "$$\n",
    "\n",
    "Combining our previous results, the weight update rule for the output layer is:\n",
    "$$\n",
    "\\Delta w_{ij}^{(n)} = \\eta  \\left(y^*_i - y_i \\right) {f_i^\\prime}^{(n)}  x^{(n-1)}_j \n",
    "$$\n",
    "\n",
    "To write the expression in a more compact way, we can define:\n",
    "$$\n",
    "\\delta_{i}^{(n)} =  \\left(y^*_i - y_i \\right) {f_i^\\prime}^{(n)} \n",
    "$$\n",
    "and finally the weight update rule for the output layer becomes:\n",
    "$$\n",
    "\\Delta w_{ij}^{(n)} = \\eta  \\delta_{i}^{(n)} x^{(n-1)}_j \n",
    "$$\n",
    "\n",
    "The update rule for the biases follows directly from the weight updates by treating each bias as a weight with a fixed input of 1. Since the bias $b_i^{(n)}$ affects only the activation of neuron $i$ in layer $n$, its gradient is simply the corresponding error term $\\delta_i^{(k)}$. Therefore, the update rule for the bias becomes:\n",
    "$$\n",
    "\\Delta b_i^{(n)} = \\eta  \\delta_i^{(n)}.\n",
    "$$\n",
    "\n",
    "\n",
    "To calculate the weight update for the weights to the hidden layer(s):\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{jl}^{(n-1)} }  = \\frac{\\partial L}{\\partial x_j^{(n-1)}} \\frac{\\partial x_j^{(n-1)}}{\\partial w_{jl}^{(n-1)} }.\n",
    "$$\n",
    "This is a correct application of the chain rule because we have expressed the chain rule via the variable $ x_j^{(n-1)}$ which is directly linked to the weight $w_{jl}^{(n-1)}$ so we do not omit any ``indirect'' pathways. Now we can calculate each partial derivative on the right-hand side separately:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j^{(n-1)}}  =  \\frac{1}{2} \\sum_{i} \\frac{\\partial \\left(y^*_i - y_i \\right)^2} {\\partial x_j^{(n-1)}}  = - \\sum_{i} \\left(y^*- y_i \\right) \\frac{\\partial y_i}{\\partial x_j^{(n-1)}},\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j^{(n-1)}} =  {f_i^\\prime}^{(n)}  w_{ij}^{(n)} \n",
    "$$\n",
    "and:\n",
    "$$\n",
    "\\frac{\\partial x_j^{(n-1)}}{\\partial w_{jl}^{(n-1)} } = {f_j^\\prime}^{(n-1)} x_l^{(n-2)}.\n",
    "$$\n",
    "\n",
    "Putting all together:\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = - \\eta  \\frac{\\partial L}{\\partial w_{jl}^{(n-1)} } =   \\eta \\sum_{i}  \\left(y^*_i - y_i \\right) {f_i^\\prime}^{(n)} w_{ij}^{(n)} {f_j^\\prime}^{(n-1)} x_l^{(n-2)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = \\eta \\sum_{i} \\delta_i^{(n)} w_{ij}^{(n)} {f_j^\\prime}^{(n-1)} x_l^{(n-2)} \n",
    "$$\n",
    "\n",
    "which can be written in a more elegant form if we define:\n",
    "\n",
    "$$\n",
    "\\delta_{j}^{(n-1)} = \\sum_{i} \\delta_i^{(n)} w_{ij}^{(n)} {f_j^\\prime}^{(n-1)},\n",
    "$$\n",
    "\n",
    "leading to:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = \\eta \\delta_{j}^{(n-1)} x_l^{(n-2)} \n",
    "$$\n",
    "\n",
    "In order to generalise the equations for more layers we note that structurally every hidden layer is identical to all others. we use $k$ to denote the layer for which we calculate the weight update, and $k$ the layer that $k-1$ connects to, with $k\\leq n$. For the hidden layers:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(k)} = \\eta \\delta_{j}^{(k)} x^{(k-1)}_l,\n",
    "$$\n",
    "\n",
    "We treat each bias as a weight with a fixed input of 1. Since the bias $b_j^{(k)}$ affects only the activation of neuron $j$ in layer $k-1$, its gradient is simply the corresponding error term $\\delta_j^{(k-1)}$. Therefore, the update rule for the bias becomes:\n",
    "$$\n",
    "\\Delta b_j^{(k)} = \\eta  \\delta_j^{(k)}.\n",
    "$$\n",
    "\n",
    "Therefore, starting from the output the error is ``backpropagated'' towards the input adjusting appropriately the parameters of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a07686",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "We consider a neural network trained using:\n",
    "\n",
    "- **Mean Squared Error (MSE) loss** in batch form:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum_{\\mathbf{x}} \\sum_i \\left( y^*_i(\\mathbf{x}) - y_i(\\mathbf{x}) \\right)^2,\n",
    "$$\n",
    "\n",
    "where $N$ is the batch size and $\\mathbf{x}$ representing the datapoints of the batch. \n",
    "\n",
    "- **ReLU activation function**:\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x), \\qquad f'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "= H(x)\n",
    "$$\n",
    "\n",
    "As a reminder,  the derivative of the ReLU activation is undefined at $x=0$, and as a convention we assign value 0.\n",
    "\n",
    "Using the methodology of the previous example, calculate the backpropagation update rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e3e7e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "We define the online version of the loss for a single datapoint:\n",
    "\n",
    "$$\n",
    "L_o = \\frac{1}{2} \\sum_i \\left( y^*_i - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "The batch version is recovered as:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{\\mathbf{x}} L_o(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "We seek to adapt the parameters $w_{ij}$ and $b_i$ in order to minimise the error. In general, the weight update for one layer is given by:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}^{(k)} = - \\eta \\frac{\\partial L}{\\partial w_{ij}^{(k)} } = -\\eta \\sum_{\\mathbf{x}} \\frac{\\partial L_o}{\\partial w_{ij}^{(k)} }\n",
    "$$\n",
    "\n",
    "and similarly for the bias:\n",
    "\n",
    "$$\n",
    "\\Delta b_i^{(k)} = -\\eta \\frac{\\partial L_o}{\\partial b_i^{(k)} }\n",
    "$$\n",
    "\n",
    "\n",
    "We now derive the gradient for the output layer. Let:\n",
    "\n",
    "$$\n",
    "y_i = f\\left( \\sum_j w_{ij}^{(n)} x_j^{(n-1)} \\right), \\qquad f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "h_i^{(n)} = \\sum_j w_{ij}^{(n)} x_j^{(n-1)}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "y_i = f(h_i^{(n)}), \\qquad f’(h_i^{(n)}) = H(h_i^{(n)})\n",
    "$$\n",
    "\n",
    "We now compute the full derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{ij}^{(n)}} = \\frac{\\partial}{\\partial w_{ij}^{(n)}} \\left[ \\frac{1}{2} \\sum_i (y^*i - y_i)^2 \\right]\n",
    "= \\frac{\\partial L_o}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial w{ij}^{(n)}}\n",
    "$$\n",
    "\n",
    "We compute each term separately:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial y_i} = - (y^*_i - y_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial w_{ij}^{(n)}} = \\frac{\\partial f(h_i^{(n)})}{\\partial h_i^{(n)}} \\cdot \\frac{\\partial h_i^{(n)}}{\\partial w_{ij}^{(n)}}\n",
    "= H(h_i^{(n)}) \\cdot x_j^{(n-1)}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{ij}^{(n)}} = - (y^*_i - y_i) H(h_i^{(n)}) x_j^{(n-1)}\n",
    "$$\n",
    "\n",
    "Weight update becomes:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}^{(n)} = - \\eta \\frac{\\partial L_o}{\\partial w_{ij}^{(n)}} = \\eta (y^*_i - y_i) H(h_i^{(n)}) x_j^{(n-1)}\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\delta_i^{(n)} = (y^*_i - y_i) H(h_i^{(n)})\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}^{(n)} = \\eta \\delta_i^{(n)} x_j^{(n-1)}, \\qquad \\Delta b_i^{(n)} = \\eta \\delta_i^{(n)}\n",
    "$$\n",
    "\n",
    "Now we compute the update for the hidden layer $n-1$ (penultimate). Recall:\n",
    "\n",
    "$$\n",
    "x_j^{(n-1)} = f(h_j^{(n-1)}), \\qquad h_j^{(n-1)} = \\sum_l w_{jl}^{(n-1)} x_l^{(n-2)}\n",
    "$$\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = -\\eta \\frac{\\partial L_o}{\\partial w_{jl}^{(n-1)}}\n",
    "$$\n",
    "\n",
    "We apply the chain rule carefully, tracing intermediate derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{jl}^{(n-1)}}\n",
    "= \\frac{\\partial L_o}{\\partial x_j^{(n-1)}} \\cdot \\frac{\\partial x_j^{(n-1)}}{\\partial w_{jl}^{(n-1)}}\n",
    "$$\n",
    "\n",
    "We now compute each partial derivative.\n",
    "\n",
    "First,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial x_j^{(n-1)}}\n",
    "= \\sum_i \\frac{\\partial L_o}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x_j^{(n-1)}}\n",
    "= \\sum_i (- (y^*_i - y_i)) \\cdot \\frac{\\partial y_i} {\\partial x_j^{(n-1)}}\n",
    "= \\sum_i (- (y^*_i - y_i)) \\cdot H(h^{(n)}_i) w_{ij}^{(n)}\n",
    "= \\sum_i \\delta_i^{(n)} w{ij}^{(n)}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_j^{(n-1)}}{\\partial w_{jl}^{(n-1)}}\n",
    "= \\frac{\\partial f(h_j^{(n-1)})}{\\partial h_j^{(n-1)}} \\cdot \\frac{\\partial h_j^{(n-1)}}{\\partial w_{jl}^{(n-1)}}\n",
    "= H(h_j^{(n-1)}) \\cdot x_l^{(n-2)}\n",
    "$$\n",
    "\n",
    "Putting all together:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_o}{\\partial w_{jl}^{(n-1)}}\n",
    "= \\left( \\sum_i \\delta_i^{(n)} w_{ij}^{(n)} \\right) H(h_j^{(n-1)}) x_l^{(n-2)}\n",
    "$$\n",
    "\n",
    "Thus, the weight update is:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = \\eta \\left( \\sum_i \\delta_i^{(n)} w_{ij}^{(n)} \\right) H(h_j^{(n-1)}) x_l^{(n-2)}\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\delta_j^{(n-1)} = \\left( \\sum_i \\delta_i^{(n)} w_{ij}^{(n)} \\right) H(h_j^{(n-1)})\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(n-1)} = \\eta \\delta_j^{(n-1)} x_l^{(n-2)}, \\qquad \\Delta b_j^{(n-1)} = \\eta \\delta_j^{(n-1)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The same structure generalises to any hidden layer $k < n$. We define recursively:\n",
    "\n",
    "$$\n",
    "\\delta_j^{(k)} = \\left( \\sum_i \\delta_i^{(k+1)} w_{ij}^{(k+1)} \\right) H(h_j^{(k)})\n",
    "$$\n",
    "\n",
    "and then:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(k)} = \\eta \\delta_j^{(k)} x_l^{(k-1)}, \\qquad \\Delta b_j^{(k)} = \\eta \\delta_j^{(k)}\n",
    "$$\n",
    "\n",
    "Batch form:\n",
    "\n",
    "$$\n",
    "\\Delta w_{jl}^{(k)} = \\frac{\\eta}{N} \\sum_{\\mathbf{x}} \\delta_j^{(k)}(\\mathbf{x}) x_l^{(k-1)}(\\mathbf{x}), \\qquad\n",
    "\\Delta b_j^{(k)} = \\frac{\\eta}{N} \\sum_{\\mathbf{x}} \\delta_j^{(k)}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0f861",
   "metadata": {},
   "source": [
    "## Replacing the Q-values Table with an ANN in Q-learning\n",
    "\n",
    "Single neurons can learn linear relationships and solve linearly separable problems. Consider, for instance, the linear track problem. It is feasible to use a perceptron to replace the Q-values table because the Q-values are expected to increase linearly from the starting state to the penultimate state, after which a reward is collected. Although approximating Q-values with a neural network is not necessary due to the simplicity of the problem, it serves as a useful demonstration.\n",
    "\n",
    "The approach to replace the Q-values table involves using a neural network with two outputs, corresponding to the two possible actions (forward and backward). The input to this network is the state of the agent.\n",
    "\n",
    "We will assume an online setting, with a ReLU activation function for the neurons and the following loss function:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\sum_{a} \\left( y_a^*(s) - y_a(s; W) \\right)^2,\n",
    "$$\n",
    "\n",
    "where the index on the output neurons is $a$ and the input is the state $s$; there are as many neurons as actions available.\n",
    "\n",
    "The outputs of the network represent the Q-values. However, we cannot modify neurons directly; instead, we must adjust their synapses:\n",
    "\n",
    "$$\n",
    "y_a(s) = Q(s, a).\n",
    "$$\n",
    "\n",
    "The network’s target is defined as:\n",
    "\n",
    "$$\n",
    "y_a^*(s) = r + \\gamma \\max_{a'} Q(s', a').\n",
    "$$\n",
    "\n",
    "Here, $s'$ represents the next state, and the index $a'$ denotes an action in the next state.\n",
    "\n",
    "This choice becomes clear if we remind ourselves of the loss function used to derive the Q-learning algorithm:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)^2.\n",
    "$$\n",
    "\n",
    "We treat the previously defined $y_a^*(s)$ as the target output of the neural network. Ideally, when the network learns, the error approaches 0 and the Q-values have been correctly learned. The loss function leads to an online update rule, while if we choose a “batch” loss function, it will lead us to a batch learning rule. Calculating the error across a batch is computationally advantageous, and many modern reinforcement learning algorithms store examples in a buffer that they replay during the learning process.\n",
    "\n",
    "A simple representation for the state in the linear track is a one-hot vector: a vector of zeros with a 1 at the agent’s position. While this is straightforward, more expressive representations can lead to faster learning, especially in continuous spaces. For instance, overlapping radial basis functions can be used to represent the state of an agent moving in a continuous space.\n",
    "\n",
    "The learning rules for a network with a single layer are:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\left( y_a^*(s) - y_a(s) \\right) \\cdot H\\left( \\sum_j w_{ij} x_j + b_i \\right) \\cdot x_j,\n",
    "$$\n",
    "\n",
    "and the corresponding bias updates are:\n",
    "\n",
    "$$\n",
    "\\Delta b_i = \\eta \\left( y_a^*(s) - y_a(s) \\right) \\cdot H\\left( \\sum_j w_{ij} x_j + b_i \\right).\n",
    "$$\n",
    "\n",
    "$H(.)$ is the Heaviside step function, i.e., the derivative of ReLU.\n",
    "\n",
    "In neural network-based Q-learning, unlike in supervised learning, we only update the weights and biases corresponding to the action taken. This means the weight updates for neurons representing the Q-values of non-selected actions should be zero.\n",
    "\n",
    "**Trick to Convert Standard Backpropagation Implementations to Deep Reinforcement Learning**\n",
    "\n",
    "To adapt this for backpropagation with hidden layers or deep networks, we can employ a “trick”: set the targets for all outputs corresponding to non-selected actions equal to their current values. This forces the weight changes relevant to the other outputs to become zero. Only for the action taken do we use:\n",
    "\n",
    "$$\n",
    "y_a^*(s) = r + \\gamma \\max_{a'} Q(s', a').\n",
    "$$\n",
    "\n",
    "This is because non-selected actions should not be informed by immediate and future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2cf6d",
   "metadata": {},
   "source": [
    "### Efficient Implementation of Q-learning with Function Approximation\n",
    "\n",
    "The following NumPy implementation illustrates how to apply Q-learning with a single-layer neural network, using the technique previously discussed: we update only the output corresponding to the action taken, while keeping the others unchanged. This approach is compatible with mini-batch learning from a replay buffer. For simplicity, we set the batch size $n = 1$ here.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder definitions\n",
    "n = ...  # Batch size\n",
    "num_states = ...\n",
    "num_actions = ...\n",
    "\n",
    "X = ...  # Current state, shape (n, num_states)\n",
    "actions_taken = ...\n",
    "rewards = ...\n",
    "next_states = ...\n",
    "gamma = ...\n",
    "\n",
    "W = ...  # Weights, shape (num_states, num_actions)\n",
    "b = ...  # Biases, shape (num_actions,)\n",
    "learning_rate = ...\n",
    "\n",
    "def activation(x):\n",
    "    ...  # e.g., ReLU\n",
    "\n",
    "def activation_derivative(x):\n",
    "    ...\n",
    "\n",
    "# Forward pass\n",
    "h = np.matmul(X, W) + b\n",
    "current_q_values = activation(h)\n",
    "\n",
    "# Next-state Q-values\n",
    "next_q_values = activation(np.matmul(next_states, W) + b)\n",
    "max_next_q_values = np.max(next_q_values, axis=1)\n",
    "targets = rewards + gamma * max_next_q_values\n",
    "\n",
    "# Adjust target for taken actions only\n",
    "adjusted_targets = current_q_values.copy()\n",
    "adjusted_targets[np.arange(n), actions_taken] = targets\n",
    "\n",
    "# Compute loss gradient\n",
    "error_output = current_q_values - adjusted_targets\n",
    "delta_output = error_output * activation_derivative(h)\n",
    "\n",
    "# Parameter updates\n",
    "grad_W = np.matmul(X.T, delta_output) / n\n",
    "grad_b = np.sum(delta_output, axis=0) / n\n",
    "\n",
    "W -= learning_rate * grad_W\n",
    "b -= learning_rate * grad_b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f2c27",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Take the linear track problem and use a perceptron to replace the Q-values table. Because of the simplicity of the environment, there is no need to approximate the Q-values with a neural network, but doing so provides a useful demonstration of function approximation in reinforcement learning.\n",
    "\n",
    "Implement Q-learning using a single-layer network (i.e., a perceptron with no hidden units). \n",
    "\n",
    "As a reminder, the linear track environment is defined as follows:\n",
    "\n",
    "A robot is placed at the beginning of a linear track (state 0) and must navigate to a reward at the end of the track (state 5). The robot has two possible actions: “forward” and “backward”. The track consists of 6 discrete states, numbered from 0 to 5. The objective is to use the Q-learning algorithm to teach the robot to reach the reward optimally—i.e., in the minimum number of steps.\n",
    "\n",
    "💡 Tip\n",
    "\n",
    "Use a one-hot encoding of the agent’s current position as the input to the network. This makes the linear structure of the environment easy for the perceptron to exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0339469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class LinearTrackQAgent:\n",
    "    def __init__(self, track_size, epsilon=0.05, eta=0.1, gamma=0.99):\n",
    "        self.states = track_size\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.actions = 2\n",
    "        self.W = np.random.randn(self.states, self.actions) * 0.01\n",
    "        self.b = np.zeros(self.actions,)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        pass\n",
    "        \n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def train(self, episodes, runs=100, max_steps=100):\n",
    "        pass\n",
    "\n",
    "    def plot_learning_progress(self, average_steps_per_episode, optimal_steps_per_episode):\n",
    "        plt.plot(average_steps_per_episode, label='Average Steps per Episode')\n",
    "        plt.axhline(y=optimal_steps_per_episode, color='r', linestyle='--', label='Optimal Steps')\n",
    "        plt.title('Learning Progress over Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Steps')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the agent with a track size of 6 states for checks then increase to 20\n",
    "agent = LinearTrackQAgent(track_size=6,epsilon=0.01, eta=0.01, gamma=0.9)\n",
    "optimal_steps_per_episode = 5\n",
    "\n",
    "# Train the agent over 100 episodes and 10 runs, then plot the learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b144f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class LinearTrackQAgent:\n",
    "    def __init__(self, track_size, epsilon=0.05, eta=0.1, gamma=0.99):\n",
    "        self.states = track_size\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.actions = 2\n",
    "        self.W = np.random.randn(self.states, self.actions) * 0.01\n",
    "        self.b = np.zeros(self.actions,)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        X = np.zeros((1,self.states))\n",
    "        X[0,state] = 1\n",
    "        h=np.matmul(X,self.W)+ self.b\n",
    "        q_values = sigmoid(h)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        X = np.zeros((1,self.states))\n",
    "        X[0,state] = 1\n",
    "        next_state_X= np.zeros((1,self.states))\n",
    "        next_state_X[0,next_state] = 1\n",
    "        \n",
    "        # Compute the Q values for current and next state\n",
    "        h=np.matmul(X, self.W) + self.b\n",
    "        q_values = sigmoid(h)\n",
    "        next_q_values = sigmoid(np.dot(next_state_X, self.W) + self.b)\n",
    "        \n",
    "        # Compute the TD target and error\n",
    "        target = reward + self.gamma * np.max(next_q_values)\n",
    "\n",
    "        adjusted_target = np.copy(q_values)\n",
    "        # Update the target only for the taken actions across the batch\n",
    "        adjusted_target[0, action] = target\n",
    "        error =  (q_values -adjusted_target) * sigmoid_derivative(h) \n",
    " \n",
    "        # Update gradient calculation to use the correct action index\n",
    "\n",
    "        grad_W = np.matmul(X.T, error) / X.shape[0]\n",
    "        grad_b = np.sum(error, axis=0) / X.shape[0]\n",
    "        \n",
    "        self.W -= self.eta * grad_W\n",
    "        self.b -= self.eta * grad_b\n",
    "\n",
    "\n",
    "    def train(self, episodes, runs=100, max_steps=100):\n",
    "        steps_run_episode = np.zeros((runs, episodes))\n",
    "        for run in range(runs):\n",
    "            for episode in range(episodes):\n",
    "                state = 0\n",
    "                steps = 0\n",
    "                while state < self.states - 1 and steps < max_steps:\n",
    "                    action = self.choose_action(state)\n",
    "                    next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "                    reward = 1 if next_state == self.states - 1 else 0\n",
    "                    self.update_Q(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                    steps += 1\n",
    "                steps_run_episode[run, episode] = steps\n",
    "        return steps_run_episode\n",
    "\n",
    "    def plot_learning_progress(self, average_steps_per_episode, optimal_steps_per_episode):\n",
    "        plt.plot(average_steps_per_episode, label='Average Steps per Episode')\n",
    "        plt.axhline(y=optimal_steps_per_episode, color='r', linestyle='--', label='Optimal Steps')\n",
    "        plt.title('Learning Progress over Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Steps')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "states=20\n",
    "\n",
    "agent = LinearTrackQAgent(track_size=states,epsilon=0.01, eta=0.01, gamma=0.99)\n",
    "optimal_steps_per_episode = states-1\n",
    "\n",
    "# Train the agent over 100 episodes and 10 runs, then plot the learning progress\n",
    "steps_run_episode = agent.train(episodes=100, runs=50, max_steps=1000)\n",
    "average_steps_per_episode = np.mean(steps_run_episode, axis=0)\n",
    "agent.plot_learning_progress(average_steps_per_episode, optimal_steps_per_episode)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67300a4a",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Extend your implementation of the linear track problem by adding a hidden layer to the neural network. This transforms the perceptron into a simple multilayer network. Observe how the addition of the hidden layer affects learning performance.\n",
    "\n",
    "Consider whether the network converges more quickly or not in such a simple environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13762a5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class LinearTrackQAgent:\n",
    "    def __init__(self, track_size, hidden_size=10, epsilon=0.05, eta=0.1, gamma=0.99):\n",
    "        self.states = track_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.actions = 2\n",
    "        self.W1 = np.random.randn(self.states, self.hidden_size) * 0.01\n",
    "        self.b1 = np.zeros(self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.actions) * 0.01\n",
    "        self.b2 = np.zeros(self.actions)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        X = np.zeros((1, self.states))\n",
    "        X[0, state] = 1\n",
    "        hidden = sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "        q_values = sigmoid(np.dot(hidden, self.W2) + self.b2)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        X = np.zeros((1, self.states))\n",
    "        X[0, state] = 1\n",
    "        next_state_X = np.zeros((1, self.states))\n",
    "        next_state_X[0, next_state] = 1\n",
    "\n",
    "        hidden = sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "        q_values = sigmoid(np.dot(hidden, self.W2) + self.b2)\n",
    "\n",
    "        next_hidden = sigmoid(np.dot(next_state_X, self.W1) + self.b1)\n",
    "        next_q_values = sigmoid(np.dot(next_hidden, self.W2) + self.b2)\n",
    "\n",
    "        target = reward + self.gamma * np.max(next_q_values)\n",
    "        adjusted_target = np.copy(q_values)\n",
    "        adjusted_target[0, action] = target\n",
    "\n",
    "        # Compute the error terms\n",
    "        delta_output = (adjusted_target-q_values) * sigmoid_derivative(np.dot(hidden, self.W2) + self.b2)\n",
    "        delta_hidden = np.dot(delta_output, self.W2.T) * sigmoid_derivative(np.dot(X, self.W1) + self.b1)\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_W2 = np.dot(hidden.T, delta_output)\n",
    "        grad_b2 = np.sum(delta_output, axis=0)\n",
    "        grad_W1 = np.dot(X.T, delta_hidden)\n",
    "        grad_b1 = np.sum(delta_hidden, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W2 += self.eta * grad_W2\n",
    "        self.b2 += self.eta * grad_b2\n",
    "        self.W1 += self.eta * grad_W1\n",
    "        self.b1 += self.eta * grad_b1\n",
    "\n",
    "    def train(self, episodes, runs=100, max_steps=100):\n",
    "        steps_run_episode = np.zeros((runs, episodes))\n",
    "        for run in range(runs):\n",
    "            for episode in range(episodes):\n",
    "                state = 0\n",
    "                steps = 0\n",
    "                while state < self.states - 1 and steps < max_steps:\n",
    "                    action = self.choose_action(state)\n",
    "                    next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "                    reward = 1 if next_state == self.states - 1 else 0\n",
    "                    self.update_Q(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                    steps += 1\n",
    "                steps_run_episode[run, episode] = steps\n",
    "        return steps_run_episode\n",
    "\n",
    "    def plot_learning_progress(self, average_steps_per_episode, optimal_steps_per_episode):\n",
    "        plt.plot(average_steps_per_episode, label='Average Steps per Episode')\n",
    "        plt.axhline(y=optimal_steps_per_episode, color='r', linestyle='--', label='Optimal Steps')\n",
    "        plt.title('Learning Progress over Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Steps')\n",
    "        plt.ylim(1, np.max(average_steps_per_episode))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Parameters for initialization and training\n",
    "states = 20\n",
    "agent = LinearTrackQAgent(track_size=states, hidden_size=10, epsilon=0.01, eta=0.1, gamma=0.99)\n",
    "optimal_steps_per_episode = states - 1\n",
    "\n",
    "# Train the agent and plot the learning progress\n",
    "steps_run_episode = agent.train(episodes=200, runs=50, max_steps=1000)\n",
    "average_steps_per_episode = np.mean(steps_run_episode, axis=0)\n",
    "agent.plot_learning_progress(average_steps_per_episode, optimal_steps_per_episode)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
