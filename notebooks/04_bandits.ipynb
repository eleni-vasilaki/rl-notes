{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f390af7f",
   "metadata": {},
   "source": [
    "# Reinforcement Learning through the lens of Optimisation. Bandits.\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/04_bandits.ipynb)\n",
    "\n",
    "## Optimisation and Decision Making  \n",
    "\n",
    "Imagine how you make choices in your life. Implicit in your choices, there is some form of optimisation.  \n",
    "\n",
    "For instance, you try, when possible, to go to the university of your choice, selecting a topic of study that you believe will be most beneficial\u2014perhaps because of career opportunities, personal interest, financial stability, or social connections. Often, it is a combination of these factors. Different things matter to different people, but in the end, you are **implicitly optimising** your life.  \n",
    "\n",
    "Maybe you didn\u2019t want to overthink everything, so you left part of the decision to chance. That means you optimised for something else\u2014your energy, your well-being, your time. You chose to focus on something you value more, and you let luck determine the rest. That, too, is an implicit **optimisation**\u2014choosing where to direct your effort.  \n",
    "\n",
    "Now, think about how you make choices in friendships. On this planet, there may be a person who could be your best friend, someone whose personality and experiences align with yours in ways you haven\u2019t yet discovered. But you may never meet them. The best friends you will ever have might still be strangers to you at this moment. I can certainly tell you that in my twenties, I had not yet met the best friends I would ever have.  \n",
    "\n",
    "Or take romantic relationships, if that interests you. Even here, you are optimising something. Not everyone is optimising for the same thing\u2014some people prioritise shared interests, others stability, others excitement. But almost everyone **optimises locally**.  \n",
    "\n",
    "Let\u2019s take dating or friendship as an example. Even if you search for connections online, thinking that you are broadening your options, your **search is still local**\u2014it is constrained to the population that uses that particular platform, social media, or social environment you have chosen to engage with.  \n",
    "\n",
    "While some people may feel they are making the **optimal** choices for their lives, in reality, our choices are wildly different because different things mean different benefits to each of us. And often, we make decisions under constraints. But one thing remains true: optimisation is at the heart of how we make choices.  \n",
    "\n",
    "Here, we will discuss **gradient descent**, a popular optimisation method that looks for an optimal local solution. We will look at reinforcement learning within this framework\u2014where what we are trying to optimise is **reward**. We will start with defining a system by a function of multiple parameters, which we want to maximise or minimise, and discuss how to do so using the concept of gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d03f56",
   "metadata": {},
   "source": [
    "## Convex Functions and Single Local Minima\n",
    "\n",
    "Consider a system described mathematically by a function $f(x_1, x_2,\\ldots, x_n)$. Such a function is known as **multivariable** because it accepts multiple variables as inputs simultaneously. A more compact way to describe it is by $f(\\mathbf{x})$, where $\\mathbf{x}=[x_1~x_2~x_3 \\ldots]^T$ is a vector representing the parameters of the system. $T$ here is the operator \"transposed,\" meaning rows become columns (and vice versa), as in mathematics, vectors are typically considered column vectors.\n",
    "\n",
    "The goal of optimisation is to find the values of $\\mathbf{x}$ that either minimise or maximise $f(\\mathbf{x})$, depending on whether $f(\\mathbf{x})$ is a measure of error or performance. When the function has a single extremum, it is possible to find its minimum or maximum through analytical methods. For a given function $f(\\mathbf{x})$, direct minimisation involves calculating the values of $\\mathbf{x}$ that lead to the lowest value of $f$.\n",
    "\n",
    "More specifically, a **strictly convex function** $f(\\mathbf{x})$, such as $f(\\mathbf{x})=x_1^2+x_2^2$, has at most one global minimum. To locate critical points, we take the partial derivative of $f$ with respect to each parameter $x_i$ and set it to zero:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial f}{\\partial x_i} = 0 \n",
    "$$\n",
    "\n",
    "This condition identifies critical points, where the function's slope is zero in every direction of the parameter space. However, this alone does not guarantee a minimum\u2014it could also be a maximum, or a saddle point, see explanation below. The key fact is that if $f(\\mathbf{x})$ is strictly convex, then any critical point must be a **global minimum**.\n",
    "\n",
    "### Saddle points\n",
    "A saddle point is a point where the gradient is zero, meaning there is no immediate direction of steepest descent or ascent. However, unlike a local minimum or maximum, the curvature of the function behaves differently in different directions:\n",
    "\n",
    "- On one side, moving away from the saddle point causes the function to increase.  \n",
    "- On another side, moving away from the saddle point causes the function to decrease.\n",
    "\n",
    "This means that the point is not a true minimum or maximum, but rather a transition point where the function \u201ccurves up\u201d in some directions and \u201ccurves down\u201d in others. \n",
    "\n",
    "### Definition of Convexity\n",
    "\n",
    "A function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is **convex** if, for all points $x, y \\in \\mathbb{R}^n$ and for all $\\theta \\in [0,1]$, it satisfies the inequality:\n",
    "\n",
    "$$\n",
    "f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y).\n",
    "$$\n",
    "\n",
    "Intuitively, this property means that the line segment connecting any two points on the graph of $f$ lies entirely above or on the graph itself. Consequently, any local minimum is also a global minimum\u2014a property particularly beneficial in optimisation problems.\n",
    "\n",
    "A function is **strictly convex** if the inequality above is strict:\n",
    "\n",
    "$$\n",
    "f(\\theta x + (1-\\theta)y) < \\theta f(x) + (1-\\theta)f(y), \\quad \\forall x \\neq y, \\quad \\forall \\theta \\in (0,1).\n",
    "$$\n",
    "\n",
    "Strict convexity ensures the existence of **at most one global minimum**, making the solution uniquely determined.\n",
    "\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the quadratic function $f(x) = ax^2 + bx + c$, where $a$, $b$, and $c$ are constants, and $a>0$. Our objective is to find its local minimum. \n",
    "\n",
    "First, we compute the derivative of $f(x)$ with respect to $x$ to identify the slope of the function at any point $x$:\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{d}{dx}(ax^2 + bx + c) = 2ax + b.\n",
    "$$\n",
    "\n",
    "Setting the derivative equal to zero gives us the condition for the critical points where the function's slope is zero:\n",
    "\n",
    "$$ \n",
    "2ax + b = 0 \n",
    "$$\n",
    "\n",
    "Solving for $x$ gives us the $x$-value at the critical point:\n",
    "\n",
    "$$\n",
    "x = -\\frac{b}{2a}.\n",
    "$$ \n",
    "\n",
    "Since this equation has a unique solution, there must be one critical point.\n",
    "\n",
    "The critical point $x = -\\frac{b}{2a}$ is where the function $f(x)$ reaches its local minimum, provided that $a > 0$. To verify this, we compute the second derivative:\n",
    "\n",
    "$$ \n",
    "f''(x) = 2a. \n",
    "$$\n",
    "\n",
    "Since the second derivative is positive when $a > 0$, a small perturbation in $x$ will result in higher function values. This confirms that the function is strictly convex and that the critical point corresponds to the lowest local value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x, a, b, c):\n",
    "    \"\"\"Quadratic function f(x) = ax^2 + bx + c.\"\"\"\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "\n",
    "# Coefficients\n",
    "a, b, c = 1, -4, 3\n",
    "\n",
    "# Evaluate\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = f(x, a, b, c)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=r'$f(x)$')\n",
    "plt.scatter(-b / (2*a), f(-b / (2*a), a, b, c),\n",
    "            color='red', zorder=3, label='Minimum')\n",
    "plt.title(r'Plot of the Convex Function $f(x) = ax^2 + bx + c$, $a>0$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e516add",
   "metadata": {},
   "source": [
    "# Multiple local minima. Introduction to Gradient Descent\n",
    "\n",
    "For more complex systems, finding parameters that minimise the error function by simply setting the derivative to zero is generally not effective. Complex systems often have multiple local minima (or maxima) and complex interdependencies between parameters. To circumvent this issue, we use a method called **gradient descent**. Starting from an initial point, we estimate the gradient of the error function and take a small step in the opposite direction of this gradient. This is because the gradient of a function at any point gives us the direction of the steepest ascent.\n",
    "\n",
    "By moving in the opposite direction, we iteratively follow the path of steepest descent, leading us towards a local minimum of the function. This technique is important for machine learning and many other optimisation algorithms. The process can be mathematically represented as:\n",
    "\n",
    "$$ \\mathbf{x}_{i+1} = \\mathbf{x}_i - \\eta \\nabla f(\\mathbf{x}), $$\n",
    "\n",
    "where $\\mathbf{x}_{i}$ is the current value of $\\mathbf{x}$, $\\mathbf{x}_{i+1}$ is the new value, $i$ is an index typically starting from $0$, which corresponds to the initial point, $\\eta$ is the learning rate, and $\\nabla f(\\mathbf{x})$ is the gradient of the function at $\\mathbf{x}$. The learning rate $\\eta$ controls the size of the steps we take towards the minimum. The gradient $\\nabla f(\\mathbf{x})$ is given by:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2},\\ldots, \\frac{\\partial f}{\\partial x_n} \\right]$$\n",
    "\n",
    "which contains information about the partial derivatives of the function.\n",
    "\n",
    "Traditionally, gradients are expressed as **column vectors** in mathematics and deep learning literature. However, tools like PyTorch store gradients as row vectors. It is important to choose a convention and use it consistently throughout calculations and implementations. \n",
    "\n",
    "The **update rule** is typically written as:\n",
    "\n",
    "$$  \\Delta \\mathbf{x} = - \\eta \\nabla f(\\mathbf{x}), $$\n",
    "\n",
    "where it is implied that $ \\Delta $ denotes the difference between two subsequent values, corresponding to the definition of the derivative:\n",
    "\n",
    "$$ \\Delta \\mathbf{x} = \\mathbf{x}_{i+1} -  \\mathbf{x}_{i}.$$\n",
    "\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Let us reconsider the function: $f(x) = ax^2 + bx + c$, where $a$, $b$, and $c$ are constants, and $a>0$. While in this case we can find the local minimum by setting the derivative to zero, we can still apply gradient descent to illustrate the method.\n",
    "\n",
    "To apply gradient descent to this function, we start with an initial guess $x_0$ and iteratively apply the update rule:\n",
    "\n",
    "$$x_{i+1} = x_{i} - \\eta (2ax_i + b)$$\n",
    "\n",
    "where $\\eta$ is the learning rate, a small positive number that controls the size of each step. By iteratively updating $x$ using this rule, we move towards the minimum value of $f(x)$.\n",
    "\n",
    "With an appropriate $\\eta$ and starting point $x_0$, the gradient descent algorithm will converge to the local minimum at $x = -\\frac{b}{2a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e27b6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Understanding the Gradient Through the Derivative\n",
    "\n",
    "The derivative of a function at a point provides the rate of change of the function\u2019s value with a small change in the input. For a single-variable function $f(x)$, the derivative at a point $x$ is defined as:\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "This tells us how much $f(x)$ increases for a small increase in $x$. For multivariable functions, this concept extends to the **gradient** $\\nabla f$, which we defined earlier as the vector of partial derivatives. Each component of $\\nabla f$ indicates how $f$ changes with respect to a small change in one input parameter, holding the others constant.\n",
    "\n",
    "#### Why Move Opposite to the Gradient?\n",
    "\n",
    "The gradient $\\nabla f$ points in the direction of the steepest increase of the function. To **minimise** $f(x)$, we move opposite to the gradient:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\eta \\nabla f(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, controlling the step size.\n",
    "\n",
    "But why exactly does the **negative sign** ensure minimisation? Let\u2019s break it down with two cases:\n",
    "\n",
    "**Case 1: The Function Increases with $x$**\n",
    "- Suppose $f(x)$ increases as $x$ increases, meaning $f'(x) > 0$.  \n",
    "- If we move in the **positive** direction, $f(x)$ would grow.\n",
    "- To minimise $f(x)$, we must **decrease** $x$, which means subtracting $f'(x)$.\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - \\eta f'(x)\n",
    "$$\n",
    "\n",
    "- Since $f'(x) > 0$, the update moves $x$ in the **negative** direction.\n",
    "\n",
    "**Case 2: The Function Decreases with $x$**\n",
    "- Suppose $f(x)$ decreases as $x$ increases, meaning $f'(x) < 0$.  \n",
    "- If we move in the **negative** direction, $f(x)$ would grow.\n",
    "- To keep minimising $f(x)$, we must **increase** $x$.\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - \\eta f'(x)\n",
    "$$\n",
    "\n",
    "- Since $f'(x)$ is negative, subtracting it results in a **positive** step in $x$.\n",
    "\n",
    "Thus, the negative sign ensures that we always move in the direction that decreases $f(x)$, regardless of whether the function is increasing or decreasing at that point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76417b",
   "metadata": {},
   "source": [
    "### Issues with Gradient Descent\n",
    "Since the gradient descent method finds only a local solution, several related challenges arise when applying it to complex problems\n",
    "\n",
    "1. **Getting Stuck in Local Minima**  \n",
    "In non-convex landscapes, gradient descent typically converges to a local minimum instead of the global minimum. Since it follows the path of steepest descent, it cannot escape a local minimum once reached.  \n",
    "\n",
    "2. **Slow Convergence in Flat Regions**  \n",
    "Some functions contain flat regions where the gradient is close to zero: $ \\nabla f(\\mathbf{x}) \\approx 0$. When the algorithm enters such a region, updates become small, slowing progress.  \n",
    "\n",
    "3. **Saddle Points**  \n",
    "A saddle point is a stationary point where the gradient is zero, but the function has both increasing and decreasing directions. Gradient descent struggles to escape saddle points because the gradient is small.     \n",
    "\n",
    "4. **Poor Scaling of Gradients**  \n",
    "If gradients vary significantly across dimensions, updates may be too large in some directions and too small in others.  This leads to slow or unstable convergence.  \n",
    "\n",
    "5. **Sensitivity to Learning Rate**  \n",
    "A learning rate ($\\eta$) that is too high can cause the algorithm to oscillate around the minimum. If $\\eta$ is too small, convergence is slow.  \n",
    "\n",
    "### Techniques to Improve Convergence\n",
    "\n",
    "In response to the issues related to gradient descent, several techniques have been developed over the years. Below, you may find a few typical techniques used particularly in the context of deep learning.\n",
    "\n",
    "1. **Starting from Different Initial Points**  \n",
    "Running gradient descent from multiple initialisations increases the chance of finding a better minimum.  \n",
    "\n",
    "2. **Momentum**  \n",
    "Momentum helps smooth updates by incorporating past gradients, helping to escape shallow local minima and speeding up convergence. A formulation of momentum is shown below; $\\beta$ is a tunable parameter that controls how much of the past gradient is retained, and $\\eta$ is the learning rate.\n",
    "$$ \n",
    "v_{i+1} = \\beta v_i - \\eta \\nabla f(\\mathbf{x}_i)\n",
    "$$\n",
    "$$ \n",
    "\\mathbf{x}_{i+1} = \\mathbf{x}_i + v_{i+1} \n",
    "$$  \n",
    " \n",
    "\n",
    "3. **Adaptive Learning Rates**  \n",
    "Methods like **AdaGrad, RMSprop, and Adam** adjust the learning rate dynamically to prevent updates from becoming too large or too small.\n",
    "\n",
    "4. **Gradient Clipping**  \n",
    "If gradients become too large, enforcing an upper bound $C$ helps stabilise training:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) \\leftarrow \\frac{\\nabla f(\\mathbf{x})}{\\max(1, ||\\nabla f(\\mathbf{x})|| / C)}\n",
    "$$  \n",
    "\n",
    "5. **Learning Rate Decay**  \n",
    "Gradually reducing $\\eta$ over time allows finer updates as convergence nears. A common schedule is shown below; $\\eta_0$ is the initial learning rate, $k$ is the decay factor, and $t$ is the iteration index:  \n",
    "$$\n",
    "\\eta_t = \\frac{\\eta_0}{1 + k t} \n",
    "$$ \n",
    "      \n",
    "\n",
    "Gradient descent is a powerful optimisation technique, and these improvements help it navigate complex loss landscapes more efficiently, avoiding issues such as poor convergence and local traps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c59144-0310-4e94-9098-d7fb0fea4cb3",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The error function of a system with respect to its parameters has multiple local minima. You start from a random initial position in that error landscape (initialising the system's parameters with permitted values). What will happen? What will happen if you initialize again at a different random position? Consider the implications of starting at different positions on the likelihood of reaching global versus local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28174eb5-c5a2-4890-989c-8b0b808bc0f0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "When you initialize the parameters of a system at a random position within an error landscape that contains multiple local minima, the gradient descent algorithm will begin to iteratively adjust the parameters to move towards the nearest minimum based on the starting point. Because the landscape has multiple local minima, the specific minimum it converges to depends on where the initialization occurred.\n",
    "\n",
    "- **First Initialization**: In the first random initialization, the algorithm might converge to one of the local minima, which may or may not be the global minimum of the error function. The specific local minimum it reaches is determined by the initial parameters and the topology of the error landscape.\n",
    "\n",
    "- **Second Initialization at a Different Random Position**: If you initialize the parameters at a different random position and run the gradient descent algorithm again, there's a chance that it might converge to a different local minimum than the first run. This highlights the sensitivity of the optimization process to initial conditions in landscapes with multiple local minima. It's possible, with multiple initializations, to explore different parts of the error landscape and potentially find a better (lower) local minimum, or with better intitial conditions, the global minimum.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d36f4-9973-41d6-bf89-3c4e9d288d6f",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Study the script and identify the function, its derivative, and the gradient descent rule. Run it a few times. What do you observe? If this function was the error function of your system, how would you make sure to achieve the lowest error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ade17-fb8b-4089-bf47-d92c24501e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function with two local minima.\"\"\"\n",
    "    return x**4 - 2 * x**3 - 12 * x**2 + 2\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative of f.\"\"\"\n",
    "    return 4 * x**3 - 6 * x**2 - 24 * x\n",
    "\n",
    "\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    \"\"\"Run gradient descent and return the trajectory.\"\"\"\n",
    "    x = starting_point\n",
    "    history = [x]\n",
    "    for _ in range(num_iterations):\n",
    "        x -= learning_rate * df(x)\n",
    "        history.append(x)\n",
    "    return np.array(history), f(np.array(history))\n",
    "\n",
    "\n",
    "# Run from a random starting point\n",
    "starting_point = np.random.uniform(-4, 4)\n",
    "learning_rate = 0.005\n",
    "num_iterations = 50\n",
    "\n",
    "history, function_values = gradient_descent(starting_point, learning_rate, num_iterations)\n",
    "\n",
    "# Plot\n",
    "x_values = np.linspace(-4, 4, 400)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, f(x_values), label='Function with Two Local Minima')\n",
    "plt.scatter(history, function_values, color='red', zorder=5, label='Gradient Descent Steps')\n",
    "plt.plot(history, function_values, color='red', linestyle='dashed', zorder=5)\n",
    "plt.annotate('Initial Position',\n",
    "             xy=(history[0], function_values[0]),\n",
    "             xytext=(history[0] + 1, function_values[0] + 50),\n",
    "             arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "plt.title('Function with Two Local Minima and Gradient Descent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2cf02-420b-4d94-bac1-c8c7da3f82a9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "The function defined in the script is \n",
    "\n",
    "$$\n",
    "f(x) = x^4 - 2x^3 - 12x^2 + 2,\n",
    "$$ \n",
    "\n",
    "and its derivative, which is used to calculate the gradient, is \n",
    "$$\n",
    "f'(x) = 4x^3 - 6x^2 - 24x.\n",
    "$$ \n",
    "\n",
    "The gradient descent rule applied in the script is \n",
    "$$\n",
    "x_{\\text{new}} = x - \\eta \\nabla f(x),\n",
    "$$ where $\\eta$ is the learning rate, and $\\nabla f(x)$ is the gradient of the function at $x$.\n",
    "\n",
    "When running the script multiple times with different random initial positions, you may observe that the gradient descent algorithm sometimes converges to one local minimum and sometimes to another. This behavior is because the function has two local minima, and the starting point significantly influences which minimum the algorithm will converge to.\n",
    "\n",
    "If this function was the error function of your system and you aimed to achieve the lowest error, you could employ several strategies to increase the chances of finding the global minimum (the lowest error) rather than getting stuck in a local minimum:\n",
    "\n",
    "1. **Multiple Initializations**: Run the gradient descent algorithm multiple times with different random initial positions. This strategy increases the likelihood of exploring different areas of the error landscape and potentially finding the global minimum.\n",
    "\n",
    "2. **Adjusting the Learning Rate**: Experiment with different learning rates. A smaller learning rate can sometimes help make finer adjustments, which might lead to escaping shallow local minima, but it also makes the convergence process slower.\n",
    "\n",
    "3. **Advanced Optimization Techniques**: Advanced optimization techniques are crucial in many areas of machine learning, including training neural networks. Two such techniques are Gradient Descent with Momentum and the Adam optimizer. These methods aim to improve upon the basic gradient descent algorithm by navigating complex error landscapes more effectively.\n",
    "\n",
    "   - **Gradient Descent with Momentum** accelerates the convergence towards the optimal solution by incorporating the 'momentum' from previous steps. This can be particularly useful for overcoming the challenges posed by local minima. For an in-depth understanding, you might explore the paper: [On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.html).\n",
    "\n",
    "   - **Adam optimizer** adapts the learning rate for each parameter, combining ideas from other optimization algorithms to perform well on complex landscapes. The original paper on Adam, [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), provides detailed insights.\n",
    "\n",
    "By applying these strategies, you can enhance the odds of minimizing the error function to its lowest possible value, ensuring optimal performance for your system.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89313c94",
   "metadata": {},
   "source": [
    "\n## Reinforcement Learning as an Optimisation Problem\n\nIn reinforcement learning, we are optimising the expected return $G_t$, defined as the sum of future rewards. In the case of a finite horizon:\n\n$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_{t+N}.$$\n\nAlternatively, in an infinite horizon setting, where future rewards are discounted by a factor $\\gamma$ to prioritise near-term rewards, the return is:\n\n$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots$$\n\nwhere\n\n$$0 \\leq \\gamma < 1.$$\n\nThe discount factor $\\gamma$ ensures that rewards far into the future contribute less to the expected return, effectively applying an exponential weighting to future rewards. This allows reinforcement learning to optimise in a finite window, as $\\gamma^k$ becomes negligible for sufficiently large $k$.\n\n\nAt each time step, the agent is in a state $s_t$ and selects an action $a_t$. The goal of reinforcement learning is to learn the total future rewards $G_t$ resulting from choosing an action in a given state.\n\nBy accurately estimating the expected future reward, the agent can select good actions that lead to higher returns. This leads to two fundamental questions:\n1.\tHow can we estimate the expected future return $G_t$ for each action?\n2.\tHow do we pick an action, i.e., how do we define a policy? \n\nAt its core, reinforcement learning is a problem of reward maximisation. However, many optimisation problems in machine learning are framed as minimisation rather than maximisation. To align reinforcement learning with typical optimisation frameworks, we can reformulate reinforcement learning as error minimisation. We can minimise the prediction error of the expected future reward. This is equivalent to ensuring that our learned estimates of $G_t$ are as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cc795",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits\n\nIn reinforcement learning, we refer to **bandits** as a class of problems where an agent must choose between multiple possible actions, each yielding an immediate but uncertain reward. The term is inspired by slot machines (\"one-armed bandits\") found in casinos, though in our setup, bandits have **more than one arm**-hence the name multi-armed bandit problem.\n\nBandits provide a fundamental problem setup for **immediate rewards**, meaning that each action results in an observable reward without considering future consequences. Unlike full reinforcement learning problems, which involve delayed rewards and sequential decision-making, bandit problems focus solely on choosing the best action based on past experiences.\n\n### Expected Returns in Bandit Problems\n\nIn bandits, we start from the generic discounted return:\n\n$$\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n$$\n\nIn a bandit trial, there is only one interaction step after choosing an action, so there are no future rewards within the same trial. Therefore, the return reduces to the immediate reward, and we can omit the time index in this one-step setting:\n\n$$\nG = R\n$$\n\nFollowing statistics terminology, we index repeated interactions as **trials**. For a single trial, if the chosen action is $A$, we write:\n\n$$\nG = R(A).\n$$\n\nNotation convention for this bandit section (important):\n- $n$ indexes **trials** (actual agent-environment interactions).\n- $i$ indexes **possible reward values** in a distribution (support points), for example in $\\sum_i r_i P(R=r_i\\mid A_n=a)$.\n\nThe **true expected return** for any action $a$, denoted as $q^*(a)$, is defined as the expected reward given that action:\n\n$$\nq^*(a) \\doteq E[G \\mid A_n = a] = E[R \\mid A_n = a].\n$$\n\nThe expectation above is across repeated one-step trials. An empirical estimate from observed trials is:\n\n$$\nE[R\\mid A_n=a] \\approx \\frac{1}{N(a)}\\sum_{n=1}^{N} R_n\\,\\mathbf{1}_{\\{A_n=a\\}},\n$$\n\nwhere $N(a)=\\sum_{n=1}^N \\mathbf{1}_{\\{A_n=a\\}}$ counts how many times action $a$ was selected.\n\nIn distribution form, this is:\n\n$$\nE[R\\mid A_n=a] = \\sum_{i} r_i~P\\left(R=r_i \\mid A_n=a \\right),\n$$\n\nwhere:\n- $r_i$ are possible reward values,\n- $P\\left(R=r_i \\mid A_n=a \\right)$ represents the probability of receiving reward $r_i$ when action $a$ is selected.\n\nHowever, **the true reward distributions are unknown** to the agent. Instead, we make an assumption of **stationarity**, meaning that the probability distribution of rewards remains **constant over trials**:\n\n$$\nP\\left(R \\mid A_n=a \\right) = P\\left(R \\mid A_{n'}=a \\right)\n$$\n\nfor any trials $n$ and $n'$ and across all actions $a$. This assumption allows the agent to track and estimate expected returns. In non-stationary environments, these probabilities may change over time, which complicates the problem and requires adaptive strategies.\n\n### Estimating the Action-Value Function\n\nSince the agent does not have direct access to the true expected reward $q^*(a)$, it must instead learn an estimate, denoted as the **Q-value**:\n\n$$\nQ(a) \\approx q^*(a)= E[R\\mid A_n=a],\n$$\n\nwhere $R$ is the observed reward drawn from $P(R \\mid A_n = a)$. The intuition is that, on average, the estimated action value $Q(a)$ should be equal to the reward when choosing action $a$.\n\nTo achieve this, we will obtain the Q-values via an optimisation process. We will construct a loss function of $Q(a)$ with minimum at $E[R\\mid A_n=a]$. In other words, we are seeking a loss function whose first derivative is zero when $Q(a) = E[R\\mid A_n=a]$.\n\nWe will hypothesise a suitable loss function and we will show that at its minimum $Q(a)$ converges to $E[R\\mid A_n=a]$.\n\n### Defining the Loss Function\n\nGuided by the requirement that $Q(a) \\approx E[R\\mid A_n=a]$ we ask to minimise:\n\n$$ E[(Q(a) - R)^2 \\mid A_n = a] $$\n\nsince $(Q(a) - R)^2$ is a convex function with minimum at $Q(a)=R$.\n\nWe replace the expectation with the sample average in defining the loss function for the action $a$ as:\n\n$$\nL(a) = \\frac{1}{2N(a)} \\sum_{n=1}^{N} (Q(a) - R_n)^2 \\mathbf{1}_{{A_n = a}}\n$$\n\nwhere:\n\n- $N$ is the total number of trials.\n\n- $N(a)$ is the total number of times action $a$ has been selected:\n\n$$\nN(a) = \\sum_{n=1}^{N} \\mathbf{1}_{A_n = a}.\n$$\n\n- $\\mathbf{1}_{{A_n = a}}$ is the **indicator function**, which is defined as:\n\n$$\n\\mathbf{1}_{{A_n = a}} =\n\\begin{cases}\n1 & \\text{if } A_n = a, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\n- $R = R_n$ is the observed reward at trial $n$.\n\n- The number 2 in the denominator is added for mathematical convenience.\n\nWe will show that minimising this loss function leads to the desired result: $Q(a) \\approx q^*(a) = E[R \\mid A_n = a]$.\n\n### Minimising the Loss Function to Derive the Update Rule\n\nSince the loss function is **convex** with respect to $Q(a)$, we can directly compute its minimum by setting the **derivative to zero**:\n\n$$\n\\frac{d L(a)}{d Q(a)} = \\frac{d}{d Q(a)} \\left( \\frac{1}{2N(a)} \\sum_{n=1}^{N} (Q(a) - R_n)^2 \\mathbf{1}_{{A_n = a}} \\right) = 0.\n$$\n\nApplying the chain rule:\n\n$$\n\\frac{1}{N(a)} \\sum_{n=1}^{N} (Q(a) - R_n) \\mathbf{1}_{{A_n = a}} = 0.\n$$\n\nSince $Q(a)$ is a constant with respect to the inner summation over $n$, it can be factored out:\n\n$$\nQ(a) \\sum_{n=1}^{N} \\mathbf{1}_{{A_n = a}} = \\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}}.\n$$\n\nand the sum of the indicator function gives how many times action $a$ was selected:\n\n$$\nQ(a) N(a) = \\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}}.\n$$\n\nDividing both sides by $N(a)$:\n\n$$\nQ(a) = \\frac{1}{N(a)} \\sum_{n=1}^{N} R_n~\\mathbf{1}_{{A_n = a}}.\n$$\n\nThis shows that the optimal estimate $Q(a)$ is simply the **sample mean** of observed rewards for that action:\n\n$$\nQ(a) \\approx E[R \\mid A_n = a] = q^*(a).\n$$\n\nThis result confirms that in a **stationary environment**, $Q(a)$ will converge to the **average reward observed when action $a$ was taken**.\n\n### Gradient Update Rule\n\nFollowing the section \"Introduction to gradient descent\", we define the gradient update rule as:\n\n$$\n\\Delta Q(a)=- \\eta N(a) \\frac{d L(a)}{d Q}= -\\eta\\sum_{n=1}^{N} {\\left(Q(a)-R_n\\right) \\mathbf{1}_{{A_n = a}}}\n$$\n\nwhere $\\eta$ is the **learning rate**.\n\nNote that we simply need to set $\\Delta Q(a)$ proportional to the gradient. We introduce the factor $N(a)$ for **mathematical convenience**, allowing us to derive the desirable result more cleanly when computing the gradient.\n\nThis approach is referred to as a **batch update** because we first collect multiple observations before adjusting our estimate of $Q(a)$. At convergence, we expect to reach the same global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e55d22",
   "metadata": {},
   "source": [
    "# Exercise \nShow that at convergence of the gradient update rule for bandits, $Q(a)$ recovers the true expected reward:$$Q(a) \\approx E[R \\mid A_n = a] = q^*(a).$$ \n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240ef47",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "The gradient update rule for bandits is given by:\n",
    "\n",
    "$$\n",
    "\\Delta Q(a) = \\eta \\sum_{n=1}^{N} (R_n - Q(a)) \\mathbf{1}_{{A_n = a}}.\n",
    "$$\n",
    "\n",
    "At **convergence**, the update should be zero:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} (R_n - Q(a)) \\mathbf{1}_{{A_n = a}} = 0.\n",
    "$$\n",
    "\n",
    "Expanding the summation:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}} - \\sum_{n=1}^{N} Q(a) \\mathbf{1}_{{A_n = a}} = 0.\n",
    "$$\n",
    "\n",
    "Since $Q(a)$ is **constant** with respect to the summation over $n$, it factors out:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}} - Q(a) \\sum_{n=1}^{N} \\mathbf{1}_{{A_n = a}} = 0.\n",
    "$$\n",
    "\n",
    "Recognizing that the sum of the indicator function counts the number of times action $a$ was selected:\n",
    "\n",
    "$$\n",
    "N(a) = \\sum_{n=1}^{N} \\mathbf{1}_{{A_n = a}},\n",
    "$$\n",
    "\n",
    "we rewrite the equation as:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}} = Q(a) N(a).\n",
    "$$\n",
    "\n",
    "Solving for $Q(a)$:\n",
    "\n",
    "$$\n",
    "Q(a) = \\frac{1}{N(a)} \\sum_{n=1}^{N} R_n \\mathbf{1}_{{A_n = a}}.\n",
    "$$\n",
    "\n",
    "Since this represents the **sample mean** of observed rewards for action $ a$, we conclude:\n",
    "\n",
    "$$\n",
    "Q(a) \\approx E[R \\mid A_n = a] = q^*(a).\n",
    "$$\n",
    "\n",
    "Thus, the gradient update rule ensures that $Q(a)$ **converges to the true expected reward** $q^*(a)$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80221e03",
   "metadata": {},
   "source": [
    "## A single loss function for the multi-armed bandit problem\n\nPreviously we optimised as many error functions as arms independently, but more elegantly, we can consider a **single error function** with parameters being the $Q$-values of the arms, and calculate the update rule for the arms.\n\nThe total loss function is given by the sum of the previously defined $L(a)$ error functions over all actions $a$, representing the overall estimation error across all actions:\n\n$$\nL = \\sum_{a=1}^{|A|} L(a)= \\sum_{a=1}^{|A|} \\left[ \\frac{1}{2N(a)}\\sum_{n=1}^{N} {\\left(Q(a)-R_n\\right)^2 \\mathbf{1}_{{A_n = a}}} \\right]\n$$\n\nwhere:\n\n$A$ is the set of all possible actions and $|A|$ the cardinality of the set A, meaning the total number of available actions. For convenience we may simply write $\\sum_a$ without explicitly specifying the limits when summing over all actions in $A$.\n\nThe indicator function $\\mathbf{1}_{{A_n = a}}$ is defined as:\n\n$$\n\\mathbf{1}_{{A_n = a}} =\n\\begin{cases}\n1 & \\text{if } A_n = a \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\n$N$ is the total number of trials and $N(a)$ is the number of trials where action $a$ was selected:\n\n$$\nN(a) = \\sum_{n=1}^{N} \\mathbf{1}_{{A_n = a}}\n$$\n\nSince exactly one action is selected per trial, the per-action counts partition the total:\n\n$$\nN = \\sum_a N(a)\n$$\n\n$R = R_n$ is the observed reward at trial $n$.\n\n### Deriving the Update Rule\nWithout loss of generality, let us take the partial derivative of $L$ with respect to $Q(1)$, i.e., for action $a=1$:\n\n$$\n\\frac{\\partial L}{\\partial Q(1)} = \\frac{\\partial}{\\partial Q(1)} \\left( \\frac{1}{2N(1)} \\sum_n (Q(1)-R_n)^2 \\mathbf{1}_{{A_n = 1}} \\right)\n$$\n\nApplying the chain rule:\n\n$$\n\\frac{\\partial L}{\\partial Q(1)} = \\frac{1}{N(1)} \\sum_n (Q(1)-R_n) \\mathbf{1}_{{A_n = 1}}\n$$\n\nThe summation over $a$ disappears because we are differentiating with respect to one particular action-value $Q(1)$, hence the terms relating to $Q(2)$, $Q(3)$ etc remain constants because changing $Q(1)$ will not affect them. Following the same convention as before (multiplying the gradient by $N(a)$ for convenience), the update rule follows:\n\n$$\n\\Delta Q(1) = \\eta \\sum_n (R_n - Q(1)) \\mathbf{1}_{{A_n = 1}}\n$$\n\nMore generally, for any action $a$:\n\n$$\n\\Delta Q(a) = \\eta  \\sum_n (R_n - Q(a)) \\mathbf{1}_{{A_n = a}}\n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5560458",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Start from the single error function $L$ for a bandit. Show that at steady state:\n",
    "\n",
    "$$ Q(a) \\approx E[R \\mid A_n=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b42fb1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "We calculate:\n",
    "\n",
    "$$\\frac{\\partial L} {\\partial Q(a)} = \\frac{\\partial}{\\partial Q(a)} \\left( \\sum_a \\frac{1}{2N(a)} \\sum_n (R_n - Q(a))^2 \\mathbf{1}_{{A_n = a}} \\right).$$\n",
    "\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Q(a)} = \\frac{1}{N(a)}\\sum_n ( Q(a)-R_n) \\mathbf{1}_{{A_n = a}}$$\n",
    "\n",
    "The $\\sum_a $ drops because we are taking the partial derivative with respect to one of these actions $a$, hence the terms related to the other actions are considered constants.\n",
    "\n",
    "We then set the gradient to zero and solve for $Q(a)$:\n",
    "\n",
    "$$0 =  \\sum_n (Q(a)-R_n) \\mathbf{1}_{{A_n = a}}$$\n",
    "\n",
    "Rearranging the terms gives us:\n",
    "\n",
    "$$ \\sum_n Q(a) \\cdot \\mathbf{1}_{{A_n = a}}= \\sum_n R_n \\cdot \\mathbf{1}_{{A_n = a}} $$\n",
    "\n",
    "Since $Q(a)$ is a constant with respect to the inner sum over $n$, it can be factored out:\n",
    "\n",
    "$$ Q(a) \\cdot \\sum_n \\mathbf{1}_{{A_n = a}} = \\sum_n R_n \\cdot \\mathbf{1}_{{A_n = a}}$$\n",
    "\n",
    "This leads us to the update rule for $Q(a)$ at the minimum of the error function:\n",
    "\n",
    "\n",
    "$$Q(a) = \\frac{\\sum_{n} R_n \\cdot \\mathbf{1}_{{A_n = a}}}{\\sum_n \\mathbf{1}_{{A_n = a}}}\\approx E[R \\mid A_n=a]$$\n",
    "\n",
    "This shows that the updated estimate of $Q(a)$ for each arm $a$ is the average of the rewards received when that arm is selected.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166d082",
   "metadata": {},
   "source": [
    "## Batch vs Online Bandit Update Rule\n\nThe update rules we have derived so far are **batch**: we first collect all $N$ observations and then make a single update informed by the entire dataset. The batch gradient update from the previous section is:\n\n$$\n\\Delta Q(a) = -\\eta \\sum_{n=1}^{N} {\\left(Q(a)-R_n \\right) \\mathbf{1}_{{A_n = a}}}\n$$\n\nIn contrast, **online learning** updates the estimate after each individual trial. If we update after trial $n$, only one observation $(A_n, R_n)$ is available, so the sum reduces to a single term:\n\n$$\n\\Delta Q(a) = -\\eta \\left(Q(a)-R_n \\right)\\mathbf{1}_{{A_n = a}}.\n$$\n\nThis is the online update rule. The question is whether updating one trial at a time still leads to a meaningful result. Below, we show that it does \u2014 in a stationary environment the online rule converges to the same sample mean as the batch solution.\n\n### Stationary environment\nAt trial $n$, after observing $(A_n, R_n)$:\n\n$$\nN(a) \\leftarrow N(a) + \\mathbf{1}_{\\{A_n=a\\}},\n$$\n\n$$\nQ(a) \\leftarrow Q(a) + \\frac{1}{N(a)}\\,\\mathbf{1}_{\\{A_n=a\\}}\\left(R_n-Q(a)\\right).\n$$\n\nSo $Q(a)$ is updated only when action $a$ is selected, and in stationary settings this converges to the sample mean reward for that action.\n\n### Non-stationary environment\nFor a **fixed learning rate** $\\eta$, use:\n\n$$\nQ(a) \\leftarrow Q(a) + \\eta\\,\\mathbf{1}_{\\{A_n=a\\}}\\left(R_n-Q(a)\\right).\n$$\n\nIf $A_n=a$, this is:\n\n$$\nQ(a) \\leftarrow (1-\\eta)Q(a)+\\eta R_n.\n$$\n\nThis means the estimate follows an **exponential moving average**, where older observations retain weight $(1-\\eta)$.\n\n- When **$\\eta$ is small**, updates are slow, and past rewards strongly influence $Q(a)$.\n- When **$\\eta$ is large**, new observations have greater influence, making the estimate more adaptive to changes in the environment.\n\nTherefore, when the reward distribution changes over time, one should choose the fixed-step-size rule for non-stationary environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92517d0",
   "metadata": {},
   "source": [
    "# Exercise\nShow that for a fixed action $a$, the online update rule for stationary environments (Cell 18) with step size $\\frac{1}{N(a)}$:\n\n$$\nQ(a) \\leftarrow Q(a) + \\frac{1}{N(a)}\\left(R_n - Q(a)\\right),\n$$\n\nproduces $Q(a)$ equal to the sample mean of all rewards observed when action $a$ was selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f411f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "To keep notation minimal, we use $Q(a)$ in the main text. For this proof only, we introduce one additional counter:\n",
    "\n",
    "- $k$: the number of times action $a$ has been selected.\n",
    "\n",
    "Let $Q_k(a)$ be the value of $Q(a)$ after $k$ selections of action $a$, and let $R_a^{(k)}$ be the reward observed on the $k$-th selection.\n",
    "\n",
    "Then the sample-average update becomes:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(a)=Q_k(a)+\\frac{1}{k+1}\\left(R_a^{(k+1)}-Q_k(a)\\right).\n",
    "$$\n",
    "\n",
    "#### **Base Case ($k=0$)**\n",
    "Assume $Q_0(a)=0$. After the first time action $a$ is selected:\n",
    "\n",
    "$$\n",
    "Q_1(a)=R_a^{(1)}.\n",
    "$$\n",
    "\n",
    "#### **Inductive Hypothesis**\n",
    "Assume after $k$ selections of action $a$:\n",
    "\n",
    "$$\n",
    "Q_k(a)=\\frac{1}{k}\\sum_{j=1}^{k}R_a^{(j)}.\n",
    "$$\n",
    "\n",
    "#### **Inductive Step**\n",
    "Using the update:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(a)=Q_k(a)+\\frac{1}{k+1}\\left(R_a^{(k+1)}-Q_k(a)\\right),\n",
    "$$\n",
    "\n",
    "substitute the hypothesis:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(a)=\\frac{1}{k}\\sum_{j=1}^{k}R_a^{(j)}+\\frac{1}{k+1}\\left(R_a^{(k+1)}-\\frac{1}{k}\\sum_{j=1}^{k}R_a^{(j)}\\right),\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(a)=\\frac{1}{k+1}\\sum_{j=1}^{k+1}R_a^{(j)}.\n",
    "$$\n",
    "\n",
    "Thus, by induction, the iterates equal the sample mean. Returning to default notation, the current value after trial $n$ can be written as:\n",
    "\n",
    "$$\n",
    "Q(a)=\\frac{1}{N(a)}\\sum_{n'=1}^{n}R_{n'}\\,\\mathbf{1}_{\\{A_{n'}=a\\}}.\n",
    "$$\n",
    "\n",
    "So the online rule with step size $1/N(a)$ converges to the empirical mean reward for each action in a stationary environment.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271411c-bbe6-4558-bc63-ace231a68644",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "We have previously seen the leaky integrator model (known as exponential average in its discrete form):\n",
    "\n",
    "$$\\tau \\frac{dX}{dt} = -X + I(t),$$\n",
    "\n",
    "where $X$ is the updated quantity, $I(t)$ is an external input, and $\\tau$ is a time constant.\n",
    "\n",
    "Notation note: here $I(t)$ is a continuous-time input signal (capital i), not the index $i$ used in $\\sum_i$ for reward values.\n",
    "\n",
    "Show that this model exhibits exponential forgetting by setting $I(t) = 0$. You can do this by replacing both sides with an appropriate exponential function and verify that it satisfies the equation.\n",
    "\n",
    "Discuss how the online update rule for the $Q(a)$ values relates to the leaky integrator differential equation seen earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a28b2-eba0-411f-98df-cc03c968e6fa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "The leaky integrator model is given by:\n",
    "\n",
    "$$\\tau \\frac{dX}{dt} = -X + I(t),$$\n",
    "\n",
    "where $X$ is the quantity being updated, $I(t)$ is an external input, and $\\tau$ is a time constant.\n",
    "\n",
    "Notation note: $I(t)$ here is an input function of continuous time, not the index $i$ used in the bandit distribution sum.\n",
    "\n",
    "**Exponential Forgetting ($I(t) = 0$)**\n",
    "\n",
    "Setting $I(t) = 0$, the equation simplifies to:\n",
    "\n",
    "$$\\tau \\frac{dX}{dt} = -X.$$\n",
    "\n",
    "This is a standard first-order linear differential equation. To solve, assume a solution of the form:\n",
    "\n",
    "$$X(t) = C e^{-t/\\tau}.$$\n",
    "\n",
    "Differentiating:\n",
    "\n",
    "$$\\frac{dX}{dt} = -\\frac{C}{\\tau} e^{-t/\\tau}.$$\n",
    "\n",
    "Substituting into the differential equation:\n",
    "\n",
    "$$\\tau \\left(-\\frac{C}{\\tau} e^{-t/\\tau} \\right) = -C e^{-t/\\tau},$$\n",
    "\n",
    "which simplifies to an identity, confirming that:\n",
    "\n",
    "$$X(t) = C e^{-t/\\tau}$$\n",
    "\n",
    "is the correct solution, demonstrating exponential decay or forgetting over time.\n",
    "\n",
    "\n",
    "**Discretisation and Connection to Online Updates**\n",
    "\n",
    "Approximating the derivative using its definition leads to:\n",
    "\n",
    "$$\\tau \\frac{\\Delta X}{\\Delta t} \\approx -X + I(t),$$\n",
    "\n",
    "solving for $\\Delta X$:\n",
    "\n",
    "$$\\Delta X = \\frac{\\Delta t}{\\tau} (I(t) - X).$$\n",
    "\n",
    "Defining the learning rate as:\n",
    "\n",
    "$$\\eta = \\frac{\\Delta t}{\\tau},$$\n",
    "\n",
    "this becomes:\n",
    "\n",
    "$$\\Delta X = \\eta (I(t) - X).$$\n",
    "\n",
    "Recognising that by considering $X$ as the estimated reward $Q(a)$ and $I(t)$ as the received reward $R_n$, we obtain the familiar online update rule:\n",
    "\n",
    "$$\\Delta Q(a) = \\eta (R_n - Q(a)).$$\n",
    "\n",
    "This shows that the online update rule is a discrete-time approximation of the leaky integrator model.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6a02c-3eb4-439b-94c4-12c32008c0bc",
   "metadata": {},
   "source": [
    "## Policies and the Exploration-Exploitation Dilemma \n",
    "\n",
    "Imagine you're navigating the diverse culinary landscape of a new city. After several dining experiences, you discover a restaurant that exceeds all expectations. Now, you face a classic decision: to explore further or to exploit this find. Exploring entails the risk of less satisfying meals, while exploiting ensures enjoyment but may preclude the discovery of superior options. This scenario illustrates the exploration-exploitation dilemma in reinforcement learning, where agents must balance the pursuit of known rewards against the potential of discovering greater rewards through exploration.\n",
    "\n",
    "In reinforcement learning, the probability by which an agent chooses their actions is known as a **policy**. Several policies exist, that navigate the exploration-exploitation dilemma in different ways. Here, we will present fixed policies, and discuss their benefits and drawbacks:\n",
    "\n",
    "- **Greedy Policy**: This policy always selects the action with the highest estimated reward based on current knowledge. It is formally expressed as:\n",
    "\n",
    "  $$a_n = \\underset{a}{\\mathrm{argmax}}\\ Q(a)$$\n",
    "\n",
    "  Here, $\\underset{a}{\\mathrm{argmax}}\\ Q(a)$ returns the action $a$ that maximizes the current estimated reward. While the greedy policy maximizes immediate rewards, it may overlook potentially higher rewards from less-explored actions.\n",
    "\n",
    "- **Optimistic Greedy Policy**: An optimistic initialization encourages exploration by starting with a high initial estimate for all actions. The action selection mechanism is the same as the greedy policy, but the initial optimism motivates exploration early on. This can be represented by initializing $Q(a)$ to a value higher than any expected reward, promoting exploration of all actions before converging to exploitation.\n",
    "\n",
    "- **Epsilon-Greedy Policy**: This policy introduces a probability $\\epsilon$ to explore random actions, allowing the agent to intermittently diverge from the greedy choice. It can be mathematically described as:\n",
    "\n",
    "  $$a_n = \\begin{cases} \n",
    "  \\text{a random action} & \\text{with probability } \\epsilon \\\\\n",
    "  \\underset{a}{\\mathrm{argmax}}\\ Q(a) & \\text{with probability } 1 - \\epsilon\n",
    "  \\end{cases}$$\n",
    "\n",
    "  In this context, $\\text{a random action}$ with probability $\\epsilon$ allows for exploration, and $\\underset{a}{\\mathrm{argmax}}\\ Q(a)$ with probability $1 - \\epsilon$ chooses the action believed to offer the highest reward, thus balancing exploration and exploitation.\n",
    "\n",
    "- **Softmax Policy**: Unlike the aforementioned policies that make decisions based on a deterministic or semi-random criterion, the Softmax policy selects actions based on a probability distribution over the estimated rewards. The probability of selecting action $a$ at trial $n$ is given by:\n",
    "\n",
    "  $$P(a) = \\frac{e^{Q(a)/\\tau}}{\\sum_{b} e^{Q(b)/\\tau}}$$\n",
    "\n",
    "  where $\\tau$ is a temperature parameter that modulates the degree of exploration: a high $\\tau$ results in more exploratory behavior, while a low $\\tau$ makes the policy behave more greedily. This policy elegantly balances exploration and exploitation by probabilistically favoring actions with higher expected rewards but still allowing for exploration.\n",
    "\n",
    "These policies represent distinct approaches to navigating the exploration-exploitation trade-off. The **Greedy Policy** prioritizes known rewards, potentially at the cost of missing out on better options. The **Optimistic Greedy Policy** mitigates this by fostering initial exploration through optimistic value estimates. The **Epsilon-Greedy Policy** offers a compromise, allowing for controlled exploration while primarily focusing on exploiting known information. The **Softmax Policy** provides a probabilistic method for action selection, smoothly transitioning between exploration and exploitation based on the temperature parameter $\\tau$.\n",
    "\n",
    "The policy is the final ingredient needed to fully define the multi-armed bandit problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1b83d-3aee-4a00-a7ff-8acc3ac34530",
   "metadata": {},
   "source": [
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "Now that we have defined rewards, action-value estimates, and policies, we can fully describe the multi-armed bandit problem, which is essentially a decision-making problem.\n",
    "\n",
    "1. **Environment**:  \n",
    "   - In the multi-armed bandit problem the environment consists of a single state.  \n",
    "   - The environment consists of multiple actions, e.g. **10 actions** $a \\in \\{1, 2, \\ldots, 10\\}$, each associated with a reward probability distribution.  \n",
    "   - The true expected reward $q^*(a)$ for each action $a$ is unknown to the agent.\n",
    "\n",
    "2. **Agent**:  \n",
    "   - The agent interacts with the environment by selecting actions over a series of trials.  \n",
    "   - The agent's knowledge about the environment is encapsulated in its policy $\\pi$.\n",
    "\n",
    "3. **Policy ($\\pi$)**:  \n",
    "   - A policy $\\pi$ determines how the agent selects actions based on past observations and rewards.  \n",
    "   - The policy can be **deterministic** or **stochastic**.\n",
    "\n",
    "### Algorithm Outline\n",
    "\n",
    "1. **Initialization**:  \n",
    "   - Initialize the estimate of action-value $Q(a)$ for all $a$.  \n",
    "   - Initialize the count $N(a)$ of times each action has been selected to $0$.  \n",
    "   - Set N to the total number of trials.\n",
    "\n",
    "2. **For n = 1 to N, do**:  \n",
    "   \n",
    "   - **Policy Decision**: \n",
    "       Based on the current action-value estimates $Q(a)$, select an action $a$ using policy $\\pi$ (e.g. Epsilon-Greedy).  \n",
    "   \n",
    "   - **Action and Reward**: Take action $a$, and observe reward $R_n$.  \n",
    "\n",
    "   - **Increase counter**:  \n",
    "      $$ N(a) = N(a) + 1 $$\n",
    "\n",
    "   - **Update Action-Value Estimates**:  \n",
    "      For **stationary environments**:  \n",
    "      $$ Q(a) = Q(a) + \\frac{1}{N(a)}(R_n - Q(a)) $$\n",
    "\n",
    "3. **Optional bookkeeping for proofs/plots/code**:\n",
    "   - If you store the full trajectory, denote it as $Q_n(a)$, or in code as an array like `Q_history[n, a]`.\n",
    "   - This indexed form is for tracking evolution; the update rule itself is still written on the current value $Q(a)$.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of the algorithm is to maximize the cumulative expected reward over trials.\n",
    "\n",
    "### Non-Stationary Environments\n",
    "\n",
    "The algorithm above can be adjusted by modifying the update rule as:  \n",
    "$$ Q(a) = Q(a) + \\eta (R_n - Q(a)) $$\n",
    "where $\\eta$ is a fixed step size (learning rate), ensuring that past rewards do not dominate future updates. A decaying learning rate can help mitigate instability and improve adaptability in non-stationary environments, as discussed earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184a5a5-9d52-437b-8462-04b3b649559b",
   "metadata": {},
   "source": [
    "# Exercise\n\nWe will explore bandits where rewards follow a binomial distribution, and where rare rewards are more valuable than common ones. Such conditions can lead agents to prematurely favor certain arms, being \"trapped\" in what is perceived as the best option based on incomplete information. Adjusting the policy becomes crucial to train an effective agent in these environments.\n\nThe provided code snippet (below) outlines a basic framework for a 10-arm bandit problem, in a stationary environment. Executing this code will reveal the expected rewards for each arm. Key (hyper-)parameters for exploration include the learning rate, which indicates the rate at which $Q$ estimates are updated; epsilon, determining the balance between exploration and exploitation for the Epsilon-Greedy; and the temperature parameter for the Softmax policy, which adjusts the degree of exploration based on the reward distribution. By hyper-parameters, we refer to parameters that do not directly change as a consequence of the algorithmic updates, but you can set them and they affect the learning; to distinguish them from the \"learnable\" parameters, those that are adapted by the algorithm, in our case, the $Q$ values.\n\nNote the intentional gaps left for you to fill in, such as policy implementations and the agent update rule. The `np.random.choice` function will be instrumental in implementing the softmax policy.\n\nThe code follows the OOP pattern from the primer (Notebook 0, Section 6): the environment is a class (`BanditProblem`), the learner is a class (`BanditAgent`) that stores and updates $Q$, and each policy is a lightweight class with a `select_action(Q)` method. The agent receives its policy via composition (`self.policy = policy`), so policies can be swapped without rewriting the agent. The simulation loop is a standalone function \u2014 it has no state of its own.\n\nAfter filling in the gaps, try different parameters to deepen your understanding of exploration/exploitation, noise, and learning rate, and confirm the concepts we discussed previously. A large part in such models is tuning, selecting the hyper-parameters appropriately to obtain good performance.\n\nYou could also enhance the policies, particularly the Epsilon-Greedy and Softmax strategies, as well as the learning rate scheme by heuristics. You could expand the implementation to also return cumulative average reward, so your graphs will be smoother, and you may need to average across fewer episodes, or remove the need for additional smoothing. We will not provide those in the solution and leave it to you to devise and implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d0df3-ed36-4e9f-b445-f0dcdc037e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass BanditProblem:\n    \"\"\"Multi-armed bandit with binary rewards of varying magnitude.\"\"\"\n\n    def __init__(self, k=10):\n        self.k = k\n        self.p_success = np.random.uniform(0.1, 0.9, self.k)\n        self.reward_magnitude = (1 / self.p_success - 1)\n\n    def get_reward(self, action):\n        success = np.random.rand() < self.p_success[action]\n        return success * self.reward_magnitude[action]\n\n    def return_expectations(self):\n        return self.p_success * self.reward_magnitude\n\n    def return_arms(self):\n        return self.k\n\n    def plot_reward_frequency(self):\n        arms = np.arange(1, self.k + 1)\n        plt.figure(figsize=(10, 6))\n        plt.bar(arms, self.p_success, color='blue')\n        plt.title('Reward Frequency for Each Arm')\n        plt.xlabel('Arm')\n        plt.ylabel('Reward Frequency')\n        plt.xticks(arms)\n        plt.show()\n\n    def plot_true_reward_distributions(self):\n        arms = np.arange(1, self.k + 1)\n        plt.figure(figsize=(10, 6))\n        plt.bar(arms, self.return_expectations(), color='skyblue')\n        plt.title('Expected Reward for Each Arm')\n        plt.xlabel('Arm')\n        plt.ylabel('Expected Reward')\n        plt.xticks(arms)\n        plt.show()\n\n\nclass EpsilonGreedyPolicy:\n    \"\"\"Decision rule: usually pick the best, sometimes explore.\"\"\"\n\n    def __init__(self, epsilon):\n        self.epsilon = epsilon\n\n    def select_action(self, Q):\n        \"\"\"Given Q-values, return an action.\n\n        TODO: implement epsilon-greedy with random tie-breaking.\n        Placeholder: uniform random action to keep notebook runnable.\"\"\"\n        return np.random.randint(len(Q))\n\n\nclass SoftmaxPolicy:\n    \"\"\"Decision rule: sample actions proportional to estimated values.\"\"\"\n\n    def __init__(self, tau):\n        self.tau = tau\n\n    def select_action(self, Q):\n        \"\"\"Given Q-values, return an action.\n\n        TODO: implement softmax action selection.\n        Placeholder: uniform random action to keep notebook runnable.\"\"\"\n        return np.random.randint(len(Q))\n\n\nclass BanditAgent:\n    \"\"\"Learner that stores action-value estimates and updates them.\"\"\"\n\n    def __init__(self, num_actions, eta, policy):\n        self.num_actions = num_actions\n        self.eta = eta\n        self.policy = policy\n        self.reset()\n\n    def reset(self):\n        self.Q = np.zeros(self.num_actions)\n        self.action_counts = np.zeros(self.num_actions, dtype=int)\n\n    def select_action(self):\n        return self.policy.select_action(self.Q)\n\n    def update(self, action, reward):\n        \"\"\"TODO: implement constant step-size update for Q(action).\"\"\"\n        return\n\n\ndef simulate_bandit(agent, bandit, trials=100):\n    reward_history = np.zeros(trials)\n    agent.reset()\n    for n in range(trials):\n        action = agent.select_action()\n        reward = bandit.get_reward(action)\n        agent.update(action, reward)\n        reward_history[n] = reward\n    return reward_history\n\n\ndef smooth_data(data, alpha=0.1):\n    if len(data) == 0:\n        return np.array([])\n    smoothed = np.zeros(len(data))\n    smoothed[0] = data[0]\n    for i in range(1, len(data)):\n        smoothed[i] = (1 - alpha) * smoothed[i - 1] + alpha * data[i]\n    return smoothed\n\n\ndef simulate_and_average(bandit, policy, eta, trials=100, runs=10):\n    num_actions = bandit.return_arms()\n    total_rewards = np.zeros(trials)\n    for _ in range(runs):\n        agent = BanditAgent(num_actions, eta, policy)\n        total_rewards += simulate_bandit(agent, bandit, trials)\n    return smooth_data(total_rewards / runs, alpha=0.01)\n\n\nk = 10\ntrials = 1000  # shorter for test\nruns = 5\nmy_bandit = BanditProblem(k)\nmy_bandit.plot_true_reward_distributions()\nmy_bandit.plot_reward_frequency()\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afe859-0390-420b-b417-e263a743d787",
   "metadata": {},
   "source": [
    "<details>\n  <summary>Show Solution</summary>\n\n ```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass BanditProblem:\n    \"\"\"Multi-armed bandit with binary rewards of varying magnitude.\"\"\"\n\n    def __init__(self, k=10):\n        \"\"\"\n        Args:\n            k (int): Number of arms.\n        \"\"\"\n        self.k = k\n        self.p_success = np.random.uniform(0.1, 0.9, self.k)\n        self.reward_magnitude = (1 / self.p_success - 1)\n\n    def get_reward(self, action):\n        \"\"\"Sample a reward from the chosen arm.\"\"\"\n        success = np.random.rand() < self.p_success[action]\n        return success * self.reward_magnitude[action]\n\n    def return_expectations(self):\n        \"\"\"Return the true expected reward for each arm.\"\"\"\n        return self.p_success * self.reward_magnitude\n\n    def return_arms(self):\n        \"\"\"Return the number of arms.\"\"\"\n        return self.k\n\n    def plot_reward_frequency(self):\n        \"\"\"Bar chart of success probabilities.\"\"\"\n        arms = np.arange(1, self.k + 1)\n        plt.figure(figsize=(10, 6))\n        plt.bar(arms, self.p_success, color='blue')\n        plt.title('Reward Frequency for Each Arm')\n        plt.xlabel('Arm')\n        plt.ylabel('Reward Frequency')\n        plt.xticks(arms)\n        plt.show()\n\n    def plot_true_reward_distributions(self):\n        \"\"\"Bar chart of expected rewards per arm.\"\"\"\n        arms = np.arange(1, self.k + 1)\n        plt.figure(figsize=(10, 6))\n        plt.bar(arms, self.return_expectations(), color='skyblue')\n        plt.title('Expected Reward for Each Arm')\n        plt.xlabel('Arm')\n        plt.ylabel('Expected Reward')\n        plt.xticks(arms)\n        plt.show()\n\n\n# --- Policies (classes, following Notebook 0 Section 6) ---\n\nclass EpsilonGreedyPolicy:\n    \"\"\"Decision rule: usually pick the best, sometimes explore.\"\"\"\n\n    def __init__(self, epsilon):\n        \"\"\"\n        Args:\n            epsilon (float): Probability of choosing a random action.\n        \"\"\"\n        self.epsilon = epsilon\n\n    def select_action(self, Q):\n        \"\"\"Given Q-values, return an action.\"\"\"\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(len(Q))\n        max_q = np.max(Q)\n        best_actions = np.where(Q == max_q)[0]\n        return np.random.choice(best_actions)\n\n\nclass SoftmaxPolicy:\n    \"\"\"Decision rule: sample actions proportional to estimated values.\"\"\"\n\n    def __init__(self, tau):\n        \"\"\"\n        Args:\n            tau (float): Temperature parameter (higher = more exploration).\n        \"\"\"\n        self.tau = tau\n\n    def select_action(self, Q):\n        \"\"\"Given Q-values, return an action.\"\"\"\n        scaled = Q / max(self.tau, 1e-8)\n        shifted = scaled - np.max(scaled)\n        exp_q = np.exp(shifted)\n        probabilities = exp_q / np.sum(exp_q)\n        return np.random.choice(len(Q), p=probabilities)\n\n\nclass BanditAgent:\n    \"\"\"Learner that stores action-value estimates and updates them.\n\n    The action-selection policy is passed in as a component (composition).\n    \"\"\"\n\n    def __init__(self, num_actions, eta, policy):\n        \"\"\"\n        Args:\n            num_actions (int): Number of available actions.\n            eta (float): Learning rate.\n            policy: An object with a select_action(Q) method.\n        \"\"\"\n        self.num_actions = num_actions\n        self.eta = eta\n        self.policy = policy\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset internal estimates and action counts.\"\"\"\n        self.Q = np.zeros(self.num_actions)\n        self.action_counts = np.zeros(self.num_actions, dtype=int)\n\n    def select_action(self):\n        \"\"\"Select an action by delegating to the policy.\"\"\"\n        return self.policy.select_action(self.Q)\n\n    def update(self, action, reward):\n        \"\"\"Constant step-size update for the selected action.\"\"\"\n        self.action_counts[action] += 1\n        self.Q[action] += self.eta * (reward - self.Q[action])\n\n\n# --- Simulation helpers (standalone functions \u2014 no state needed) ---\n\ndef simulate_bandit(agent, bandit, trials=100):\n    \"\"\"Run one bandit training run and return rewards per trial.\"\"\"\n    reward_history = np.zeros(trials)\n    agent.reset()\n    for n in range(trials):\n        action = agent.select_action()\n        reward = bandit.get_reward(action)\n        agent.update(action, reward)\n        reward_history[n] = reward\n    return reward_history\n\n\ndef smooth_data(data, alpha=0.1):\n    \"\"\"Exponential moving average for smoothing a time series.\"\"\"\n    if len(data) == 0:\n        return np.array([])\n    smoothed = np.zeros(len(data))\n    smoothed[0] = data[0]\n    for i in range(1, len(data)):\n        smoothed[i] = (1 - alpha) * smoothed[i - 1] + alpha * data[i]\n    return smoothed\n\n\ndef simulate_and_average(bandit, policy, eta, trials=100, runs=10):\n    \"\"\"Average reward curves over independent runs, then smooth.\"\"\"\n    num_actions = bandit.return_arms()\n    total_rewards = np.zeros(trials)\n    for _ in range(runs):\n        agent = BanditAgent(num_actions, eta, policy)\n        total_rewards += simulate_bandit(agent, bandit, trials)\n    return smooth_data(total_rewards / runs, alpha=0.01)\n\n\n# --- Main ---\n\nk = 10\ntrials = 10000\nruns = 100\nepsilon = 0.2\ntau = 0.3\neta = 0.001\n\nmy_bandit = BanditProblem(k)\nmy_bandit.plot_true_reward_distributions()\nmy_bandit.plot_reward_frequency()\n\nrewards_eg = simulate_and_average(\n    my_bandit, EpsilonGreedyPolicy(epsilon=epsilon), eta,\n    trials=trials, runs=runs\n)\nrewards_g = simulate_and_average(\n    my_bandit, EpsilonGreedyPolicy(epsilon=0), eta,\n    trials=trials, runs=runs\n)\nrewards_sm = simulate_and_average(\n    my_bandit, SoftmaxPolicy(tau=tau), eta,\n    trials=trials, runs=runs\n)\n\nprint(\"True mean rewards:\\n\", my_bandit.return_expectations())\n\nplt.figure(figsize=(10, 6))\nplt.plot(rewards_eg, label='Epsilon-Greedy')\nplt.plot(rewards_g,  label='Greedy')\nplt.plot(rewards_sm, label='Soft-Max')\nmax_r = np.max(my_bandit.return_expectations())\nmean_r = np.mean(my_bandit.return_expectations())\nplt.axhline(y=max_r, color='r', linestyle='--', label=f'Max Expected: {max_r:.2f}')\nplt.axhline(y=mean_r, color='b', linestyle='--', label=f'Mean Expected: {mean_r:.2f}')\nplt.xlabel('Trials')\nplt.ylabel('Average Reward')\nplt.title('Performance of Various Policies Over Time')\nplt.legend()\nplt.show()\n\n```\n</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}