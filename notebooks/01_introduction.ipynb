{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/01_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Reinforcement Learning Tells Me About Happiness\n",
    "\n",
    "I'd go so far as to say that there is no other machine learning technique as relevant to life as reinforcement learning. It is not only its origin—reinforcement learning is rooted in psychological experiments—but also the fact that reinforcement learning ideas can be found in philosophical documents dating back to at least the time of Plato. Even today, reinforcement learning can tell me how to achieve happiness.\n",
    "\n",
    "If you think I am biased, I will agree with you. This is my interpretation of the philosophical texts I have read, the technical books I teach, my studies on reinforcement learning, and—of course—my own experiences. Thus, I am going to start from Epicurus, who is one of my favourite philosophers because he is misunderstood as a hedonist, while he was actually practising and teaching a theory akin to reinforcement learning.\n",
    "\n",
    "Epicurus believed in optimising a reward function across one’s life. He explicitly said that pursuing the pleasures of the flesh is not the point. For the exact phrasing, I will direct you to my other text *[\"Was Epicurus the Father of Reinforcement Learning?\"](https://arxiv.org/abs/1710.04582)*, which is based on my talk for our Machine Learning retreat in 2017. Back then, I thought that since I became the head of the Machine Learning group, I no longer needed to prove myself by giving technical talks. Besides, at 9:00 am, people would rather hear something different for a change. It was meant to be an amusing talk.\n",
    "\n",
    "Epicurus suggests we should choose the actions that benefit our souls. This is a very interesting choice of words for someone who limited the presence of gods in his philosophy and who didn’t believe in the afterlife. I will therefore interpret avoiding actions that bring turmoil in our souls as what is good in the long run. Epicurus was suggesting that we also incorporate any future punishment that today’s pleasure will bring. If you have ever got drunk, you certainly know what I am talking about.\n",
    "\n",
    "Please forgive me if I turn everything into an equation. I assure you that on the surface it is a most trivial one, though in its essence it is the meaning of it all. We can write the total returns of an action at time t to be:\n",
    "\n",
    "$$\n",
    "G(t) = R(t+1) + R(t + 2) + R(t + 3) + … + R(t + N)\n",
    "$$\n",
    "\n",
    "where $R(t)$ represents the reward-or, if negative, punishment-at time point $t$. Rewards can of course, be 0 meaning the absence of it. \n",
    "\n",
    "In writing this equation, I assume that I will eventually die, that my moments of pleasure and punishment are finite, and their importance doesn’t diminish with how far in the future they are. It is an important notion as I cannot maximise a function of infinite value. Otherwise, if I feel invincible, which I occasionally still do, I will have to write:\n",
    "\n",
    "$$\n",
    "G(t) = R(t+1) + R(t + 2)\\gamma + R(t + 3)\\gamma^2 + R(t + 4)\\gamma^3 + …\n",
    "$$\n",
    "\n",
    "Now I can live forever, but there is a discount factor $\\gamma$ multiplying each reward that I receive, a positive real number $\\gamma < 1$, which is raised to a power depending on how far away in the future the reward is. This just tells me that the reward I receive now will always be better than the same amount of reward that I will get next year, and it explains why I am so impatient.\n",
    "\n",
    "In the Epicurean philosophy, there are clear instructions or suggestions regarding the values of various actions. For instance, Epicurus’ advice is to pursue friendship rather than romance because the latter brings jealousy and pain. I will therefore write in an equation that the value of friendship is higher than the value of romance:\n",
    "\n",
    "$$\n",
    "Q(\\text{friendship}) > Q(\\text{romance})\n",
    "$$\n",
    "\n",
    "where $Q$ is the value of the action to be considered and is interpreted as the expected sum of all the future rewards (and punishments), discounted, of course, that can result from this action. Here, there is an omission: saying that the action is independent of our state is clearly an oversimplification. For instance, we can consider a state $S_t$ that includes our own \"state of mind\" and the other person involved at time $t$. The value of friendship itself must also depend on the person we choose to offer our friendship. A more complete statement is therefore:\n",
    "\n",
    "$$\n",
    "Q(S(t), \\text{friendship}) > Q(S(t), \\text{romance})\n",
    "$$\n",
    "\n",
    "You can argue that the correctness of this inequality may very well depend on the specific state $s$, but if I sample a state, on average this is more likely to be true. Of course, nobody says that $Q(S_t, \\text{friendship})$ cannot have a very low value itself if investing in friendship with the wrong person, but it is likely to still be higher than $Q(S_t, \\text{romance})$ if investing in romance with the same wrong person.\n",
    "\n",
    "Implicit here is the investment in all these actions. Any action of friendship, or of anything else in fact, rarely comes for free: it typically involves some effort. In this framework, and to keep things simple, I may consider the investment as a negative reward, i.e., something that I pay now in order to get a higher return in the future. The update rule for learning the $Q$ values according to the well-known [SARSA algorithm](http://incompleteideas.net/book/the-book.html) is:\n",
    "\n",
    "$$\n",
    "\\Delta Q(S_t, A_t) = \\alpha \\left( R(t) + \\gamma Q(S_{t+1}, A_{t+1}) \\right) - Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "which I can interpret as \"total reward minus expected reward.\" The value $Q(S_t, A_t)$ is the expectation for immediate and future rewards that I will receive when I am in state $S_t$ and choose action $A_t$. If my prediction is correct, $Q(S_t, A_t)$ should be equal to the immediate reward I will receive as a consequence of my action  $R_{t+1}$ plus the $Q$ value of the future state-action pair $(S_{t+1}, A_{t+1})$, i.e., my expected reward for the future state $S_{t+1}$ when taking future action $A_{t+1}$, discounted. If my expectation is wrong, then the terms do not match, and I need to update $Q(S_t, A_t)$.\n",
    "\n",
    "Given my investments on the way to my goal, represented as negative rewards, I need, and perhaps expect, future rewards that are large enough to compensate my investment and that arrive before they feel heavily discounted.\n",
    "\n",
    "Therefore, receiving a smaller reward than anticipated can feel like punishment: the difference is negative. The film that your friend told you is amazing might disappoint you if you watch it with great expectations. Correspondingly, I remember watching the end of [\"Lost\"](https://en.wikipedia.org/wiki/Lost_(TV_series)) many years after its first airing, having often heard how awful it was. However, it didn’t seem quite as bad to me. You see, after all the negative comments I had heard, my expectations were pretty low.\n",
    "\n",
    "In friendship or romance, or indeed anything else, great expectations and high investment are likely to lead to disappointment. Reinforcement learning suggests you should have low expectations in situations you cannot really control and should avoid over-investment. In doing so, any reward is more likely to feel rewarding and any punishment as less punishing.\n",
    "\n",
    "Ongoing investments also lead to a natural bias in the perception of people. I, as an external observer for someone else, may be aware of signs of success (rewards) but I am likely unaware of their investments (punishments). On the contrary, I am perfectly aware of my own investments, and therefore any perception of my personal success may feel less to me than in the eyes of other people. Sometimes it may even feel like a punishment: since I consumed the punishment first, the success may not be enough to make up for it—after all, I am impatient!\n",
    "\n",
    "This is how reinforcement learning tells me to live my life: enjoy simple things; do not expect too much from others; do not over-invest; and never underestimate the effort or investment that people made in reaching their goals. Is there an element of luck? Reinforcement learning says there is, though unless you are trapped in local maxima, you will eventually find the optimal solution given sufficient time. This, however, is a discussion for another time.\n",
    "\n",
    "*Eleni*\n",
    "\n",
    "**Acknowledgements:** Thanks to Peter Dayan for his amazingly fast feedback on this text (as always!) and for pointing me to the work of [Kent Berridge](https://www.ncbi.nlm.nih.gov/pubmed/28943891), who makes the distinction between “liking” vs “wanting,” as well as the work of [Robb Rutledge](https://www.ncbi.nlm.nih.gov/pubmed/25092308), a modern-day Epicurus who proposed a computational model of momentary subjective happiness. Also to ChatGPT for meticulous proofreading. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to NumPy\n",
    "\n",
    "**NumPy** is a fundamental library for numerical computing in Python. It provides the efficient, flexible multi-dimensional array object (`ndarray`) and a wide range of mathematical functions. In machine learning, data is most naturally represented as matrices because many machine learning algorithms (e.g. linear regression, neural networks, PCA) rely on linear algebra operations (e.g. matrix multiplication, transposition) that are highly optimized in NumPy. Datasets are also typically organized in matrices. NumPy’s support for vectorized computations means that operations on large datasets can be performed quickly without explicit loops. In what follows, familiarity with Python and the basics of Object-Oriented Programming (OOP) is assumed. The **[Official Python Tutorial](https://docs.python.org/3/tutorial/index.html)** can be used for revision or further learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Definitions:\n",
    "A **vector** is an ordered collection of numbers, which can represent points in space, directions, or quantities.\n",
    "\n",
    "- A **column vector** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- A **row vector** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = [v_1, v_2, v_3, \\dots, v_n]\n",
    "$$\n",
    "\n",
    "A **matrix** is a rectangular array of numbers arranged in rows and columns. In machine learning, matrices are used to represent datasets (where rows are data samples and columns are features), perform linear transformations, and facilitate operations like matrix multiplication for neural networks.\n",
    "\n",
    "- A **matrix** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column vector:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "Shape of the column vector: (4, 1)\n",
      "\n",
      "Row vector: [1 2 3 4]\n",
      "Shape of the row vector: (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "\n",
    "# Defining a column vector \n",
    "v = np.array([[1], [2], [3], [4]])  # Represents a 4-dimensional column vector\n",
    "\n",
    "# Row vector\n",
    "v_row = np.array([1, 2, 3, 4])  # Represents a 4-dimensional row vector\n",
    "\n",
    "print(\"Column vector:\\n\", v)\n",
    "print(\"Shape of the column vector:\", v.shape)\n",
    "\n",
    "print(\"\\nRow vector:\", v_row)\n",
    "print(\"Shape of the row vector:\", v_row.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In NumPy, a vector with shape `(n,)` is a 1-dimensional array. However, for matrix operations like dot products or when interfacing with ML models, we often require explicit 2-dimensional **column vectors** with shape `(n, 1)`.\n",
    "\n",
    "The **`reshape(-1, 1)`** method reshapes the array while inferring the number of rows automatically. Here:  \n",
    "- **`-1`** means *\"infer this dimension based on the length of the array,\"*  \n",
    "- **`1`** means *\"reshape it into a single column.\" \n",
    "- **Instead of** `1` you can give it another positive value **x** , assuming there are enough elements in the vector to fill exaclty **x** columns. \n",
    "\n",
    "When reshaping, **the elements are filled in row-major order (C-style order)**, which means:  \n",
    "- **Elements are filled column-wise from the original array.**  \n",
    "- The reshaped column vector maintains the same order of elements as the original row vector, just stacked vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([1, 2, 3, 4])  # Shape (4,)\n",
    "v_column = v.reshape(-1, 1)  # Reshape to (4, 1)\n",
    "\n",
    "print(v_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Initialising an $n$-dimensional vector with random numbers  \n",
    "\n",
    "Generate a vector of length $n$ with random numbers between 0 and 1. Set $n$ to a small positive value. Convert it to dimensions $(n,1)$. Display the dimensions of the vector.  \n",
    "\n",
    "**Hint:** Use `np.random.rand(n)`, which generates a vector of length $n$ with elements drawn from a uniform distribution over $[0, 1)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    " # Initialising an n-dimensional vector with random numbers\n",
    "n = 5  # Dimension of the vector\n",
    "v = np.random.rand(n)  # Generating a vector with random numbers between 0 and 1\n",
    "\n",
    "print(\"Random vector:\", v)  # Display the random vector\n",
    "\n",
    "# Checking the dimensions (shape) of the vector\n",
    "# Shape (n,) indicates a 1-dimensional array with n elements\n",
    "print(\"Shape of the vector:\", v.shape)\n",
    "\n",
    "# Converting a vector of shape (n,) to (n,1) (common in ML for matrix operations)\n",
    "# Reshape (-1, 1) infers the correct number of rows automatically\n",
    "v_column = v.reshape(-1, 1)\n",
    "print(\"Shape of the vector after convertion:\", v_column.shape)\n",
    "\n",
    "print(\"Reshaped column vector:\\n\", v_column)  # Display the reshaped column vector\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Dimensions: 2x3\n",
      "\n",
      "Matrix B:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Filled Matrix:\n",
      " [[2. 3. 4.]\n",
      " [3. 4. 5.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Defining a matrix explicitly\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])  # A 2x3 matrix with predefined elements\n",
    "\n",
    "m,n=A.shape\n",
    "print(f\"Matrix A:\\n{A}\\nDimensions: {m}x{n}\")  # Displays matrix A and its dimensions\n",
    "\n",
    "# Initialise a matrix with zeros\n",
    "B = np.zeros((2, 3))  # A 2x3 matrix with all elements as zeros\n",
    "\n",
    "print(f\"\\nMatrix B:\\n{B}\")  # Displays matrix B \n",
    "\n",
    "\n",
    "#Fill in a matrix by setting every element equal to [row index + column index] as in the mathematical definition, \n",
    "# where rows and colums start from 1.\n",
    "# Define matrix dimensions\n",
    "rows, cols = 3, 3 \n",
    "# Initialize an empty matrix\n",
    "matrix = np.empty((rows, cols))  # Define a 3x3 Matrix\n",
    "\n",
    "for i in range(1, rows+1): \n",
    "    for j in range(1, cols+1): \n",
    "        matrix[i-1, j-1] = i + j \n",
    "\n",
    "# Display the resulting matrix\n",
    "print(\"\\nFilled Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Initialising an $n \\times m$ matrix with random numbers  \n",
    "\n",
    "Generate a matrix with dimensions $3 \\times 4$ and fill it with uniformly distributed numbers between 0 and 1.  \n",
    "\n",
    "**Hint:** Use `np.random.rand(n, m)`, which generates an $n \\times m$ matrix with elements drawn from a uniform distribution over $[0, 1)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "# Initialising a matrix with random numbers\n",
    "# Matrix A with shape (3, 4), representing 3 samples with 4 features each\n",
    "A = np.random.rand(3, 4)\n",
    "\n",
    "print(\"Random matrix (3x4):\\n\", A)  # Display the random matrix\n",
    "\n",
    "# Checking the dimensions (shape) of the matrix\n",
    "print(\"Shape of the matrix:\", A.shape)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Vector Operations\n",
    "\n",
    "Below we revise key vector operations with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector-Scalar Multiplication\n",
    "\n",
    "For a vector $ \\mathbf{v} \\in \\mathbb{R}^{n} $ and a scalar $ \\alpha $,\n",
    "\n",
    "$$\n",
    "\\alpha \\mathbf{v} = \\begin{bmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\vdots \\\\ \\alpha v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element of the vector is multiplied by the scalar.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector: [1 2 3 4 5]\n",
      "Scaled vector: [ 3  6  9 12 15]\n"
     ]
    }
   ],
   "source": [
    "# Define a vector v\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Define a scalar\n",
    "alpha = 3\n",
    "\n",
    "# Multiply vector by scalar\n",
    "scaled_v = alpha * v\n",
    "\n",
    "print(\"Original vector:\", v)\n",
    "print(\"Scaled vector:\", scaled_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Addition\n",
    "\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $,\n",
    "\n",
    "$$\n",
    "\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Vector addition is performed **element-wise**.  \n",
    "\n",
    "**Note:** In mathematics, vectors are typically written as **column vectors**, but in NumPy, 1D arrays have shape **$(n,)$**, meaning they are neither row nor column vectors explicitly.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector u: [1 2 3 4 5]\n",
      "Vector v: [5 4 3 2 1]\n",
      "Sum u + v: [6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "# Define two vectors of the same size\n",
    "u = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Perform vector addition\n",
    "sum_vector = u + v\n",
    "\n",
    "print(\"Vector u:\", u)\n",
    "print(\"Vector v:\", v)\n",
    "print(\"Sum u + v:\", sum_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $, their **dot product** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i\n",
    "$$\n",
    "\n",
    "The dot product results in a **scalar** value.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product u · v: 35\n"
     ]
    }
   ],
   "source": [
    "# Compute the dot product of two vectors\n",
    "dot_product = np.dot(u, v)\n",
    "\n",
    "print(\"Dot Product u · v:\", dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Product\n",
    "\n",
    "For two vectors $ \\mathbf{u} \\in \\mathbb{R}^{m} $ and $ \\mathbf{v} \\in \\mathbb{R}^{n} $, the **outer product** results in an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{u} \\otimes \\mathbf{v} =\n",
    "\\begin{bmatrix}\n",
    "u_1 v_1 & u_1 v_2 & \\dots & u_1 v_n \\\\\n",
    "u_2 v_1 & u_2 v_2 & \\dots & u_2 v_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_m v_1 & u_m v_2 & \\dots & u_m v_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Unlike the dot product, the outer product results in a **matrix**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Product:\n",
      " [[ 5  4  3  2  1]\n",
      " [10  8  6  4  2]\n",
      " [15 12  9  6  3]\n",
      " [20 16 12  8  4]\n",
      " [25 20 15 10  5]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the outer product of two vectors\n",
    "outer_product = np.outer(u, v)\n",
    "\n",
    "print(\"Outer Product:\\n\", outer_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard (Element-wise) Product\n",
    "\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $, their **Hadamard (element-wise) product** is:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\circ \\mathbf{v} = \\begin{bmatrix} u_1 v_1 \\\\ u_2 v_2 \\\\ \\vdots \\\\ u_n v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Unlike the dot product, the Hadamard product **preserves the shape of the original vectors**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard Product u ∘ v: [5 8 9 8 5]\n"
     ]
    }
   ],
   "source": [
    "# Compute the element-wise (Hadamard) product\n",
    "hadamard_product = u * v\n",
    "\n",
    "print(\"Hadamard Product u ∘ v:\", hadamard_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Matrix Operations\n",
    "\n",
    "Below are key matrix operations with definitions, motivations, and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Vector Multiplication\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $ and a vector $ \\mathbf{v} \\in \\mathbb{R}^{n} $,  \n",
    "their multiplication results in another vector $ \\mathbf{b} \\in \\mathbb{R}^{m} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = A \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\n",
    "\\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}\n",
    "\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element of $ \\mathbf{b} $ is computed as:\n",
    "\n",
    "$$\n",
    "b_i = \\sum_{j=1}^{n} a_{ij} v_j\n",
    "$$\n",
    "\n",
    "where each row of $ A $ is multiplied by the vector $ \\mathbf{v} $.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[2 1 3]\n",
      " [4 2 1]\n",
      " [1 5 2]]\n",
      "\n",
      "Vector v:\n",
      " [1 2 3]\n",
      "\n",
      "Result of A * v:\n",
      " [13 11 17]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix A (3x3)\n",
    "A = np.array([[2, 1, 3],\n",
    "              [4, 2, 1],\n",
    "              [1, 5, 2]])\n",
    "\n",
    "# Define a vector v (3x1)\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "# Perform matrix-vector multiplication\n",
    "b = np.dot(A, v)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nVector v:\\n\", v)\n",
    "print(\"\\nResult of A * v:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Matrix Multiplication\n",
    "\n",
    "For two matrices $ A \\in \\mathbb{R}^{m \\times n} $ and $ B \\in \\mathbb{R}^{n \\times p} $,  \n",
    "their multiplication results in a new matrix $ C \\in \\mathbb{R}^{m \\times p} $:\n",
    "\n",
    "$$\n",
    "C = A B\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "where each row of $ A $ is multiplied by each column of $ B $.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[2 3]\n",
      " [1 4]\n",
      " [3 2]]\n",
      "\n",
      " Matrix B:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      " Result of A * B:\n",
      " [[14 19 24]\n",
      " [17 22 27]\n",
      " [11 16 21]]\n"
     ]
    }
   ],
   "source": [
    "# Define two matrices A (3x2) and B (2x3)\n",
    "A = np.array([[2, 3],\n",
    "              [1, 4],\n",
    "              [3, 2]])\n",
    "\n",
    "B = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Perform matrix-matrix multiplication\n",
    "C = A @ B # or C = np.dot(A, B)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\n Matrix B:\\n\", B)\n",
    "print(\"\\n Result of A * B:\\n\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard (Element-wise) Product\n",
    "\n",
    "For two matrices $ A, B \\in \\mathbb{R}^{m \\times n} $,  \n",
    "the Hadamard (element-wise) product is:\n",
    "\n",
    "$$\n",
    "(A \\circ B)_{ij} = A_{ij} B_{ij}\n",
    "$$\n",
    "\n",
    "Unlike matrix multiplication, this operation is performed **element-wise**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Matrix B:\n",
      " [[9 8 7]\n",
      " [6 5 4]\n",
      " [3 2 1]]\n",
      "\n",
      "Hadamard Product A ∘ B:\n",
      " [[ 9 16 21]\n",
      " [24 25 24]\n",
      " [21 16  9]]\n"
     ]
    }
   ],
   "source": [
    "# Define two matrices A and B (3x3)\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "B = np.array([[9, 8, 7],\n",
    "              [6, 5, 4],\n",
    "              [3, 2, 1]])\n",
    "\n",
    "# Compute the Hadamard product (element-wise multiplication)\n",
    "H = A * B\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nMatrix B:\\n\", B)\n",
    "print(\"\\nHadamard Product A ∘ B:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transpose\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $,  \n",
    "its transpose $ A^T \\in \\mathbb{R}^{n \\times m} $ is defined as:\n",
    "\n",
    "$$\n",
    "(A^T)_{ij} = A_{ji}\n",
    "$$\n",
    "\n",
    "which means the rows and columns of $ A $ are swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Transpose of A:\n",
      " [[1 3 5]\n",
      " [2 4 6]]\n",
      "\n",
      "Again, transpose of A:\n",
      " [[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A (3x2)\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "\n",
    "# Compute the transpose of A\n",
    "A_T = A.T\n",
    "# Alternative way to compute the transpose\n",
    "A_T_= np.transpose(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nTranspose of A:\\n\", A_T)\n",
    "print(\"\\nAgain, transpose of A:\\n\", A_T_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** An important and useful identity for machine learning is:\n",
    "\n",
    "$$\n",
    "(AB)^T = B^T A^T.\n",
    "$$\n",
    "\n",
    "This identity states that the **transpose of the product of two matrices** equals the **product of their transposes in reverse order**. It assumes that the matrices can be multiplied, meaning that **$A$ is an $m \\times n$ matrix** and **$B$ is an $n \\times p$ matrix**.\n",
    "\n",
    "Use **pen and paper** (or type if you prefer) to **prove** this identity. You will need to use the **definitions of matrix multiplication and transposition** and show that the **left-hand side equals the right-hand side** of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "$$\n",
    "(AB)_{ij} = \\sum_{k=1}^{n} A_{ik}B_{kj}\n",
    "$$\n",
    "\n",
    "Taking the transpose of $AB$:\n",
    "$$\n",
    "[(AB)^T]_{ji} = (AB)_{ij} = \\sum_{k=1}^{n} A_{ik}B_{kj}\n",
    "$$\n",
    "\n",
    "For the product $B^T A^T$:\n",
    "$$\n",
    "(B^T A^T)_{ji} = \\sum_{k=1}^{n} (B^T)_{jk}(A^T)_{ki} = \\sum_{k=1}^{n} B_{kj}A_{ik}\n",
    "$$\n",
    "\n",
    "Since matrix multiplication is commutative under addition:\n",
    "$$\n",
    "\\sum_{k=1}^{n} A_{ik}B_{kj} = \\sum_{k=1}^{n} B_{kj}A_{ik}\n",
    "$$\n",
    "\n",
    "Thus, proving $(AB)^T = B^T A^T$.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Demonstrate $(AB)^T = B^T A^T$ using numerical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices A and B\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Compute AB\n",
    "#AB =\n",
    "\n",
    "# Compute the transpose of AB\n",
    "#AB_T = \n",
    "\n",
    "# Compute the transpose of A and B\n",
    "#A_T = \n",
    "#B_T =\n",
    "\n",
    "# Compute B^T A^T\n",
    "#B_T_A_T =\n",
    "\n",
    "# Verify (AB)^T = B^T A^T\n",
    "\n",
    "#The assert statement in Python is used to check if a given condition is True. \n",
    "#If the condition is True, the program does nothing and moves to the next line of code. \n",
    "#However, if the condition is False, the program raises an AssertionError. \n",
    "#Optionally it can display a specified message.\n",
    "\n",
    "#assert np.array_equal(AB_T, B_T_A_T), \"The identity does not hold.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "# Define matrices A and B\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Compute AB\n",
    "AB = np.dot(A, B)\n",
    "\n",
    "# Compute the transpose of AB\n",
    "AB_T = np.transpose(AB)\n",
    "\n",
    "# Compute the transpose of A and B\n",
    "A_T = np.transpose(A)\n",
    "B_T = np.transpose(B)\n",
    "\n",
    "# Compute B^T A^T\n",
    "B_T_A_T = np.dot(B_T, A_T)\n",
    "\n",
    "# Verify (AB)^T = B^T A^T\n",
    "#The assert statement in Python is used to check if a given condition is True. \n",
    "#If the condition is True, the program does nothing and moves to the next line of code. \n",
    "#However, if the condition is False, the program raises an AssertionError. \n",
    "#Optionally it can display a specified message.\n",
    "assert np.array_equal(AB_T, B_T_A_T), \"The identity does not hold.\"\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare Matrix Multiplication Method\n",
    "\n",
    "Matrix multiplication is a fundamental operation in machine learning and numerical computing. There are different ways to compute it, and some are much faster than others.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Implement a function using the NumPy matrix multiplication operator.\n",
    "2. Implement your own function for matrix multiplication using for-loops.\n",
    "3. Measure and compare execution times for both methods.\n",
    "4. Visualize the results using `matplotlib`.\n",
    "\n",
    "Before we start, let's briefly introduce:\n",
    "- The `timeit` module for measuring execution time,\n",
    "- The `matplotlib` library for plotting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏱ Measuring Execution Time with `timeit`\n",
    "\n",
    "The `timeit` module helps measure how long a function takes to run.\n",
    "\n",
    "```python\n",
    "import timeit\n",
    "\n",
    "def example_function():\n",
    "    sum([i for i in range(1000)])  # List comprehension\n",
    "\n",
    "# Using timeit with a lambda function to pass arguments correctly\n",
    "time_taken = timeit.timeit(lambda: example_function(), number=100)\n",
    "print(\"Execution time:\", time_taken)\n",
    "```\n",
    "- **`number=100`** → Runs the function **100 times** and reports total time.\n",
    "- **Dividing total time by 100** → Gives the **average execution time per run**.\n",
    "\n",
    "Why Do We Use `lambda` in `timeit.timeit()`?\n",
    "- `timeit.timeit()` expects a **function reference**, not a direct function call.\n",
    "- If a function **has no parameters**, we can pass it directly:\n",
    "  ```python\n",
    "  timeit.timeit(example_function, number=100)  \n",
    "  ```\n",
    "- **But if a function requires arguments**, we **must use `lambda`** to delay execution:\n",
    "  ```python\n",
    "  timeit.timeit(lambda: example_function(5), number=100) \n",
    "  ```\n",
    "- `lambda` ensures that the function is **only executed when `timeit` calls it**, rather than immediately.\n",
    "\n",
    "\n",
    "Using `timeit` helps compare different implementations and find the most efficient approach.\n",
    "\n",
    "---\n",
    "\n",
    " 📊 Plotting Results with `matplotlib`\n",
    "\n",
    "`matplotlib` is a library for creating visualizations and analyzing data trends.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(x, y, label=\"Example Line\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Sample Plot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "This will generate a **simple line plot** with labeled axes and a legend. `plt.show()` displays the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Matrix multiplication using NumPy (efficient)\n",
    "# Define a function that takes two matrices as input and returns their product\n",
    "def multiply_numpy(A, B): \n",
    "    return A @ B  # NumPy's optimized matrix multiplication\n",
    "\n",
    "# Matrix multiplication using nested loops (inefficient)\n",
    "def multiply_loops(A, B):\n",
    "    result = 0 #incorrect, please correct this line\n",
    "  #complete the calcuations\n",
    "    return result\n",
    "\n",
    "# Benchmarking\n",
    "max_n = 100  \n",
    "numpy_times = np.zeros(max_n)\n",
    "loop_times = np.zeros(max_n)\n",
    "\n",
    "for size in range(1, max_n + 1):\n",
    "    #initialise matrices with random values\n",
    "    matrix1 = np.zeros((size, size)) #incorrect, please correct this line\n",
    "    matrix2 = np.zeros((size, size)) #incorrect, please correct this line\n",
    "\n",
    "    # Measure execution time using direct function calls (NumPy and loops)\n",
    "    numpy_times[size - 1] = timeit.timeit(lambda: multiply_numpy(matrix1, matrix2), number=1)\n",
    "\n",
    "# Plot execution times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matrix multiplication using NumPy (efficient)\n",
    "def multiply_numpy(A, B):\n",
    "    assert A.shape[1] == B.shape[0], \"Matrix dimensions do not match for multiplication!\"\n",
    "    return A @ B  # NumPy's optimized matrix multiplication\n",
    "\n",
    "# Matrix multiplication using nested loops (inefficient)\n",
    "def multiply_loops(A, B):\n",
    "    assert A.shape[1] == B.shape[0], \"Matrix dimensions do not match for multiplication!\"\n",
    "    rows, cols = A.shape  \n",
    "    cols, cols_B = B.shape  \n",
    "    result = np.zeros((rows, cols_B))  \n",
    "\n",
    "    for i in range(rows):  \n",
    "        for j in range(cols_B):  \n",
    "            for k in range(cols):  \n",
    "                result[i, j] += A[i, k] * B[k, j]  \n",
    "\n",
    "    return result\n",
    "\n",
    "# Benchmarking\n",
    "max_n = 100  \n",
    "numpy_times = np.zeros(max_n)\n",
    "loop_times = np.zeros(max_n)\n",
    "\n",
    "for size in range(1, max_n + 1):\n",
    "    matrix1 = np.random.rand(size, size)\n",
    "    matrix2 = np.random.rand(size, size)\n",
    "\n",
    "    # Measure execution time using direct function calls (NumPy and loops)\n",
    "    numpy_times[size - 1] = timeit.timeit(lambda: multiply_numpy(matrix1, matrix2), number=1)\n",
    "    loop_times[size - 1] = timeit.timeit(lambda: multiply_loops(matrix1, matrix2), number=1)\n",
    "\n",
    "# Plot execution times\n",
    "plt.plot(range(1, max_n + 1), numpy_times, label=\"NumPy multiplication\")\n",
    "plt.plot(range(1, max_n + 1), loop_times, label=\"For-loop multiplication\")\n",
    "\n",
    "plt.xlabel(\"Matrix Size $n$\")\n",
    "plt.ylabel(\"Execution Time (seconds)\")\n",
    "plt.title(\"Performance: NumPy vs. For-Loops\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "  ```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
