{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/01_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Reinforcement Learning Tells Me About Happiness\n",
    "\n",
    "I'd go so far as to say that there is no other machine learning technique as relevant to life as reinforcement learning. It is not only its origin‚Äîreinforcement learning is rooted in psychological experiments‚Äîbut also the fact that reinforcement learning ideas can be found in philosophical documents dating back to at least the time of Plato. Even today, reinforcement learning can tell me how to achieve happiness.\n",
    "\n",
    "If you think I am biased, I will agree with you. This is my interpretation of the philosophical texts I have read, the technical books I teach, my studies on reinforcement learning, and‚Äîof course‚Äîmy own experiences. Thus, I am going to start from Epicurus, who is one of my favourite philosophers because he is misunderstood as a hedonist, while he was actually practising and teaching a theory akin to reinforcement learning.\n",
    "\n",
    "Epicurus believed in optimising a reward function across one‚Äôs life. He explicitly said that pursuing the pleasures of the flesh is not the point. For the exact phrasing, I will direct you to my other text **[Was Epicurus the Father of Reinforcement Learning?](https://arxiv.org/abs/1710.04582)**, which is based on my talk for our Machine Learning retreat in 2017. Back then, I thought that since I became the head of the Machine Learning group, I no longer needed to prove myself by giving technical talks. Besides, at 9:00 am, people would rather hear something different for a change. It was meant to be an amusing talk.\n",
    "\n",
    "Epicurus suggests we should choose the actions that benefit our souls. This is a very interesting choice of words for someone who limited the presence of gods in his philosophy and who didn‚Äôt believe in the afterlife. I will therefore interpret avoiding actions that bring turmoil in our souls as what is good in the long run. Epicurus was suggesting that we also incorporate any future punishment that today‚Äôs pleasure will bring. If you have ever got drunk, you certainly know what I am talking about.\n",
    "\n",
    "Please forgive me if I turn everything into an equation. I assure you that on the surface it is a most trivial one, though in its essence it is the meaning of it all. We can write the total returns of an action at time t to be:\n",
    "\n",
    "$$\n",
    "G(t) = R(t+1) + R(t + 2) + R(t + 3) + ‚Ä¶ + R(t + N)\n",
    "$$\n",
    "\n",
    "where $R(t)$ represents the reward-or, if negative, punishment-at time point $t$. Rewards can of course, be 0 meaning the absence of it. \n",
    "\n",
    "In writing this equation, I assume that I will eventually die, that my moments of pleasure and punishment are finite, and their importance doesn‚Äôt diminish with how far in the future they are. It is an important notion as I cannot maximise a function of infinite value. Otherwise, if I feel invincible, which I occasionally still do, I will have to write:\n",
    "\n",
    "$$\n",
    "G(t) = R(t+1) + R(t + 2)\\gamma + R(t + 3)\\gamma^2 + R(t + 4)\\gamma^3 + ‚Ä¶\n",
    "$$\n",
    "\n",
    "Now I can live forever, but there is a discount factor $\\gamma$ multiplying each reward that I receive, a positive real number $\\gamma < 1$, which is raised to a power depending on how far away in the future the reward is. This just tells me that the reward I receive now will always be better than the same amount of reward that I will get next year, and it explains why I am so impatient.\n",
    "\n",
    "In the Epicurean philosophy, there are clear instructions or suggestions regarding the values of various actions. For instance, Epicurus‚Äô advice is to pursue friendship rather than romance because the latter brings jealousy and pain. I will therefore write in an equation that the value of friendship is higher than the value of romance:\n",
    "\n",
    "$$\n",
    "Q(\\text{friendship}) > Q(\\text{romance})\n",
    "$$\n",
    "\n",
    "where $Q$ is the value of the action to be considered and is interpreted as the expected sum of all the future rewards (and punishments), discounted, of course, that can result from this action. Here, there is an omission: saying that the action is independent of our state is clearly an oversimplification. For instance, we can consider a state $S_t$ that includes our own \"state of mind\" and the other person involved at time $t$. The value of friendship itself must also depend on the person we choose to offer our friendship. A more complete statement is therefore:\n",
    "\n",
    "$$\n",
    "Q(S(t), \\text{friendship}) > Q(S(t), \\text{romance})\n",
    "$$\n",
    "\n",
    "You can argue that the correctness of this inequality may very well depend on the specific state $s$, but if I sample a state, on average this is more likely to be true. Of course, nobody says that $Q(S_t, \\text{friendship})$ cannot have a very low value itself if investing in friendship with the wrong person, but it is likely to still be higher than $Q(S_t, \\text{romance})$ if investing in romance with the same wrong person.\n",
    "\n",
    "Implicit here is the investment in all these actions. Any action of friendship, or of anything else in fact, rarely comes for free: it typically involves some effort. In this framework, and to keep things simple, I may consider the investment as a negative reward, i.e., something that I pay now in order to get a higher return in the future. The update rule for learning the $Q$ values according to the well-known [SARSA algorithm](http://incompleteideas.net/book/the-book.html) is:\n",
    "\n",
    "$$\n",
    "\\Delta Q(S_t, A_t) = \\alpha \\left( R(t+1) + \\gamma Q(S_{t+1}, A_{t+1}) \\right) - Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "which I can interpret as \"total reward minus expected reward.\" The value $Q(S_t, A_t)$ is the expectation for immediate and future rewards that I will receive when I am in state $S_t$ and choose action $A_t$. If my prediction is correct, $Q(S_t, A_t)$ should be equal to the immediate reward I will receive as a consequence of my action  $R_{t+1}$ plus the $Q$ value of the future state-action pair $(S_{t+1}, A_{t+1})$, i.e., my expected reward for the future state $S_{t+1}$ when taking future action $A_{t+1}$, discounted. If my expectation is wrong, then the terms do not match, and I need to update $Q(S_t, A_t)$.\n",
    "\n",
    "Given my investments on the way to my goal, represented as negative rewards, I need, and perhaps expect, future rewards that are large enough to compensate my investment and that arrive before they feel heavily discounted.\n",
    "\n",
    "Therefore, receiving a smaller reward than anticipated can feel like punishment: the difference is negative. The film that your friend told you is amazing might disappoint you if you watch it with great expectations. Correspondingly, I remember watching the end of [\"Lost\"](https://en.wikipedia.org/wiki/Lost_(TV_series)) many years after its first airing, having often heard how awful it was. However, it didn‚Äôt seem quite as bad to me. You see, after all the negative comments I had heard, my expectations were pretty low.\n",
    "\n",
    "In friendship or romance, or indeed anything else, great expectations and high investment are likely to lead to disappointment. Reinforcement learning suggests you should have low expectations in situations you cannot really control and should avoid over-investment. In doing so, any reward is more likely to feel rewarding and any punishment as less punishing.\n",
    "\n",
    "Ongoing investments also lead to a natural bias in the perception of people. I, as an external observer for someone else, may be aware of signs of success (rewards) but I am likely unaware of their investments (punishments). On the contrary, I am perfectly aware of my own investments, and therefore any perception of my personal success may feel less to me than in the eyes of other people. Sometimes it may even feel like a punishment: since I consumed the punishment first, the success may not be enough to make up for it‚Äîafter all, I am impatient!\n",
    "\n",
    "This is how reinforcement learning tells me to live my life: enjoy simple things; do not expect too much from others; do not over-invest; and never underestimate the effort or investment that people made in reaching their goals. Is there an element of luck? Reinforcement learning says there is, though unless you are trapped in local maxima, you will eventually find the optimal solution given sufficient time. This, however, is a discussion for another time.\n",
    "\n",
    "*Eleni*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements:** I am grateful to Peter Dayan for his incredibly fast feedback (as always!) on the text above and for pointing me to the work of [Kent Berridge](https://www.ncbi.nlm.nih.gov/pubmed/28943891), who makes the distinction between ‚Äúliking‚Äù vs ‚Äúwanting,‚Äù as well as the work of [Robb Rutledge](https://www.ncbi.nlm.nih.gov/pubmed/25092308), a modern-day Epicurus who proposed a computational model of momentary subjective happiness. A special thanks goes to ChatGPT for their meticulous proofreading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** This material was developed with the invaluable assistance of generative AI, particularly ChatGPT and Claude. AI is a powerful tool for learning and exploration, and its use is encouraged. However, like any tool, it should be used thoughtfully‚Äîover-reliance can get in the way of understanding. This material can be opened directly in Google Colab using the button at the top. In Colab, Gemini is the default AI agent, which can assist with understanding and improving the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to NumPy\n",
    "\n",
    "**NumPy** is a fundamental library for numerical computing in Python. It provides the efficient, flexible multi-dimensional array object (`ndarray`) and a wide range of mathematical functions. In machine learning, data is most naturally represented as matrices because many machine learning algorithms (e.g. linear regression, neural networks, PCA) rely on linear algebra operations (e.g. matrix multiplication, transposition) that are highly optimized in NumPy. Datasets are also typically organized in matrices. NumPy‚Äôs support for vectorized computations means that operations on large datasets can be performed quickly without explicit loops. In what follows, familiarity with Python and the basics of Object-Oriented Programming (OOP) is assumed; however an **[OOP Primer](https://github.com/eleni-vasilaki/rl-notes/blob/main/notebooks/00_oop_primer.ipynb)** is provided that would be useful to review when reinforcement learning algorithms are introduced. The **[Official Python Tutorial](https://docs.python.org/3/tutorial/index.html)** can be used for revision or further learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Definitions:\n",
    "A **vector** is an ordered collection of numbers, which can represent points in space, directions, or quantities.\n",
    "\n",
    "- A **column vector** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- A **row vector** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = [v_1, v_2, v_3, \\dots, v_n]\n",
    "$$\n",
    "\n",
    "A **matrix** is a rectangular array of numbers arranged in rows and columns. In machine learning, matrices are used to represent datasets (where rows are data samples and columns are features), perform linear transformations, and facilitate operations like matrix multiplication for neural networks.\n",
    "\n",
    "- A **matrix** is written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column vector:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "Shape of the column vector: (4, 1)\n",
      "\n",
      "Row vector: [1 2 3 4]\n",
      "Shape of the row vector: (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a column vector as a 2D array with shape (4, 1)\n",
    "v = np.array([[1], [2], [3], [4]])\n",
    "\n",
    "# Define a row vector as a 1D array with shape (4,)\n",
    "v_row = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(\"Column vector:\\n\", v)\n",
    "print(\"Shape of the column vector:\", v.shape)\n",
    "\n",
    "print(\"\\nRow vector:\", v_row)\n",
    "print(\"Shape of the row vector:\", v_row.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In NumPy, a vector with shape `(n,)` is a 1-dimensional array. However, for matrix operations like dot products or when interfacing with ML models, we often require explicit 2-dimensional **column vectors** with shape `(n, 1)`.\n",
    "\n",
    "The **`reshape(-1, 1)`** method reshapes the array while inferring the number of rows automatically. Here:  \n",
    "- **`-1`** means *\"infer this dimension based on the length of the array,\"*  \n",
    "- **`1`** means *\"reshape it into a single column.\" \n",
    "- **Instead of** `1` you can give it another positive value **x** , assuming there are enough elements in the vector to fill exaclty **x** columns. \n",
    "\n",
    "When reshaping, **the elements are filled in row-major order (C-style order)**, which means:  \n",
    "- **Elements are filled column-wise from the original array.**  \n",
    "- The reshaped column vector maintains the same order of elements as the original row vector, just stacked vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([1, 2, 3, 4])  # Shape (4,)\n",
    "v_column = v.reshape(-1, 1)  # Reshape to (4, 1)\n",
    "\n",
    "print(v_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Initialising an $n$-dimensional vector with random numbers  \n",
    "\n",
    "Generate a vector of length $n$ with random numbers between 0 and 1. Set $n$ to a small positive value. Convert it to dimensions $(n,1)$. Display the dimensions of the vector.  \n",
    "\n",
    "**Hint:** Use `np.random.rand(n)`, which generates a vector of length $n$ with elements drawn from a uniform distribution over $[0, 1)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivation: use NumPy commands instead of manual loops for simpler, faster code.\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    " # Initialising an n-dimensional vector with random numbers\n",
    "n = 5  # Dimension of the vector\n",
    "v = np.random.rand(n)  # Generating a vector with random numbers between 0 and 1\n",
    "\n",
    "print(\"Random vector:\", v)  # Display the random vector\n",
    "\n",
    "# Checking the dimensions (shape) of the vector\n",
    "# Shape (n,) indicates a 1-dimensional array with n elements\n",
    "print(\"Shape of the vector:\", v.shape)\n",
    "\n",
    "# Converting a vector of shape (n,) to (n,1) (common in ML for matrix operations)\n",
    "# Reshape (-1, 1) infers the correct number of rows automatically\n",
    "v_column = v.reshape(-1, 1)\n",
    "print(\"Shape of the vector after convertion:\", v_column.shape)\n",
    "\n",
    "print(\"Reshaped column vector:\\n\", v_column)  # Display the reshaped column vector\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Dimensions: 2x3\n",
      "\n",
      "Matrix B (zeros):\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Filled Matrix:\n",
      " [[2. 3. 4.]\n",
      " [3. 4. 5.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix explicitly\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])  # A 2x3 matrix\n",
    "\n",
    "m, n = A.shape\n",
    "print(f\"Matrix A:\\n{A}\\nDimensions: {m}x{n}\")\n",
    "\n",
    "# Initialise a matrix of zeros\n",
    "B = np.zeros((2, 3))\n",
    "print(f\"\\nMatrix B (zeros):\\n{B}\")\n",
    "\n",
    "# Fill a matrix element-by-element: a[i,j] = i + j (1-indexed)\n",
    "rows, cols = 3, 3\n",
    "matrix = np.empty((rows, cols))\n",
    "\n",
    "for i in range(1, rows + 1):\n",
    "    for j in range(1, cols + 1):\n",
    "        matrix[i - 1, j - 1] = i + j\n",
    "\n",
    "print(\"\\nFilled Matrix:\\n\", matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Initialising an $n \\times m$ matrix with random numbers  \n",
    "\n",
    "Generate a matrix with dimensions $3 \\times 4$ and fill it with uniformly distributed numbers between 0 and 1.  \n",
    "\n",
    "**Hint:** Use `np.random.rand(n, m)`, which generates an $n \\times m$ matrix with elements drawn from a uniform distribution over $[0, 1)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivation: use NumPy broadcasting instead of manual loops for simpler, faster code.\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "# Initialising a matrix with random numbers\n",
    "# Matrix A with shape (3, 4), representing 3 samples with 4 features each\n",
    "A = np.random.rand(3, 4)\n",
    "\n",
    "print(\"Random matrix (3x4):\\n\", A)  # Display the random matrix\n",
    "\n",
    "# Checking the dimensions (shape) of the matrix\n",
    "print(\"Shape of the matrix:\", A.shape)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Vector Operations\n",
    "\n",
    "Below we revise key vector operations with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector-Scalar Multiplication\n",
    "\n",
    "For a vector $ \\mathbf{v} \\in \\mathbb{R}^{n} $ and a scalar $ \\alpha $,\n",
    "\n",
    "$$\n",
    "\\alpha \\mathbf{v} = \\begin{bmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\vdots \\\\ \\alpha v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element of the vector is multiplied by the scalar.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector: [1 2 3 4 5]\n",
      "Scaled vector: [ 3  6  9 12 15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a vector and a scalar\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "alpha = 3\n",
    "\n",
    "# Multiply vector by scalar (element-wise)\n",
    "scaled_v = alpha * v\n",
    "\n",
    "print(\"Original vector:\", v)\n",
    "print(\"Scaled vector:\", scaled_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Addition\n",
    "\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $,\n",
    "\n",
    "$$\n",
    "\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Vector addition is performed **element-wise**.  \n",
    "\n",
    "**Note:** In mathematics, vectors are typically written as **column vectors**, but in NumPy, 1D arrays have shape **$(n,)$**, meaning they are neither row nor column vectors explicitly.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector u: [1 2 3 4 5]\n",
      "Vector v: [5 4 3 2 1]\n",
      "Sum u + v: [6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors of the same size\n",
    "u = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Element-wise vector addition\n",
    "sum_vector = u + v\n",
    "\n",
    "print(\"Vector u:\", u)\n",
    "print(\"Vector v:\", v)\n",
    "print(\"Sum u + v:\", sum_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $, their **dot product** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i\n",
    "$$\n",
    "\n",
    "The dot product results in a **scalar** value.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product u ¬∑ v: 35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors of the same size\n",
    "u = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Dot product: returns a scalar\n",
    "dot_product = np.dot(u, v)\n",
    "\n",
    "print(\"Dot Product u ¬∑ v:\", dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Product\n",
    "\n",
    "For two vectors $ \\mathbf{u} \\in \\mathbb{R}^{m} $ and $ \\mathbf{v} \\in \\mathbb{R}^{n} $, the **outer product** results in an $ m \\times n $ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{u} \\otimes \\mathbf{v} =\n",
    "\\begin{bmatrix}\n",
    "u_1 v_1 & u_1 v_2 & \\dots & u_1 v_n \\\\\n",
    "u_2 v_1 & u_2 v_2 & \\dots & u_2 v_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_m v_1 & u_m v_2 & \\dots & u_m v_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Unlike the dot product, the outer product results in a **matrix**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Product:\n",
      " [[ 5  4  3  2  1]\n",
      " [10  8  6  4  2]\n",
      " [15 12  9  6  3]\n",
      " [20 16 12  8  4]\n",
      " [25 20 15 10  5]\n",
      " [30 24 18 12  6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors - they can be of different sizes for the outer product\n",
    "u = np.array([1, 2, 3, 4, 5, 6])\n",
    "v = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Outer product: returns an (m x n) matrix\n",
    "outer_product = np.outer(u, v)\n",
    "\n",
    "print(\"Outer Product:\\n\", outer_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard (Element-wise) Product\n",
    "\n",
    "For two vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{n} $, their **Hadamard (element-wise) product** is:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\circ \\mathbf{v} = \\begin{bmatrix} u_1 v_1 \\\\ u_2 v_2 \\\\ \\vdots \\\\ u_n v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Unlike the dot product, the Hadamard product **preserves the shape of the original vectors**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard Product u ‚àò v: [5 8 9 8 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors of the same size\n",
    "u = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Hadamard (element-wise) product: preserves shape\n",
    "hadamard_product = u * v\n",
    "\n",
    "print(\"Hadamard Product u ‚àò v:\", hadamard_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Matrix Operations\n",
    "\n",
    "Below are key matrix operations with definitions, motivations, and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Vector Multiplication\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $ and a vector $ \\mathbf{v} \\in \\mathbb{R}^{n} $,  \n",
    "their multiplication results in another vector $ \\mathbf{b} \\in \\mathbb{R}^{m} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = A \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\n",
    "\\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}\n",
    "\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element of $ \\mathbf{b} $ is computed as:\n",
    "\n",
    "$$\n",
    "b_i = \\sum_{j=1}^{n} a_{ij} v_j\n",
    "$$\n",
    "\n",
    "where each row of $ A $ is multiplied by the vector $ \\mathbf{v} $.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[2 1 3]\n",
      " [4 2 1]\n",
      " [1 5 2]]\n",
      "\n",
      "Vector v:\n",
      " [1 2 3]\n",
      "\n",
      "Result of A * v:\n",
      " [13 11 17]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a 3x3 matrix and a compatible vector\n",
    "A = np.array([[2, 1, 3],\n",
    "              [4, 2, 1],\n",
    "              [1, 5, 2]])\n",
    "\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "# Matrix-vector multiplication: result is a vector\n",
    "b = np.dot(A, v)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nVector v:\\n\", v)\n",
    "print(\"\\nResult of A * v:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Matrix Multiplication\n",
    "\n",
    "For two matrices $ A \\in \\mathbb{R}^{m \\times n} $ and $ B \\in \\mathbb{R}^{n \\times p} $,  \n",
    "their multiplication results in a new matrix $ C \\in \\mathbb{R}^{m \\times p} $:\n",
    "\n",
    "$$\n",
    "C = A B\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "where each row of $ A $ is multiplied by each column of $ B $.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[2 3]\n",
      " [1 4]\n",
      " [3 2]]\n",
      "\n",
      "Matrix B:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Result of A @ B:\n",
      " [[14 19 24]\n",
      " [17 22 27]\n",
      " [11 16 21]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define A (3x2) and B (2x3)\n",
    "A = np.array([[2, 3],\n",
    "              [1, 4],\n",
    "              [3, 2]])\n",
    "\n",
    "B = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Matrix-matrix multiplication using the @ operator\n",
    "C = A @ B  # Equivalently: np.dot(A, B)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nMatrix B:\\n\", B)\n",
    "print(\"\\nResult of A @ B:\\n\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard (Element-wise) Product\n",
    "\n",
    "For two matrices $ A, B \\in \\mathbb{R}^{m \\times n} $,  \n",
    "the Hadamard (element-wise) product is:\n",
    "\n",
    "$$\n",
    "(A \\circ B)_{ij} = A_{ij} B_{ij}\n",
    "$$\n",
    "\n",
    "Unlike matrix multiplication, this operation is performed **element-wise**.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Matrix B:\n",
      " [[9 8 7]\n",
      " [6 5 4]\n",
      " [3 2 1]]\n",
      "\n",
      "Hadamard Product A ‚àò B:\n",
      " [[ 9 16 21]\n",
      " [24 25 24]\n",
      " [21 16  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two 3x3 matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "B = np.array([[9, 8, 7],\n",
    "              [6, 5, 4],\n",
    "              [3, 2, 1]])\n",
    "\n",
    "# Hadamard product (element-wise multiplication)\n",
    "H = A * B\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nMatrix B:\\n\", B)\n",
    "print(\"\\nHadamard Product A ‚àò B:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transpose\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $,  \n",
    "its transpose $ A^T \\in \\mathbb{R}^{n \\times m} $ is defined as:\n",
    "\n",
    "$$\n",
    "(A^T)_{ij} = A_{ji}\n",
    "$$\n",
    "\n",
    "which means the rows and columns of $ A $ are swapped.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Transpose of A:\n",
      " [[1 3 5]\n",
      " [2 4 6]]\n",
      "\n",
      "Again, transpose of A:\n",
      " [[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a 3x2 matrix\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "\n",
    "# Two equivalent ways to compute the transpose\n",
    "A_T = A.T\n",
    "A_T_ = np.transpose(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nTranspose of A:\\n\", A_T)\n",
    "print(\"\\nAgain, transpose of A:\\n\", A_T_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Useful matrix identity \n",
    "\n",
    "An important and useful identity for machine learning is:\n",
    "\n",
    "$$\n",
    "(AB)^T = B^T A^T.\n",
    "$$\n",
    "\n",
    "This identity states that the **transpose of the product of two matrices** equals the **product of their transposes in reverse order**. It assumes that the matrices can be multiplied, meaning that **$A$ is an $m \\times n$ matrix** and **$B$ is an $n \\times p$ matrix**.\n",
    "\n",
    "Use **pen and paper** (or type if you prefer) to **prove** this identity. You will need to use the **definitions of matrix multiplication and transposition** and show that the **left-hand side equals the right-hand side** of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "$$\n",
    "(AB)_{ij} = \\sum_{k=1}^{n} A_{ik}B_{kj}\n",
    "$$\n",
    "\n",
    "Taking the transpose of $AB$:\n",
    "$$\n",
    "[(AB)^T]_{ji} = (AB)_{ij} = \\sum_{k=1}^{n} A_{ik}B_{kj}\n",
    "$$\n",
    "\n",
    "For the product $B^T A^T$:\n",
    "$$\n",
    "(B^T A^T)_{ji} = \\sum_{k=1}^{n} (B^T)_{jk}(A^T)_{ki} = \\sum_{k=1}^{n} B_{kj}A_{ik}\n",
    "$$\n",
    "\n",
    "Since matrix multiplication is commutative under addition:\n",
    "$$\n",
    "\\sum_{k=1}^{n} A_{ik}B_{kj} = \\sum_{k=1}^{n} B_{kj}A_{ik}\n",
    "$$\n",
    "\n",
    "Thus, proving $(AB)^T = B^T A^T$.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Demonstrate $(AB)^T = B^T A^T$ using numerical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivation: transpose identity (AB)^T = B^T A^T underpins shape reasoning and backprop.\n",
    "# Define matrices A and B\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# TODO: Compute AB\n",
    "# AB = (A @ B) \n",
    "\n",
    "# TODO: Compute the transpose of AB\n",
    "# AB_T = \n",
    "\n",
    "# TODO: Compute the transpose of A and B\n",
    "# A_T = \n",
    "# B_T = \n",
    "\n",
    "# TODO: Compute B^T A^T\n",
    "# B_T_A_T = \n",
    "\n",
    "# Verify (AB)^T = B^T A^T\n",
    "# The assert statement checks if a condition is True.\n",
    "# If True, execution continues. If False, it raises an AssertionError.\n",
    "# assert np.array_equal(AB_T, B_T_A_T), \"The identity does not hold.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "# Define matrices A and B\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Compute AB\n",
    "AB = np.dot(A, B)\n",
    "\n",
    "# Compute the transpose of AB\n",
    "AB_T = np.transpose(AB)\n",
    "\n",
    "# Compute the transpose of A and B\n",
    "A_T = np.transpose(A)\n",
    "B_T = np.transpose(B)\n",
    "\n",
    "# Compute B^T A^T\n",
    "B_T_A_T = np.dot(B_T, A_T)\n",
    "\n",
    "# Verify (AB)^T = B^T A^T\n",
    "#The assert statement in Python is used to check if a given condition is True. \n",
    "#If the condition is True, the program does nothing and moves to the next line of code. \n",
    "#However, if the condition is False, the program raises an AssertionError. \n",
    "#Optionally it can display a specified message.\n",
    "assert np.array_equal(AB_T, B_T_A_T), \"The identity does not hold.\"\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Compare Matrix Multiplication Methods\n",
    "\n",
    "Matrix multiplication is a fundamental operation in machine learning and numerical computing. There are different ways to compute it, and some are much faster than others.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Implement a function using the NumPy matrix multiplication operator.\n",
    "2. Implement your own function for matrix multiplication using for-loops.\n",
    "3. Measure and compare execution times for both methods.\n",
    "4. Visualize the results using `matplotlib`.\n",
    "\n",
    "Before we start, let's briefly introduce:\n",
    "- The `timeit` module for measuring execution time,\n",
    "- The `matplotlib` library for plotting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚è± Measuring Execution Time with `timeit`\n",
    "\n",
    "The `timeit` module helps measure how long a function takes to run.\n",
    "\n",
    "```python\n",
    "import timeit\n",
    "\n",
    "def example_function():\n",
    "    sum([i for i in range(1000)])  # List comprehension\n",
    "\n",
    "# Using timeit with a lambda function to pass arguments correctly\n",
    "time_taken = timeit.timeit(lambda: example_function(), number=100)\n",
    "print(\"Execution time:\", time_taken)\n",
    "```\n",
    "- **`number=100`** ‚Üí Runs the function **100 times** and reports total time.\n",
    "- **Dividing total time by 100** ‚Üí Gives the **average execution time per run**.\n",
    "\n",
    "Why Do We Use `lambda` in `timeit.timeit()`?\n",
    "- `timeit.timeit()` expects a **function reference**, not a direct function call.\n",
    "- If a function **has no parameters**, we can pass it directly:\n",
    "  ```python\n",
    "  timeit.timeit(example_function, number=100)  \n",
    "  ```\n",
    "- **But if a function requires arguments**, we **must use `lambda`** to delay execution:\n",
    "  ```python\n",
    "  timeit.timeit(lambda: example_function(5), number=100) \n",
    "  ```\n",
    "- `lambda` ensures that the function is **only executed when `timeit` calls it**, rather than immediately.\n",
    "\n",
    "\n",
    "Using `timeit` helps compare different implementations and find the most efficient approach.\n",
    "\n",
    "---\n",
    "\n",
    " üìä Plotting Results with `matplotlib`\n",
    "\n",
    "`matplotlib` is a library for creating visualizations and analyzing data trends.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(x, y, label=\"Example Line\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Sample Plot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "This will generate a **simple line plot** with labeled axes and a legend. `plt.show()` displays the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivation: compare vectorized NumPy vs pure Python loops to see why we prefer vectorization.\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def multiply_numpy(A, B):\n",
    "    \"\"\"Matrix multiplication using NumPy's optimised operator.\"\"\"\n",
    "    return A @ B\n",
    "\n",
    "\n",
    "def multiply_loops(A, B):\n",
    "    \"\"\"Matrix multiplication using nested for-loops (inefficient).\"\"\"\n",
    "    result = 0  # TODO: initialise result with correct shape\n",
    "    # TODO: implement the triple nested loop\n",
    "    return result\n",
    "\n",
    "\n",
    "# Benchmarking\n",
    "max_n = 100\n",
    "numpy_times = np.zeros(max_n)\n",
    "loop_times = np.zeros(max_n)\n",
    "\n",
    "for size in range(1, max_n + 1):\n",
    "    matrix1 = np.zeros((size, size))  # TODO: replace with random values\n",
    "    matrix2 = np.zeros((size, size))  # TODO: replace with random values\n",
    "\n",
    "    # Measure execution time for each method\n",
    "    numpy_times[size - 1] = timeit.timeit(\n",
    "        lambda: multiply_numpy(matrix1, matrix2), number=1\n",
    "    )\n",
    "\n",
    "# TODO: Plot execution times for both methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "  ```python\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matrix multiplication using NumPy (efficient)\n",
    "def multiply_numpy(A, B):\n",
    "    assert A.shape[1] == B.shape[0], \"Matrix dimensions do not match for multiplication!\"\n",
    "    return A @ B  # NumPy's optimized matrix multiplication\n",
    "\n",
    "# Matrix multiplication using nested loops (inefficient)\n",
    "def multiply_loops(A, B):\n",
    "    assert A.shape[1] == B.shape[0], \"Matrix dimensions do not match for multiplication!\" \n",
    "    rows_A, cols = A.shape  \n",
    "    cols, cols_B = B.shape  \n",
    "    result = np.zeros((rows_A, cols_B))\n",
    "\n",
    "    for i in range(rows_A):  \n",
    "        for j in range(cols_B):  \n",
    "            for k in range(cols):  \n",
    "                result[i, j] += A[i, k] * B[k, j]  \n",
    "\n",
    "    return result\n",
    "\n",
    "# Benchmarking\n",
    "max_n = 100  \n",
    "numpy_times = np.zeros(max_n)\n",
    "loop_times = np.zeros(max_n)\n",
    "\n",
    "for size in range(1, max_n + 1):\n",
    "    matrix1 = np.random.rand(size, size)\n",
    "    matrix2 = np.random.rand(size, size)\n",
    "\n",
    "    # Measure execution time using direct function calls (NumPy and loops)\n",
    "    numpy_times[size - 1] = timeit.timeit(lambda: multiply_numpy(matrix1, matrix2), number=1)\n",
    "    loop_times[size - 1] = timeit.timeit(lambda: multiply_loops(matrix1, matrix2), number=1)\n",
    "\n",
    "# Plot execution times\n",
    "plt.plot(range(1, max_n + 1), numpy_times, label=\"NumPy multiplication\")\n",
    "plt.plot(range(1, max_n + 1), loop_times, label=\"For-loop multiplication\")\n",
    "\n",
    "plt.xlabel(\"Matrix Size $n$\")\n",
    "plt.ylabel(\"Execution Time (seconds)\")\n",
    "plt.title(\"Performance: NumPy vs. For-Loops\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Important Matrix Operations in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Slicing\n",
    "Slicing allows you to extract specific rows, columns, or submatrices from a NumPy array.\n",
    "\n",
    "Consider the **3√ó3 matrix** \\( A \\):\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Extracting the First Row**\n",
    "\n",
    "$$\n",
    "\\text{Result: } \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Extracting the Second Column**\n",
    "\n",
    "$$\n",
    "\\text{Result: }\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "5 \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Extracting a 2√ó2 Submatrix**\n",
    "\n",
    "$$\n",
    "\\text{Result: }\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "First Row:\n",
      " [1 2 3]\n",
      "\n",
      "Second Column:\n",
      " [2 5 8]\n",
      "\n",
      "Submatrix:\n",
      " [[2 3]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x3 matrix\n",
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Extract the first row (all columns)\n",
    "row_1 = a[0, :]\n",
    "\n",
    "# Extract the second column (all rows)\n",
    "col_2 = a[:, 1]\n",
    "\n",
    "# Extract a 2x2 submatrix (first two rows, last two columns)\n",
    "submatrix = a[0:2, 1:3]\n",
    "\n",
    "print(\"Original Matrix:\\n\", a)\n",
    "print(\"\\nFirst Row:\\n\", row_1)\n",
    "print(\"\\nSecond Column:\\n\", col_2)\n",
    "print(\"\\nSubmatrix:\\n\", submatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** There is a difference between column vector representation in mathematics and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Broadcasting allows NumPy to perform element-wise operations on arrays of different shapes without explicit looping.\n",
    "Consider the **3√ó3 matrix** \\( A \\):\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Adding a Scalar to Every Element**\n",
    "\n",
    "$$\n",
    "A + 10 =\n",
    "\\begin{bmatrix}\n",
    "11 & 12 & 13 \\\\\n",
    "14 & 15 & 16 \\\\\n",
    "17 & 18 & 19\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Adding a Row Vector to Each Row**\n",
    "\n",
    "$$\n",
    "A +\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 4 & 6 \\\\\n",
    "5 & 7 & 9 \\\\\n",
    "8 & 10 & 12\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + 10:\n",
      " [[11 12 13]\n",
      " [14 15 16]\n",
      " [17 18 19]]\n",
      "\n",
      "Matrix + Row Vector:\n",
      " [[ 2  4  6]\n",
      " [ 5  7  9]\n",
      " [ 8 10 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Adding a scalar broadcasts to every element\n",
    "b_plus_scalar = b+10\n",
    "print(\"Matrix + 10:\\n\", b_plus_scalar)\n",
    "\n",
    "# Adding a row vector broadcasts to each row of the matrix\n",
    "row_vector = np.array([1, 2, 3])\n",
    "broadcasted_sum = b + row_vector\n",
    "print(\"\\nMatrix + Row Vector:\\n\", broadcasted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Are Slicing and Broadcasting Useful?\n",
    "- **Avoids explicit loops**, making operations more efficient.\n",
    "- **Optimized for performance**, allowing large datasets to be processed faster.\n",
    "- **Flexible operations** on arrays of different shapes without reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Matrix slicing and broadcasting \n",
    "\n",
    "1. Create a **5√ó5** NumPy matrix filled with numbers from **1 to 25**.\n",
    "2. Extract the **middle 3√ó3 submatrix**.\n",
    "3. Add **5 to all elements** in the submatrix using broadcasting.\n",
    "4. Print the **original matrix** and the **modified matrix** to observe the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivation: use NumPy broadcasting instead of manual loops for simpler, faster code.\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><strong>Show Solution</strong></summary>\n",
    "\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  # Create a 5x5 matrix with values from 1 to 25\n",
    "  matrix = np.arange(1, 26).reshape(5, 5)\n",
    "\n",
    "  # Extract middle 3x3 submatrix\n",
    "  submatrix = matrix[1:4, 1:4]\n",
    "\n",
    "  # Add 5 to all elements in submatrix\n",
    "  submatrix += 5\n",
    "\n",
    "  # Print results\n",
    "  print(\"Original Matrix:\\n\", matrix)\n",
    "  ```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
