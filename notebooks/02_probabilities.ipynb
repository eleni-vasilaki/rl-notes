{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d587e563",
   "metadata": {},
   "source": [
    "# Revisiting  Probabilities \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/02_probabilities.ipynb)\n",
    "\n",
    "Reinforcement learning as a technique is deeply rooted in probabilities. This is because rewards in such systems are inherently stochastic—meaning they are not guaranteed, may only appear intermittently, and can fluctuate unpredictably. Even actions themselves can be probabilistic. You might take a specific action expecting a particular outcome, yet due to the nature of the environment, that same action could lead to different results.\n",
    "\n",
    "Consider the case of a slot machine. The action remains the same—you pull the lever—but the outcome is uncertain. Sometimes you win, sometimes you don’t. Or think of a robot navigating a terrain. You issue a simple command: move forward. In an ideal setting, this would produce a predictable result. But what if the floor is slippery? The robot’s forward movement might not be as expected—it could travel a shorter distance, slip, or veer slightly off course.\n",
    "\n",
    "At the core of reinforcement learning, then, is the question of probability. Our objective is always to maximize the expected reward, navigating through uncertainty by making the best possible decisions given the information available. To deepen the understanding of reinforcement learning theory, we will first revisit fundamental probability concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff657918-5ce4-40c0-b1fa-8b0af6e78145",
   "metadata": {},
   "source": [
    "## Probabilities\n",
    "\n",
    "### Definition\n",
    "Probability quantifies the likelihood that an event will occur. In the context of probability theory, we often deal with *random variables*, which are variables that can assume different values, each with a certain probability.\n",
    "\n",
    "### Notation\n",
    "The probability of an event $A$ occurring is denoted by $P(A)$.\n",
    "\n",
    "### Properties\n",
    "- Probabilities range between 0 and 1: $0 \\leq P(A) \\leq 1$.\n",
    "- The probability of the sample space $S$ (representing all possible outcomes) is 1: $P(S) = 1$.\n",
    "- If two events $A$ and $B$ are mutually exclusive (i.e., they cannot occur at the same time), then the probability of either event occurring is $P(A \\cup B) = P(A) + P(B)$.\n",
    "\n",
    "#### Example\n",
    "Consider flipping a fair coin. The probability of getting heads ($A$) is 0.5, and the probability of getting tails ($B$) is also 0.5. Since getting heads and tails are mutually exclusive events, $P(A \\cup B) = P(A) + P(B) = 0.5 + 0.5 = 1$.\n",
    "\n",
    "## Probability Distribution\n",
    "A *probability distribution* specifies how probabilities are distributed over the values of a random variable. For a discrete random variable $X$, the probability distribution is given by a probability mass function (PMF) which assigns a probability to each possible value of $X$.\n",
    "\n",
    "#### Notation\n",
    "The probability that $X$ takes on a particular value $x$ is denoted by $P(X = x)$.\n",
    "\n",
    "### Example\n",
    "\n",
    "When rolling a four-sided die (1d4), the probability of rolling any specific number (1 through 4) is $\\frac{1}{4}$. The probability distribution of the die's outcomes is uniform because each outcome has the same probability. This can be formally described by the uniform distribution notation for a discrete case.\n",
    "\n",
    "For a discrete uniform distribution, which applies to the scenario of rolling a four-sided die, the probability mass function (PMF) is defined as:\n",
    "$$\n",
    "P(X=x) = \\frac{1}{n}\n",
    "$$\n",
    "for $x = x_1, x_2, ..., x_n$, where $n$ is the number of outcomes. In the case of the four-sided die, $n = 4$, and the outcomes $x$ can be $1, 2, 3,$ or $4$, each with a probability of $\\frac{1}{4}$.\n",
    "\n",
    "Therefore, for the four-sided die, the uniform distribution is represented as:\n",
    "$$\n",
    "P(X=x) = \\frac{1}{4}\n",
    "$$\n",
    "for $x = 1, 2, 3, 4$.\n",
    "\n",
    "## Expectation\n",
    "\n",
    "### Definition\n",
    "The *expectation* $E[X]$ of a random variable $X$ is the theoretical mean of its probability distribution, representing the average outcome if the random experiment were repeated many times.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a discrete random variable $X$, the expectation is calculated as:\n",
    "$$E[X] = \\sum x \\cdot P(X=x)$$\n",
    "\n",
    "#### Example\n",
    "For a four-sided die (1d4), the expectation of the outcome $X$ is $E[X] = \\frac{1}{4}(1 + 2 + 3 + 4) = 2.5$.\n",
    "\n",
    "## Variance\n",
    "\n",
    "### Definition\n",
    "*Variance* measures the spread or dispersion of the random variable's values around its mean (expectation). A low variance indicates that the values tend to be close to the mean, while a high variance shows that the values are spread out.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a discrete random variable $X$ with expectation $E[X]$, the variance $Var(X)$ is calculated as the expected value of the squared deviations from the mean:\n",
    "$$Var(X) = E\\left[(X - E[X])^2\\right]$$\n",
    "\n",
    "#### Example\n",
    "For the four-sided die (1d4), the variance of $X$ can be calculated as follows:\n",
    "$$Var(X) = \\frac{1}{4}((1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2) = 1.25$$\n",
    "\n",
    "# Exercise: Expectation and Variance of a Coin\n",
    "Consider a fair coin. Let $X$ be a random variable that takes the number 1 when the coin flips to Tail and 2 when it flips to Head.\n",
    "\n",
    "## Calculate the Expectation\n",
    "Determine the expectation $E[X]$ of the outcome $X$. Use the definition and mathematical formulation of expectation for a discrete random variable.\n",
    "\n",
    "\n",
    "## Calculate the Variance of $X$\n",
    "Once you have the expectation, calculate the variance $Var(X)$ of the outcome $X$. Use the definition and mathematical formulation of variance for a discrete random variable, considering the expectation you found in Question 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1d9fa-06fe-4874-a64a-fb43add16030",
   "metadata": {},
   "source": [
    "### Solution: \n",
    "\n",
    "The expectation $E[X]$ is calculated as:\n",
    "$$E[X] = \\frac{1}{2} \\times (1 + 2) = 1.5$$\n",
    "\n",
    "The variance $Var(X)$ is calculated as:\n",
    "$$Var(X) = \\frac{1}{2}((1 - 1.5)^2 + (2 - 1.5)^2) = 0.25$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d737b9-3434-4753-8233-07c855156f57",
   "metadata": {},
   "source": [
    "## Joint Probability and Joint Probability Distribution\n",
    "\n",
    "Joint probability and joint probability distribution are foundational concepts in probability theory that play a crucial role in understanding the simultaneous behavior of two or more discrete random variables.\n",
    "\n",
    "### Joint Probability\n",
    "\n",
    "Joint probability quantifies the likelihood that two discrete random variables, say $X$ and $Y$, will take on specific values simultaneously. It's a measure of the occurrence of two events together and is mathematically defined as:\n",
    "$$\n",
    "P(X = x, Y = y) = P(\\{X = x\\} \\cap \\{Y = y\\}),\n",
    "$$\n",
    "where $P(X = x, Y = y)$ denotes the probability that $X$ equals $x$ and $Y$ equals $y$ at the same time.\n",
    "\n",
    "### Joint Probability Distribution\n",
    "\n",
    "While joint probability refers to the probability of a particular combination of outcomes, the joint probability distribution provides a complete picture by describing the probabilities of all possible combinations of outcomes for $X$ and $Y$. \n",
    "\n",
    "The joint probability distribution can be represented in a table (for finite variables), a formula, or a graph, and it satisfies the following properties:\n",
    "- All probabilities are non-negative: $P(X = x, Y = y) \\geq 0$ for all $x$ and $y$.\n",
    "- The sum of all joint probabilities equals 1, reflecting the certainty that some combination of outcomes for $X$ and $Y$ will occur:\n",
    "$$\n",
    "\\sum_{x} \\sum_{y} P(X = x, Y = y) = 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d905025-5a7a-40d5-af37-a427265e785b",
   "metadata": {},
   "source": [
    "## Independent Variables\n",
    "\n",
    "### Definition\n",
    "Two random variables $X$ and $Y$ are considered *independent* if the occurrence of one does not affect the probability of occurrence of the other. In other words, knowing the outcome of $X$ provides no information about the outcome of $Y$, and vice versa.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For two independent variables $X$ and $Y$, the probability that $X$ takes on a particular value $x$ and $Y$ takes on a value $y$ simultaneously is the product of their individual probabilities:\n",
    "$$P(X = x \\text{ and } Y = y) = P(X = x) \\cdot P(Y = y)$$\n",
    "\n",
    "#### Example\n",
    "Consider rolling two different four-sided dice (1d4), one red and one blue. The outcome of rolling the red die is independent of the outcome of rolling the blue die. The probability of rolling a 2 on the red die is $\\frac{1}{4}$, and the probability of rolling a 3 on the blue die is also $\\frac{1}{4}$. Therefore, the probability of rolling a 2 on the red die and a 3 on the blue die simultaneously is $\\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{1}{16}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2e49d-d634-45b9-a79c-991daba24f9b",
   "metadata": {},
   "source": [
    "# Exercise: Probability of Hitting with Advantage in D&D\n",
    "\n",
    "In Dungeons & Dragons (D&D), \"advantage\" allows a player to roll two twenty-sided dice (d20) instead of one for their attack and choose the higher roll for the outcome. This mechanic improves the player's chance of success for actions like attacking or performing a skill check.\n",
    "\n",
    "When a player rolls with advantage, we analyze the joint probability of the two rolls, focusing on achieving at least one successful hit (hitting the target). Define $p_{\\text{hit}}$ as the probability of hitting the target with a single die roll. \n",
    "\n",
    "1. Plot the probability of hitting the opponent with advantage $P_{\\text{Hit}}(\\text{Advantage})$ vs. $p_{\\text{hit}}$ for a range of hit probabilities ($p_{\\text{hit}}$) from 0 to 1.\n",
    "2. On the same graph, plot the line $p_{\\text{hit}} = p_{\\text{hit}}$ to compare the probability of hitting with advantage versus without advantage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7df3cd-f6e0-4017-a83b-9c1086ca82e7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    " Given two independent rolls, there are four possible outcomes: the first roll hits, the second roll hits, both rolls hit, or neither hits. The probabilities of these outcomes sum to 1.\n",
    "\n",
    "The probability  $p_{\\text{hit}}$ is the probability of hitting the target with a single die roll.  Conversely, the probability of missing the target is $1 - p_{\\text{hit}}$.\n",
    "\n",
    "To calculate the probability of hitting with advantage, it's easier to compute the probability of missing both rolls, which is the product of their individual miss probabilities due to their independence: $(1 - p_{\\text{hit}})^2$.\n",
    "\n",
    "Therefore, the probability of hitting the target with at least one die when rolling with advantage is the complement of both dice missing:\n",
    "$$\n",
    "P_{\\text{Hit}}(\\text{Advantage}) = 1 - (1 - p_{\\text{hit}})^2.\n",
    "$$\n",
    "\n",
    "The code below creates the graph. \n",
    "\n",
    " ```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of p_hit values from 0 to 1\n",
    "p_hit = np.linspace(0, 1, 100)\n",
    "\n",
    "# Calculate the probability of hitting with advantage\n",
    "p_hit_advantage = 1 - (1 - p_hit)**2\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(p_hit, p_hit_advantage, label='P_Hit (Advantage)', color='blue')\n",
    "plt.plot(p_hit, p_hit, '--', label='p_hit (No Advantage)', color='red')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('p_hit')\n",
    "plt.ylabel('Probability of Hitting')\n",
    "plt.title('Probability of Hitting with Advantage vs. Without Advantage in D&D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efe09d-d5da-4d19-b4f8-c3ca558fe895",
   "metadata": {},
   "source": [
    "## Properties of Expectation\n",
    "\n",
    "### Linearity of Expectation\n",
    "Given random variables $X$ and $Y$, and constants $a$ and $b$, the expectation operator is linear. This means:\n",
    "$$E[aX + bY] = a \\cdot E[X] + b \\cdot E[Y]$$\n",
    "This property holds regardless of whether $X$ and $Y$ are dependent or independent.\n",
    "\n",
    "#### Proof\n",
    "The linearity of expectation does not depend on the independence of $X$ and $Y$. \n",
    "$$E[aX + bY] = \\sum_x \\sum_y(ax + by) \\cdot P(X=x, Y=y)$$\n",
    "This simplifies to:\n",
    "$$E[aX + bY] = a \\sum_x x \\sum_y \\cdot P(X=x, Y=y) + b  \\sum_y y \\sum_x \\cdot P(X=x, Y=y) $$ \n",
    "$$= a \\sum_x x \\cdot P(X=x) + b \\sum_y y \\cdot P(Y=y)$$\n",
    "$$=a \\cdot E[X] + b \\cdot E[Y]$$\n",
    "This is because $\\sum_y \\cdot P(X=x, Y=y)$ marginalise Y, we have taken the joint probability but considered all possible values of Y, which leaves us with just $P(X=x)$.\n",
    "\n",
    "### Expectation of Products\n",
    "For two **independent** random variables $X$ and $Y$, the expectation of their product is the product of their expectations:\n",
    "$$E[XY] = E[X] \\cdot E[Y]$$\n",
    "For dependent variables, this property may not hold.\n",
    "\n",
    "#### Proof for Independent Variables\n",
    "For independent variables $X$ and $Y$:\n",
    "$$E[XY] = \\sum_x \\sum_y xy \\cdot P(X=x) \\cdot P(Y=y)$$\n",
    "Given their independence, we can separate their probabilities:\n",
    "$$E[XY] = \\left(\\sum_x x \\cdot P(X=x)\\right) \\left(\\sum_y y \\cdot P(Y=y)\\right) = E[X] \\cdot E[Y]$$\n",
    "This establishes that for independent variables, the expectation of their product equals the product of their expectations.\n",
    "\n",
    "### Expectation of a Constant\n",
    "For any constant $c$, its expectation is equal to itself. This property is straightforward and does not involve the concept of variable independence.\n",
    "$$E[c] = c$$\n",
    "\n",
    "#### Proof\n",
    "The expectation of a constant is derived from its definition:\n",
    "$$E[c] = \\sum c \\cdot P(c) = c \\cdot 1 = c$$\n",
    "Since a constant does not vary, its expectation is simply the constant itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b528340-e226-47d0-a342-583bc8e64a65",
   "metadata": {},
   "source": [
    "### Alterative expression for the variance\n",
    "The variance can also be expressed as:\n",
    "\n",
    "$$\\text{Var}(X) = E[X^2] - (E[X])^2$$\n",
    "\n",
    "**Proof:**  \n",
    "To prove this formula, expand the square and apply linearity of expectation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}(X) & = E[(X - E[X])^2] \\\\\n",
    "& = E[X^2 - 2XE[X] + (E[X])^2] \\\\\n",
    "& = E[X^2] - 2E[X]E[X] + (E[X])^2 \\\\\n",
    "& = E[X^2] - 2(E[X])^2 + (E[X])^2 \\\\\n",
    "& = E[X^2] - (E[X])^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a997d-54ab-485f-9a46-0844025168a8",
   "metadata": {},
   "source": [
    "### Variance of the Sum of Two Independent Random Variables\n",
    "\n",
    "Let \\(X\\) and \\(Y\\) be two independent random variables. The variance of their sum, \\(Z = X + Y\\), is the sum of their variances. That is,\n",
    "$$\n",
    "\\text{Var}(Z) = \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "Recall the definition of variance for a random variable \\(X\\):\n",
    "$$\n",
    "\\text{Var}(X) = E[(X - E[X])^2]\n",
    "$$\n",
    "\n",
    "For two independent random variables \\(X\\) and \\(Y\\), we want to show that:\n",
    "$$\n",
    "\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "Starting with the left side of the equation:\n",
    "$$\n",
    "\\text{Var}(X + Y) = E[((X + Y) - E[X + Y])^2]\n",
    "$$\n",
    "Since \\(E[X + Y] = E[X] + E[Y]\\) by the linearity of expectation, we have:\n",
    "$$\n",
    "= E[((X + Y) - (E[X] + E[Y]))^2]\n",
    "$$\n",
    "Expanding the square gives:\n",
    "$$\n",
    "= E[(X - E[X] + Y - E[Y])^2]\n",
    "$$\n",
    "Expanding the expression further, we get:\n",
    "$$\n",
    "= E[(X - E[X])^2 + 2(X - E[X])(Y - E[Y]) + (Y - E[Y])^2]\n",
    "$$\n",
    "Since \\(X\\) and \\(Y\\) are independent, \\(E[(X - E[X])(Y - E[Y])] = E[X - E[X]]E[Y - E[Y]] = 0\\), so the middle term vanishes, and we are left with:\n",
    "$$\n",
    "= E[(X - E[X])^2] + E[(Y - E[Y])^2]\n",
    "$$\n",
    "This simplifies to:\n",
    "$$\n",
    "= \\text{Var}(X) + \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "Thus, we have shown that the variance of the sum of two independent random variables is equal to the sum of their individual variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cb967-6927-46cd-9a6b-b886263368de",
   "metadata": {},
   "source": [
    "### Example: Calculating Mean and Variance for Two Independent Dice Rolls\n",
    "\n",
    "Let's consider two independent six-sided dice rolls. Let \\(X\\) be the outcome of the first roll, and \\(Y\\) the outcome of the second roll. Each die has outcomes {1, 2, 3, 4, 5, 6}, with each outcome equally likely.\n",
    "\n",
    "#### Mean (Expectation) Calculation\n",
    "\n",
    "The mean (or expectation) of a single six-sided die roll is:\n",
    "$$\n",
    "E[X] = E[Y] = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5\n",
    "$$\n",
    "\n",
    "or can be faster calculated as the middle point:\n",
    "\n",
    "$$\n",
    "E[X] = E[Y] = \\frac{1 + 6}{2} = 3.5\n",
    "$$\n",
    "\n",
    "Since the rolls are independent, the expectation for the sum \\(Z = X + Y\\) is simply the sum of the expectations:\n",
    "$$\n",
    "E[Z] = E[X] + E[Y] = 3.5 + 3.5 = 7\n",
    "$$\n",
    "\n",
    "#### Variance Calculation\n",
    "\n",
    "The variance of a single six-sided die roll, using the variance formula $\\text{Var}(X) = E[X^2] - (E[X])^2$, can be calculated as follows. First, calculate \\(E[X^2]\\):\n",
    "$$\n",
    "E[X^2] = \\frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6} = \\frac{91}{6}\n",
    "$$\n",
    "Then, the variance of \\(X\\) (and similarly for \\(Y\\) since they are identically distributed) is:\n",
    "$$\n",
    "\\text{Var}(X) = \\text{Var}(Y) = \\frac{91}{6} - (3.5)^2 = \\frac{35}{12}\n",
    "$$\n",
    "For two independent dice rolls, the variance of their sum \\(Z = X + Y\\) is the sum of their variances:\n",
    "$$\n",
    "\\text{Var}(Z) = \\text{Var}(X) + \\text{Var}(Y) = \\frac{35}{12} + \\frac{35}{12} = \\frac{35}{6}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de74b4-8d98-43b2-bb74-4a3a2ed8bd05",
   "metadata": {},
   "source": [
    "# Exercise: The Importance of the Variance\n",
    "\n",
    "In Dungeons & Dragons (D&D), sorcerers and wizards have access to two cantrips: **Magic Missile** and **Burning Hands**. \n",
    "\n",
    "Your task is to calculate the mean damage and variance for each cantrip and then simulate dice rolls for each case 100 times to plot their distributions. \n",
    "\n",
    "In theory, your calculations should match your simulation results. As you increase the number of repetitions (dice rolls), observe whether the simulation results more closely align with your theoretical calculations.\n",
    "\n",
    "1. **Magic Missile** involves rolling three times a $d4$ and adding 1 to each roll for the damage, then summing the results for the total damage.\n",
    "2. **Burning Hands** involves rolling three times a $d6$, summing the results for the total damage.\n",
    "\n",
    "**Definitions:**\n",
    "- $d4$: A four-sided die, with outcomes ranging from 1 to 4.\n",
    "- $d6$: A six-sided die, with outcomes ranging from 1 to 6.\n",
    "\n",
    "You will need the following functions: \n",
    "\n",
    "`np.random.randint(low, high, size)`\n",
    "\n",
    "`np.mean(array)`\n",
    "\n",
    "`np.var(array, ddof=0)`\n",
    "\n",
    "Use the NumPy documentation to find more information about them.\n",
    "\n",
    "Assume each roll is a random variable and solve by applying the properties we have discussed above. For variance calculation, `ddof=0` is used to denote the population variance. \n",
    "\n",
    "Plot the distribution of the results using an appropriate type of plot (e.g., histogram) to visualize the distribution of damage outputs for each cantrip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8088a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aaaed8-da45-4706-b01b-1c3c95063ad1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "##### Magic Missile\n",
    "- **Mean Damage Calculation**:\n",
    "    The mean (expected value) of a single $d4$ roll is:\n",
    "  \n",
    "   $$\n",
    "    E[d4] = \\frac{1+2+3+4}{4} = 2.5\n",
    "   $$\n",
    "    For Magic Missile, since we add 1 to each roll and roll it three times, the mean damage is:\n",
    "    $$\n",
    "    E[\\text{Magic Missile}] = 3 \\times (E[d4] + 1) = 3 \\times (2.5 + 1) = 10.5\n",
    "    $$\n",
    "\n",
    "- **Variance Calculation**:\n",
    "    The variance of a single $d4$ roll, where variance $Var(X) = E[X^2] - (E[X])^2$, is calculated as follows:\n",
    "    First, calculate $E[X^2]$ for a $d4$:\n",
    "    $$\n",
    "    E[X^2] = \\frac{1^2 + 2^2 + 3^2 + 4^2}{4} = \\frac{30}{4} = 7.5\n",
    "    $$\n",
    "    Then, the variance of $X$ is:\n",
    "    $$\n",
    "    Var(d4) = E[X^2] - (E[X])^2 = 7.5 - (2.5)^2 = 1.25\n",
    "    $$\n",
    "    Since we roll three times and each roll is independent, the total variance for Magic Missile is:\n",
    "    $$\n",
    "    Var(\\text{Magic Missile}) = 3 \\times Var(d4) = 3 \\times 1.25 = 3.75\n",
    "    $$\n",
    "\n",
    "##### Burning Hands\n",
    "- **Mean Damage Calculation**:\n",
    "    The mean (expected value) of a single $d6$ roll is:\n",
    "    $$\n",
    "    E[d6] = \\frac{1+2+3+4+5+6}{6} = 3.5\n",
    "    $$\n",
    "    For Burning Hands, since we roll 3 $d6$, the mean damage is:\n",
    "    $$\n",
    "    E[\\text{Burning Hands}] = 3 \\times E[d6] = 3 \\times 3.5 = 10.5\n",
    "    $$\n",
    "\n",
    "- **Variance Calculation**:\n",
    "    First, calculate $E[X^2]$ for a $d6$:\n",
    "    $$\n",
    "    E[X^2] = \\frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6} = \\frac{91}{6}\n",
    "    $$\n",
    "    Then, the variance of $X$ for a $d6$ is:\n",
    "    $$\n",
    "    Var(d6) = E[X^2] - (E[X])^2 = \\frac{91}{6} - (3.5)^2 = 2.9167\n",
    "    $$\n",
    "    Since we roll 3 $d6$ for Burning Hands and each roll is independent, the total variance is:\n",
    "    $$\n",
    "    Var(\\text{Burning Hands}) = 3 \\times Var(d6) = 3 \\times 2.9167 = 8.75\n",
    "    $$\n",
    "\n",
    "\n",
    "Variance measures the dispersion of damage values around the mean. A smaller variance indicates that the actual damage values are more tightly clustered around the average, leading to more predictable outcomes. We can explain this more simply by saying that a smaller variance suggests more reliable delivery of the average damage.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rolls = 20000\n",
    "\n",
    "# Simulate dice rolls for Magic Missile (3 rolls of d4 + 1 each)\n",
    "magic_missile_simulations = np.random.randint(1, 5, (rolls, 3)) + 1\n",
    "magic_missile_damages = np.sum(magic_missile_simulations, axis=1)\n",
    "\n",
    "# Simulate dice rolls for Burning Hands (3 rolls of d6)\n",
    "burning_hands_simulations = np.random.randint(1, 7, (rolls, 3))\n",
    "burning_hands_damages = np.sum(burning_hands_simulations, axis=1)\n",
    "\n",
    "# Calculate mean and variance for each cantrip\n",
    "magic_missile_mean = np.mean(magic_missile_damages)\n",
    "magic_missile_variance = np.var(magic_missile_damages)\n",
    "burning_hands_mean = np.mean(burning_hands_damages)\n",
    "burning_hands_variance = np.var(burning_hands_damages)\n",
    "\n",
    "# Plotting the distributions on the same plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot both histograms\n",
    "plt.hist(magic_missile_damages, bins=range(int(min(magic_missile_damages)), int(max(magic_missile_damages)) + 2), color='blue', alpha=0.7, label='Magic Missile')\n",
    "plt.hist(burning_hands_damages, bins=range(int(min(burning_hands_damages)), int(max(burning_hands_damages)) + 2), color='red', alpha=0.7, label='Burning Hands')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Damage Distribution Comparison')\n",
    "plt.xlabel('Damage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Magic Missile - Mean Damage: {magic_missile_mean}, Variance: {magic_missile_variance}\")\n",
    "print(f\"Burning Hands - Mean Damage: {burning_hands_mean}, Variance: {burning_hands_variance}\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642820e-3202-4ded-acb6-9ea14d995c4f",
   "metadata": {},
   "source": [
    "## Conditional Probability \n",
    "\n",
    "### Definition\n",
    "\n",
    "Conditional probability of an event $A$ given the event $B$ has occurred is defined as:\n",
    "    $$\n",
    "    P(A|B) = \\frac{P(A \\cap B)}{P(B)},\n",
    "    $$\n",
    "    provided that $P(B) > 0$.\n",
    "\n",
    "- $P(A \\cap B)$ is the joint probability of both events $A$ and $B$ occurring together. It measures the likelihood that both events happen simultaneously.\n",
    "\n",
    "- $P(A)$ is the probability of event $A$ occurring.\n",
    "\n",
    "### Example: Calculate the joint probability of drawing two aces from the deck (without replacement).\n",
    "\n",
    "Consider a standard deck of 52 playing cards. We're interested in finding the joint probability of drawing two  aces from the deck - without replacement, meaning that once we draw one card, we won't place it back in the deck.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "- Let $A$ represent the event of drawing an ace on the first draw.\n",
    "- Let $B$ represent the event of drawing an ace on the second draw.\n",
    "\n",
    "\n",
    "Since there are 4 aces in a deck of 52 cards, the probability of drawing an ace on the first draw ($P(A)$) is $\\frac{4}{52}$. After drawing an ace on the first draw, there are now 3 aces left in a deck of 51 cards.\n",
    "\n",
    "\n",
    "We aim to find $P(B|A)$, the probability of event $B$ occurring given that event $A$ has already occurred. The conditional probability of drawing an ace on the second draw given that the first draw was an ace is the ratio of the number of favorable outcomes to the total number of outcomes in this reduced sample space:\n",
    "$$\n",
    "P(B|A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of outcomes}} = \\frac{3}{51}\n",
    "$$\n",
    "\n",
    "The joint probability $P(A \\cap B)$ can be found as:\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\times P(B|A) = \\frac{4}{52} \\times \\frac{3}{51}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a90bb-2e4f-4a6d-8e56-3791c031b8ae",
   "metadata": {},
   "source": [
    "### Law of Total Probability\n",
    "\n",
    "If $B_1, B_2, \\ldots, B_n$ are mutually exclusive and exhaustive events that cover the whole sample space, then the probability of an event $A$ can be expressed as:\n",
    "\n",
    "$$\n",
    "P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "\n",
    "Given a sample space $S$ divided into $n$ mutually exclusive and exhaustive events $B_1, B_2, \\ldots, B_n$, meaning:\n",
    "\n",
    "1. **Mutually Exclusive**: For any $i \\neq j$, $B_i \\cap B_j = \\emptyset$, implying no two events can occur at the same time.\n",
    "2. **Exhaustive**: $B_1 \\cup B_2 \\cup \\cdots \\cup B_n = S$, ensuring at least one of the $B_i$ events must occur for any outcome in the sample space.\n",
    "\n",
    "For an event $A$, it can be expressed as the union of intersections with each $B_i$:\n",
    "\n",
    "$$\n",
    "A = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_n)\n",
    "$$\n",
    "\n",
    "Since $B_i$ are mutually exclusive, the intersections $A \\cap B_i$ are also mutually exclusive. Thus, their probabilities can be summed to get the probability of $A$:\n",
    "\n",
    "$$\n",
    "P(A) = P(A \\cap B_1) + P(A \\cap B_2) + \\cdots + P(A \\cap B_n)\n",
    "$$\n",
    "\n",
    "Using the definition of conditional probability ($P(A \\cap B) = P(A|B)P(B)$), this can be rewritten as:\n",
    "\n",
    "$$\n",
    "P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\n",
    "$$\n",
    "\n",
    "This formula shows how to calculate the probability of event $A$ by considering all the different, mutually exclusive, and exhaustive ways $A$ can occur.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's calculate the probability of drawing an Ace from a standard 52-card deck using the Law of Total Probability, partitioning our sample space into four mutually exclusive and exhaustive events based on the suit of the card:\n",
    "\n",
    "- $B_1$: Drawing a Spade\n",
    "- $B_2$: Drawing a Heart\n",
    "- $B_3$: Drawing a Diamond\n",
    "- $B_4$: Drawing a Club\n",
    "\n",
    "Our event $A$ is \"drawing an Ace.\"\n",
    "\n",
    "Lets start by the information that each suit (Spade, Heart, Diamond, Club) contains 13 cards among which one ace: \n",
    "\n",
    "- $P(A|B_1) = P(A|B_2) = P(A|B_3) = P(A|B_4) = \\frac{1}{13}$.\n",
    "\n",
    "The probability of selecting any suit is the same since the deck is evenly divided among the four suits:\n",
    "\n",
    "- $P(B_1) = P(B_2) = P(B_3) = P(B_4) = \\frac{1}{4}$\n",
    "\n",
    "Applying the Law of Total Probability to find the probability of drawing an Ace, $P(A)$:\n",
    "\n",
    "$$\n",
    "P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + P(A|B_3)P(B_3) + P(A|B_4)P(B_4)\n",
    "$$\n",
    "\n",
    "Substituting the known values:\n",
    "\n",
    "$$\n",
    "P(A) = \\left(\\frac{1}{13}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{13}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{13}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{13}\\right)\\left(\\frac{1}{4}\\right) = 4 \\times \\left(\\frac{1}{13}\\right)\\left(\\frac{1}{4}\\right) = \\frac{1}{13}\n",
    "$$\n",
    "\n",
    "Therefore, the probability of drawing an Ace from a standard 52-card deck is $\\frac{1}{13}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19951e05-d9f1-46b5-80d0-0f18561be59e",
   "metadata": {},
   "source": [
    "### Chain Rule for Probabilities\n",
    "\n",
    "The chain rule for probabilities states that the joint probability of $n$ events $A_1, A_2, \\ldots, A_n$ can be expressed as:\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\cdot P(A_2|A_1) \\cdot P(A_3|A_1 \\cap A_2) \\cdot \\ldots \\cdot P(A_n|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1})\n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "\n",
    "**1. Base Case:**\n",
    "\n",
    "For $n = 2$, the joint probability of two events $A_1$ and $A_2$ occurring is given by the basic rule of conditional probability:\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2) = P(A_1) \\cdot P(A_2|A_1)\n",
    "$$\n",
    "\n",
    "This equation directly follows from the definition of conditional probability, which is $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$, rearranged to express the joint probability of $A$ and $B$.\n",
    "\n",
    "**2. Inductive Step:**\n",
    "\n",
    "Assume the rule holds for $n-1$ events. That is, we assume:\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1}) = P(A_1) \\cdot P(A_2|A_1) \\cdot \\ldots \\cdot P(A_{n-1}|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-2})\n",
    "$$\n",
    "\n",
    "is true. Now, we need to prove that this implies the rule also holds for $n$ events.\n",
    "\n",
    "Consider the joint probability of $n$ events. By the definition of conditional probability, we have:\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1}) \\cdot P(A_n|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1})\n",
    "$$\n",
    "\n",
    "Using our assumption for $n-1$ events, we substitute the expression for $P(A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1})$:\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = \\left( P(A_1) \\cdot P(A_2|A_1) \\cdot \\ldots \\cdot P(A_{n-1}|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-2}) \\right) \\cdot P(A_n|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1})\n",
    "$$\n",
    "\n",
    "This confirms that the rule indeed holds for $n$ events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae8ab8-0458-43fa-8b6d-d73477c82718",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem allows us to update the probability of an event based on new evidence. It is formulated as:\n",
    "    $$\n",
    "    P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}.\n",
    "    $$\n",
    "\n",
    "#### Proof\n",
    "By the definition of conditional probability, and noting that $P(A \\cap B) = P(B|A) \\cdot P(A)$, we can rewrite $P(A|B)$ as shown above, which serves as the foundation for updating probabilities upon receiving new information.\n",
    "\n",
    "#### Bayes' Theorem for Multiple Variables\n",
    "\n",
    "For multiple variables or events, Bayes' Theorem can be extended to account for a set of conditions or evidence. If $A$ is an event and $B_1, B_2, ..., B_n$ are conditions or evidence, Bayes' Theorem can be expressed as:\n",
    "    $$\n",
    "    P(A|B_1, B_2, ..., B_n) = \\frac{P(B_1, B_2, ..., B_n|A) \\cdot P(A)}{P(B_1, B_2, ..., B_n)},\n",
    "    $$\n",
    "    assuming $P(B_1, B_2, ..., B_n) > 0$.\n",
    "\n",
    "### Example \n",
    "\n",
    "Imagine a deck of cards consisting of 20 cards: 10 red cards and 10 black cards. Among the red cards, there are 4 aces, and among the black cards, there is 1 ace.\n",
    "\n",
    "You select a card at random and find that it's an ace. What is the probability that this ace is red?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Let $A$ be the event that a selected card is red.\n",
    "Let $B$ be the event that the selected card is an ace.\n",
    "\n",
    "We aim to find $P(A|B)$, the probability that the card is red given it's an ace.\n",
    "\n",
    "$P(A)$, the probability that a card is red, is $\\frac{10}{20}$ or $\\frac{1}{2}$, since 10 out of 20 cards are red.\n",
    "\n",
    "$P(B|A)$, the probability of drawing an ace given the card is red, is $\\frac{4}{10}$, because there are 4 aces among the 10 red cards.\n",
    "$P(B)$, the probability of drawing an ace from the entire deck, is $\\frac{5}{20}$ or $\\frac{1}{4}$, as there are 5 aces in total (4 red and 1 black) out of 20 cards.\n",
    "\n",
    "Bayes' Theorem states:\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Substituting the given values:\n",
    "$$\n",
    "P(A|B) = \\frac{\\frac{4}{10} \\times \\frac{1}{2}}{\\frac{1}{4}} = \\frac{4}{5}\n",
    "$$\n",
    "\n",
    "# Exercise\n",
    "\n",
    "Imagine you have a box containing two types of dice: a six-sided die (1d6) and a twelve-sided die (1d12). One fourth of the dice in the box are six-sided, and three fourths are twelve-sided. You randomly select a die from the box and roll it, obtaining a result of four.\n",
    "\n",
    "What is the probability that the die you rolled was a six-sided die?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7108b2-f2a3-47d1-9be2-a3145c481e97",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Solution</summary>\n",
    "  \n",
    "$P(A) = \\frac{1}{4} $, the probability of selecting a six-sided die.\n",
    "$P(A') = \\frac{3}{4}$, the probability of selecting a twelve-sided die.\n",
    "$P(B|A) = \\frac{1}{6}$, the probability of rolling a four on a six-sided die.\n",
    "$P(B|A') = \\frac{1}{12}$, the probability of rolling a four on a twelve-sided die.\n",
    "\n",
    "The total probability of rolling a four, considering both types of dice, is:\n",
    "$$\n",
    "P(B) = P(B|A)P(A) + P(B|A')P(A') = \\frac{1}{6} \\times \\frac{1}{4} + \\frac{1}{12} \\times \\frac{3}{4} = \\frac{1}{24} + \\frac{1}{16} =  \\frac{1}{3 \\cdot 8} + \\frac{1}{2 \\cdot 8} = \\frac{2}{6 \\cdot 8} + \\frac{3}{6 \\cdot 8} = \\frac{5}{6 \\cdot 8}\n",
    "$$\n",
    "\n",
    "Using Bayes' Theorem to find the probability that the die was six-sided, given that a four was rolled:\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{\\frac{1}{6} \\times \\frac{1}{4}}{\\frac{5}{6 \\cdot 8}} = \\frac{2}{5}. \n",
    "$$\n",
    "\n",
    "Thus, the probability that the die you rolled was a six-sided die, given that you rolled a four, is 2/5 or 40%.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3944a-f31d-4ae0-bd01-681afcc61cdb",
   "metadata": {},
   "source": [
    "## Conditional Expectations\n",
    "\n",
    "The conditional expectation quantifies the expected value of a random variable given that a certain condition is satisfied, related to another random variable. It provides a measure of the average outcome of the random variable under the specified condition.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a random variable $X$ and a condition defined by the random variable $Y$, the conditional expectation of $X$ given $Y=y$ is denoted and defined as:\n",
    "$$\n",
    "E[X|Y=y] = \\sum_{x} x \\cdot P(X=x|Y=y)\n",
    "$$\n",
    "for discrete variables. We will not discuss continuous variables in this tutorial.\n",
    "\n",
    "This expectation $E[X|Y=y]$ represents the average or expected value of $X$ assuming that $Y$ takes a specific value $y$. The notation emphasizes that the expectation is conditional on the particular outcome $y$ of the variable $Y$.\n",
    "\n",
    "\n",
    "### Law of Total Expectation\n",
    "\n",
    "#### Property\n",
    "The Law of Total Expectation states that the overall expected value of a random variable $X$ is equal to the expected value of the conditional expected values of $X$ given another variable $Y$, formally expressed as:\n",
    "$$\n",
    "E[X] = E[E[X|Y]].\n",
    "$$\n",
    "\n",
    "**Clarification:** This formula implies that we first calculate the expected value of the conditioned variable X for each specific value of Y. Then, we take the average of these conditional expectations across Y, considering the probability distribution of Y. $E[X] = E_Y[E_X[X|Y]]$.\n",
    "\n",
    "#### Proof\n",
    "Consider a random variable $X$ and a variable $Y$ taking values $y$. The expectation of $X$, considering all possible outcomes, is calculated by:\n",
    "$$\n",
    "E[X] = \\sum_{x} x \\cdot P(X=x) = \\sum_{x} x \\cdot \\sum_{y} P(X=x|Y=y)P(Y=y).\n",
    "$$\n",
    "This can be rearranged to:\n",
    "$$\n",
    "E[X] = \\sum_{y} \\left( \\sum_{x} x \\cdot P(X=x|Y=y) \\right) P(Y=y) = \\sum_{y} E[X|Y=y] \\cdot P(Y=y),\n",
    "$$\n",
    "where $\\sum_{y} E[X|Y=y] \\cdot P(Y=y)$ represents the sum of the expected values of $X$ conditional on each possible value $y$ of $Y$, weighted by the probability of $Y$ taking on the value $y$. This is effectively the expectation of $X$ across all values of $Y$, leading to $E[X] = E[E[X|Y]]$ for discrete scenarios.\n",
    "\n",
    "### Linearity of Conditional Expectation\n",
    "\n",
    "#### Property\n",
    "The conditional expectation is linear. For any two random variables $X$ and $Y$, and constants $a$ and $b$, given another random variable $Z$, the linearity of conditional expectation can be expressed as:\n",
    "$$\n",
    "E[aX + bY | Z] = aE[X | Z] + bE[Y | Z].\n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "Consider the expectation of the sum of two random variables $X$ and $Y$, scaled by constants $a$ and $b$, conditioned on $Z$. By definition of conditional expectation, we start with:\n",
    "  $$\n",
    "  E[aX + bY | Z] = \\sum_{x} \\sum_{y} (ax + by) P(X=x, Y=y | Z) =\n",
    "  $$\n",
    "  $$\n",
    "  = a\\sum_{x} x  \\sum_{y} P(X=x, Y=y | Z) + b\\sum_{y} y \\sum_{x} P(X=x, Y=y | Z).\n",
    "  $$\n",
    "  By summing across all possible values of $y$ for the first term and all possible values of $x$ for the second term, we effectively marginalize over $y$ and $x$, respectively, resulting in:\n",
    "  $$\n",
    "  E[aX + bY | Z] = a\\sum_{x} x P(X=x | Z) + b\\sum_{y} y P(Y=y | Z),\n",
    "  $$\n",
    "  where $\\sum_{x} x P(X=x | Z)$ and $\\sum_{y} y P(Y=y | Z)$ represent $E[X | Z]$ and $E[Y | Z]$, respectively.\n",
    "\n",
    "  Thus, it simplifies to:\n",
    "  $$\n",
    "E[aX + bY | Z] = aE[X | Z] + bE[Y | Z].\n",
    "  $$\n",
    "This revised proof correctly follows the principles of probability and showcases the logical steps leading to the conclusion that conditional expectation is indeed linear.\n",
    "\n",
    "\n",
    "### Independence and Conditional Expectation\n",
    "\n",
    "#### Property\n",
    "If $X$ and $Y$ are independent, then the conditional expectation of $X$ given $Y$ is simply the unconditional expectation of $X$:\n",
    "    \n",
    "$$ E[X|Y] = E[X]. $$\n",
    "    \n",
    "#### Proof\n",
    "Independence between $X$ and $Y$ implies $P(X|Y) = P(X)$. Therefore, the expectation of $X$ given $Y$, which sums over all possible values of $X$ weighted by their conditional probabilities given $Y$, simplifies to the unconditional expectation of $X$:\n",
    "    \n",
    " $$ E[X|Y] = \\sum x P(X=x|Y) = \\sum x P(X=x) = E[X].$$\n",
    "\n",
    "### The properties of conditional expectations are needed for the Bellman equations that underpin the theory of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb436e-589a-474d-b894-1a399806f133",
   "metadata": {},
   "source": [
    "# Exercise: The Monty Hall Problem Part 1\n",
    "You are a contestant on a game show presented with three doors. Behind one door is a car, and behind the other two doors, there are goats. You choose one door, hoping to find the car. Without opening the door you chose, the host, who knows what's behind all the doors, opens one of the other two doors, revealing a goat. You are then given a choice: stick with your original selection or switch to the other unopened door. Should you switch or stay to maximize your chances of winning the car?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5c3de-66b7-4867-b29d-aead099d970f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "\n",
    "The probability of the car being behind the door you initially chose is $1/3$. The combined probability of the car being behind the two doors you didn't choose is $2/3$.\n",
    "\n",
    "When the host reveals a goat behind one of these doors, it doesn't change the $1/3$ probability associated with your initial choice—this probability is fixed at the time of selection; the future cannot alter the past. However, the host’s deliberate action (showing a goat) shifts the conditional probability such that the unopened door you didn’t initially choose now carries the entire $2/3$ probability of concealing the car.\n",
    "\n",
    "If there are 3 doors, the probability of choosing the door that contains the car is $1/3$. Let's assume the player chooses door 1; the probability of containing the car is $p_1 = 1/3$ (regardless of which door is chosen). The probability of the car being behind one of the other two doors is $p_{others} = 2/3 = p_2 + p_3$ (whether behind door 2 or door 3). Since the host opens one of these doors—say, door 2—revealing a goat, we set $p_2 = 0$, hence $p_3 = 2/3$.\n",
    "\n",
    "If this is still confusing, consider a scenario with a trillion doors: You choose one door (with nearly 0 probability of hiding the car) while the remaining trillion minus one doors hold almost all the probability. The host deliberately opens a trillion minus 2 doors, each showing a goat, leaving only one other door. Thus, either you originally chose the car (with an extremely small probability) or, most likely, the car is behind the remaining door. \n",
    "\n",
    "The Monty Hall problem shows that common intuition may fail, as most people would assume equal probability for staying or switching—when in fact, switching is always better.\n",
    "\n",
    "\n",
    "### Conditional Probability Setup:\n",
    "\n",
    "Let $C_i$ be the event that the car is behind door $i$, for $i = 1, 2, 3$. Assume that the player initially chooses door 1, so that\n",
    "\n",
    "$$\n",
    "P(C_1) = \\frac{1}{3}, \\quad P(C_2) = \\frac{1}{3}, \\quad P(C_3) = \\frac{1}{3}.\n",
    "$$\n",
    "\n",
    "Let $H_2$ be the event that the host opens door 2 to reveal a goat.\n",
    "\n",
    "Then:\n",
    "\n",
    "The probability that the host opens door $H_2$ given that the car is behind door 1 is 0.5. If the player has chosen the car, the host can open any door. \n",
    "$$\n",
    "P(H_2 \\mid C_1) = \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "The probability that the host opens door $H_2$ given that the car is behind door 2 is 0. The host always shows a goat, they will not reveal the car. Hence,\n",
    "$$\n",
    "P(H_2 \\mid C_2) = 0.\n",
    "$$\n",
    "\n",
    "The probability that the host opens door $H_2$ given that the car is behind door 3 is 1. They will not open the door showing the car, leading to:\n",
    "\n",
    "$$\n",
    "P(H_2 \\mid C_3) = 1.\n",
    "$$\n",
    "\n",
    "By Bayes’ theorem, the conditional probability that the car is behind door 3 given $H_2$ is\n",
    "\n",
    "$$\n",
    "P(C_3 \\mid H_2) = \\frac{P(H_2 \\mid C_3) ~ P(C_3)}{P(H_2 \\mid C_1) ~ P(C_1) + P(H_2 \\mid C_2) ~ P(C_2) + P(H_2 \\mid C_3) ~ P(C_3)}.\n",
    "$$\n",
    "\n",
    "Substituting the values, we get\n",
    "\n",
    "$$\n",
    "P(C_3 \\mid H_2) = \\frac{1 \\cdot \\frac{1}{3}}{\\frac{1}{2} \\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3}} = \\frac{\\frac{1}{3}}{\\frac{1}{6} + \\frac{1}{3}} = \\frac{\\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}.\n",
    "$$\n",
    "\n",
    "Thus, after the host opens door 2 (i.e. event $H_2$ occurs), the unopened door (door 3) now carries a probability of $\\frac{2}{3}$ of concealing the car, while the initial door (door 1) retains a probability of $\\frac{1}{3}$.\n",
    "\n",
    "A similar argument holds if the host opens any door other than your initial choice (by appropriately relabelling the events).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb109ef-073b-44dc-8a99-07db90d6ae71",
   "metadata": {},
   "source": [
    "# Exercise: The Monty Hall Problem Part 2\n",
    "\n",
    "In the Monty Hall problem, extend the scenario to include a variable number of doors n, ranging from 3 to 100. Your task is to empirically validate the theoretical probabilities through simulation. In this scenario, the host opens all but two doors (the player’s chosen door and one other), so that n-2 doors are revealed, each showing a goat. Argue mathematically what you expect to be the resulting probabilities for n approaching infinity.  Then, use Python and the numpy library to implement the simulation, and compare the two strategies: switching and non-switching doors.\n",
    "\n",
    "**Setup:** For each simulation, create an array representing the doors. The array should have a length equal to the number of doors, with all elements set to 0 except for one randomly chosen position, which represents the door with the car and is set to 1.\n",
    "\n",
    "**Agents:**\n",
    "- Non-switching Agent: This agent randomly selects one of the doors. After the host reveals a goat behind one of the unselected doors, this agent sticks with the original choice.\n",
    "- Switching Agent: Similarly, this agent initially selects a door at random. However, after the host opens the n-2 doors to reveal a goat, this agent switches to the other unopened door, the one that was not initially selected.\n",
    "\n",
    "**Simulation Objective:** \n",
    "Run simulations for scenarios with door counts ranging from 3 to 100. For each scenario, calculate the average performance (win rate) of both agents over a significant number of trials. Additionally, calculate the variance in performance for each agent across these trials.\n",
    "\n",
    "**Visualization:** \n",
    "Plot the average win rates of both agents as a function of the number of doors over 200 trials. Include error bars in the graph to represent the standard deviation (the square root of the variance) in win rates for each number of doors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0bcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MontyHallSimulation:\n",
    "    def __init__(self, n_doors, n_trials):\n",
    "        self.n_doors = n_doors\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def run_simulation(self):\n",
    "        #store results of the two strategies\n",
    "        #outcomes_stay = \n",
    "        #outcomes_switch =\n",
    " \n",
    "        door_counts =range(3, self.n_doors + 1)\n",
    "\n",
    "        #ADD YOUR CODE HERE\n",
    "\n",
    "        #stay_win_rates = outcomes_stay.mean(axis=1)\n",
    "        #switch_win_rates = outcomes_switch.mean(axis=1)\n",
    "        #stay_variances = outcomes_stay.var(axis=1)\n",
    "        #switch_variances = outcomes_switch.var(axis=1)\n",
    "\n",
    "        #return door_counts, stay_win_rates, switch_win_rates, stay_variances, switch_variances\n",
    "\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"run sumulation, do plots.\"\"\"\n",
    "        #ADD YOUR CODE HERE\n",
    "\n",
    "# Parameters for the simulation\n",
    "n_doors = 100\n",
    "n_trials = 200\n",
    "\n",
    "# Creating an instance of MontyHallSimulation\n",
    "#simulation = MontyHallSimulation(n_doors, n_trials)\n",
    "\n",
    "#simulation.plot_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a37bb-efe4-4a26-87bd-f74b882fb116",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show Solution</summary>\n",
    "  \n",
    "If there are $n$ doors, the probability of choosing the door that contains the car is $\\frac{1}{n}$. Let's assume the player chooses one door; the probability that a car is behind it is $p_{stay} = \\frac{1}{n}$. The probability of the car being behind one of the other doors is $p_{others} = \\frac{n-1}{n}$. After the player's choice, the host opened all the other doors apart from one, which we will call the switch door. Since all the open doors have goats behind them, the probability that the car is behind the switch door is $p_{switch} = \\frac{n-1}{n} = 1 - \\frac{1}{n}$.\n",
    "\n",
    "As $n$ approaches infinity, $p_{stay}$, approaches 0: $\\lim_{n \\to \\infty} p_{stay} = \\lim_{n \\to \\infty} \\frac{1}{n} = 0$. Conversely, $p_{switch}$, approaches 1: $\\lim_{n \\to \\infty} p_{switch} = \\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right) = 1$.\n",
    "\n",
    "A key concept to this explanation is that future actions do not affect the past probabilities; the intuition is that to win by staying, you should have selected correctly in the first instance. For a very large number of doors, it is clear that the probability of selecting correctly in the first instance goes to 0.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MontyHallSimulation:\n",
    "    def __init__(self, n_doors, n_trials):\n",
    "        self.n_doors = n_doors\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def run_simulation(self):\n",
    "        outcomes_stay = np.zeros((self.n_doors - 2, self.n_trials))\n",
    "        outcomes_switch = np.zeros((self.n_doors - 2, self.n_trials))\n",
    "        door_counts =range(3, self.n_doors + 1)\n",
    "        \n",
    "        for doors in door_counts:\n",
    "            for trial in range(self.n_trials):\n",
    "                car_position = np.random.randint(doors)\n",
    "                contestant_choice = np.random.randint(doors)\n",
    "                outcomes_stay[doors-3, trial] = 1 if contestant_choice == car_position else 0\n",
    "                outcomes_switch[doors-3, trial] = 1 if contestant_choice != car_position else 0\n",
    "\n",
    "        stay_win_rates = outcomes_stay.mean(axis=1)\n",
    "        switch_win_rates = outcomes_switch.mean(axis=1)\n",
    "        stay_variances = outcomes_stay.var(axis=1)\n",
    "        switch_variances = outcomes_switch.var(axis=1)\n",
    "\n",
    "        \n",
    "        return door_counts, stay_win_rates, switch_win_rates, stay_variances, switch_variances\n",
    "\n",
    "    def plot_results(self):\n",
    "        door_counts, stay_win_rates, switch_win_rates, stay_variances, switch_variances = self.run_simulation()\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.errorbar(door_counts, stay_win_rates, yerr=np.sqrt(stay_variances), label='Stay', fmt='-o', capsize=5)\n",
    "        plt.errorbar(door_counts, switch_win_rates, yerr=np.sqrt(switch_variances), label='Switch', fmt='-x', capsize=5)\n",
    "        plt.xlabel('Number of Doors')\n",
    "        plt.ylabel('Win Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "   \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Parameters for the simulation\n",
    "n_doors = 100\n",
    "n_trials = 200\n",
    "\n",
    "# Creating an instance of MontyHallSimulation\n",
    "simulation = MontyHallSimulation(n_doors, n_trials)\n",
    "# Plot without smoothing variance\n",
    "simulation.plot_results()\n",
    "  ```\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
